{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8a6232-fbdd-466a-a420-4fc50b79008a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1103ad1b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba3d47-70ac-4104-ab4e-162ac51f322c",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "When we build a model that predicts an output quantity from a given input, the **loss function** measures the discrepancy between the model’s prediction and the true target value. Once the loss is defined, we need a method to adjust the model’s parameters so that this loss is **minimized**.\n",
    "\n",
    "This process is called **training the model**, and the specific method used to **update the parameters** is referred to as the **optimization step**. In this lab, we will demonstrate some techniques used to optimize model parameters.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956f7f1-7838-4c14-8284-8bbd0f4694d8",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "| Optimizer | Key Innovation | Advantages | Disadvantages | Best Use Cases |\n",
    "|-----------|---------------|------------|---------------|----------------|\n",
    "| **SGD** | Basic gradient descent with optional momentum | • Simple and interpretable<br>• Low memory overhead<br>• Works well with momentum<br>• Good generalization with proper tuning | • Requires careful learning rate tuning<br>• Same learning rate for all parameters<br>• Can be slow to converge<br>• Struggles with sparse gradients | • Well-understood problems<br>• When generalization is critical<br>• Large-scale models (memory efficient)<br>• CNNs with momentum |\n",
    "| **ASGD** | Averages parameters over time with adaptive learning rate decay | • Improved convergence over SGD<br>• Theoretically sound averaging<br>• Can escape sharp minima<br>• Better generalization than SGD alone | • Averaging only starts after t₀ steps<br>• t₀ is hard to tune (default: 1M steps)<br>• Less commonly used/tested<br>• Limited practical adoption | • Convex optimization problems<br>• When training stability is important<br>• Long training runs where averaging helps<br>• Theoretical research |\n",
    "| **Adagrad** | Adapts learning rate per parameter based on accumulated squared gradients | • No manual learning rate tuning needed<br>• Works well with sparse features<br>• Larger updates for infrequent features<br>• Good for sparse data (NLP, embeddings) | • Learning rate monotonically decreases<br>• Can stop learning too early<br>• Accumulates all historical gradients<br>• Poor for non-convex optimization | • Sparse data and features<br>• NLP tasks (word embeddings)<br>• Recommendation systems<br>• Problems with varying feature frequencies |\n",
    "| **Adadelta** | Fixes Adagrad's diminishing learning rate using exponential moving average | • No learning rate parameter needed<br>• More robust than Adagrad<br>• Adapts to gradient history<br>• Good for non-convex problems | • Can be slower than Adam<br>• Less commonly used in practice<br>• Hyperparameter ρ needs tuning<br>• May not converge as fast as Adam | • When you want Adagrad without decay issues<br>• Problems where learning rate tuning is difficult<br>• Mid-sized neural networks<br>• Alternative to RMSprop |\n",
    "| **RMSprop** | Uses exponential moving average of squared gradients with optional momentum | • Solves Adagrad's learning rate decay<br>• Works well in practice<br>• Good for RNNs and non-stationary problems<br>• Widely used and tested | • No bias correction<br>• Still requires learning rate tuning<br>• Can be unstable without proper hyperparameters<br>• Less popular than Adam | • Recurrent Neural Networks (RNNs)<br>• Non-stationary objectives<br>• Online learning settings<br>• When Adam is too complex |\n",
    "| **Adam** | Combines momentum and RMSprop with bias correction | • Generally works well out-of-the-box<br>• Fast convergence<br>• Bias correction for moments<br>• Most widely used optimizer<br>• Robust to hyperparameters | • Can overfit more than SGD<br>• May not generalize as well as SGD<br>• Uses more memory (stores moments)<br>• Can get stuck in poor local minima | • Default choice for most deep learning<br>• Transformers and attention models<br>• GANs and generative models<br>• When fast prototyping is needed<br>• Most NLP and vision tasks |\n",
    "| **AdamW** | Decouples weight decay from gradient update (proper L2 regularization) | • Better generalization than Adam<br>• Correct weight decay implementation<br>• State-of-the-art for many tasks<br>• Widely used in modern architectures | • Slightly more hyperparameters<br>• Still can overfit without proper regularization<br>• Higher memory usage<br>• May need different weight decay values than Adam | • Modern transformers (BERT, GPT, etc.)<br>• Large language models<br>• Vision transformers (ViT)<br>• When proper weight decay is critical<br>• Fine-tuning pretrained models |\n",
    "\n",
    "## General Recommendations\n",
    "\n",
    "- **Start with AdamW** for most modern deep learning tasks (transformers, large models)\n",
    "- **Use SGD with momentum** when generalization is critical and you have time to tune\n",
    "- **Use Adam** for quick prototyping or when AdamW isn't necessary\n",
    "- **Use Adagrad** for sparse data problems (embeddings, recommendation systems)\n",
    "- **Use RMSprop** for RNNs or when you want something simpler than Adam\n",
    "- **Avoid Adadelta and ASGD** unless you have specific reasons (less commonly used)\n",
    "\n",
    "## Key Hyperparameters to Tune\n",
    "\n",
    "| Optimizer | Critical Hyperparameters | Typical Good Values |\n",
    "|-----------|-------------------------|---------------------|\n",
    "| SGD | Learning rate, momentum | lr: 0.01-0.1, momentum: 0.9 |\n",
    "| ASGD | Learning rate, t₀ | lr: 0.01, t₀: problem-dependent |\n",
    "| Adagrad | Learning rate | lr: 0.01 |\n",
    "| Adadelta | ρ (rho) | ρ: 0.9-0.95 |\n",
    "| RMSprop | Learning rate, α (alpha) | lr: 0.001, α: 0.99 |\n",
    "| Adam | Learning rate, β₁, β₂ | lr: 0.001, β₁: 0.9, β₂: 0.999 |\n",
    "| AdamW | Learning rate, weight decay | lr: 0.001, weight_decay: 0.01 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf49604-aeb3-417c-97fa-bb15b42eba93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205326f-a75b-4cc6-a752-26930d0e3b82",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Stochastic Gradient Descent (SGD) updates the model parameters by computing the gradient of the loss function with respect to the parameters and usually taking a step in the opposite direction of this gradient. Unlike standard gradient descent, which uses the entire training dataset to compute each update, SGD estimates the gradient using **a single data point or a small batch of data**. This makes SGD computationally efficient and well-suited for large datasets, though the updates can be noisy.\n",
    "\n",
    "\n",
    "In general, the init method in optim-SGD has the following default arguments:\n",
    "\n",
    "```(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False, fused=None)```\n",
    "\n",
    "The default arguments ```differentiable``` and ```fused``` will be discussed later in more details. For now, if we denote the model's parameters at training step $t$ by $\\theta_t$ (starting with $t=0$), the learning rate by $\\gamma$, the momentum coefficient by $\\mu$, the dampening parameter by $\\tau$, and the weight decay by $\\lambda$, then the optimization step in SGD takes the following general form:\n",
    "\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "g_t &= \\begin{cases}\n",
    "-\\nabla_\\theta f(\\theta_{t-1}) & \\text{if maximize} \\\\\n",
    "\\nabla_\\theta f(\\theta_{t-1}) & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "g_t &\\leftarrow g_t + \\lambda \\theta_{t-1} \\quad \\text{if } \\lambda \\neq 0 \\\\\n",
    "b_t &= \\begin{cases}\n",
    "g_t & \\text{if } t = 1 \\\\\n",
    "\\mu b_{t-1} + (1-\\tau)g_t & \\text{if } t > 1\n",
    "\\end{cases} \\quad \\text{if } \\mu \\neq 0 \\\\\n",
    "g_t &\\leftarrow \\begin{cases}\n",
    "g_t + \\mu b_t & \\text{if nesterov} \\\\\n",
    "b_t & \\text{otherwise}\n",
    "\\end{cases} \\quad \\text{if } \\mu \\neq 0 \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\gamma g_t\n",
    "\\end{aligned}\n",
    "   $$\n",
    "That is,\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "\\theta_t =& \\theta_{t-1} - \\gamma \\tilde{g}_t\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\quad \\tilde{g}_t =& \\begin{cases}\n",
    "g_t + \\mu b_t & \\text{if nesterov and } \\mu \\neq 0 \\\\\n",
    "b_t & \\text{if } \\mu \\neq 0 \\text{ and not nesterov} \\\\\n",
    "g_t & \\text{if } \\mu = 0\n",
    "\\end{cases}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "b_t =& \\begin{cases}\n",
    "g_t & \\text{if } t = 1 \\\\\n",
    "\\mu b_{t-1} + (1-\\tau)g_t & \\text{if } t > 1\n",
    "\\end{cases} \\quad \\text{(only computed if } \\mu \\neq 0\\text{)}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "g_t = \\nabla_\\theta f(\\theta_{t-1}) + \\lambda\\theta_{t-1}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "293da7b5-2f93-47c4-8580-fcb2a378ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how this exactly work in PyTorch we create first some random trainable parameters:\n",
    "\n",
    "parameter_1 = nn.Parameter(torch.rand((2, 4), dtype=torch.float64))\n",
    "parameter_2 = nn.Parameter(torch.rand(4, dtype=torch.float64))\n",
    "\n",
    "params = nn.ParameterList([parameter_1, parameter_2])\n",
    "\n",
    "# Let's save the initial values of the parameters\n",
    "initial_param1 = parameter_1.clone().detach()\n",
    "initial_param2 = parameter_2.clone().detach()\n",
    "\n",
    "# Create a simple loss function\n",
    "def dummy_loss():\n",
    "    # Simple loss: sum of all parameters\n",
    "    return (parameter_1**2).sum() + (parameter_2**3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a10a3e5-51fc-44ee-991d-487c8145b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may also take the following choices for the other arguments:\n",
    "lr = 0.1\n",
    "momentum = 0.5\n",
    "dampening = 1\n",
    "weight_decay = 0.02\n",
    "nesterov = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fe6e1c1e-94d9-408b-a643-ae62bd5283e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(params, lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay,nesterov=nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "132c9996-7a53-441e-925b-cdaec5988595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients:\n",
      "grad parameter_1:\n",
      " tensor([[1.4953, 1.2757, 1.0132, 0.1502],\n",
      "        [1.2270, 1.9302, 0.6604, 1.8370]], dtype=torch.float64)\n",
      "grad parameter_2:\n",
      " tensor([1.0079e+00, 2.6933e-04, 1.0688e+00, 1.1233e+00], dtype=torch.float64)\n",
      "\n",
      "After one step:\n",
      "Updated parameter_1:\n",
      " Parameter containing:\n",
      "tensor([[0.5966, 0.5090, 0.4043, 0.0599],\n",
      "        [0.4896, 0.7701, 0.2635, 0.7330]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Updated parameter_2:\n",
      " Parameter containing:\n",
      "tensor([0.4777, 0.0094, 0.4888, 0.4984], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "\n",
      "Change in parameters:\n",
      "Δ parameter_1:\n",
      " tensor([[-0.1510, -0.1288, -0.1023, -0.0152],\n",
      "        [-0.1239, -0.1949, -0.0667, -0.1855]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "Δ parameter_2:\n",
      " tensor([-1.0195e-01, -4.5883e-05, -1.0808e-01, -1.1355e-01],\n",
      "       dtype=torch.float64, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Perform one optimization step\n",
    "optimizer.zero_grad()  # Clear previous gradients\n",
    "loss = dummy_loss()    # Compute loss\n",
    "loss.backward()        # Compute gradients\n",
    "\n",
    "print(\"\\nGradients:\")\n",
    "print(\"grad parameter_1:\\n\", parameter_1.grad)\n",
    "print(\"grad parameter_2:\\n\", parameter_2.grad)\n",
    "\n",
    "# Save gradients before optimizer.step()\n",
    "grad1 = parameter_1.grad.clone()\n",
    "grad2 = parameter_2.grad.clone()\n",
    "\n",
    "optimizer.step() #Update parameters\n",
    "\n",
    "print(\"\\nAfter one step:\")\n",
    "print(\"Updated parameter_1:\\n\", parameter_1)\n",
    "print(\"Updated parameter_2:\\n\", parameter_2)\n",
    "\n",
    "\n",
    "print(\"\\nChange in parameters:\")\n",
    "print(\"Δ parameter_1:\\n\", parameter_1 - initial_param1)\n",
    "print(\"Δ parameter_2:\\n\", parameter_2 - initial_param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa4f8f61-6405-43af-a5cf-06776a8c0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manual parameter_1:\n",
      " tensor([[0.5966, 0.5090, 0.4043, 0.0599],\n",
      "        [0.4896, 0.7701, 0.2635, 0.7330]], dtype=torch.float64)\n",
      "Manual parameter_2:\n",
      " tensor([0.4777, 0.0094, 0.4888, 0.4984], dtype=torch.float64)\n",
      "parameter_1 match: True\n",
      "parameter_2 match: True\n",
      "Max difference param1: 0.0\n",
      "Max difference param2: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Now manually reproduce the same update\n",
    "\n",
    "# Reset to initial values\n",
    "manual_param1 = initial_param1.clone()\n",
    "manual_param2 = initial_param2.clone()\n",
    "\n",
    "# Step 1: Compute g_t (gradient + weight decay)\n",
    "g1 = grad1 + weight_decay * initial_param1\n",
    "g2 = grad2 + weight_decay * initial_param2\n",
    "\n",
    "\n",
    "# Step 2: Apply momentum (for t=1, b_t = g_t since it's the first step)\n",
    "b1 = g1  # momentum buffer for parameter_1\n",
    "b2 = g2  # momentum buffer for parameter_2\n",
    "\n",
    "\n",
    "# Step 3: Since nesterov=False and momentum!=0, use g_t = b_t\n",
    "final_g1 = b1\n",
    "final_g2 = b2\n",
    "\n",
    "# Step 4: Update parameters\n",
    "manual_param1 = manual_param1 - lr * final_g1\n",
    "manual_param2 = manual_param2 - lr * final_g2\n",
    "\n",
    "print(\"\\nManual parameter_1:\\n\", manual_param1)\n",
    "print(\"Manual parameter_2:\\n\", manual_param2)\n",
    "\n",
    "# Verify they match\n",
    "\n",
    "print(\"parameter_1 match:\", torch.allclose(parameter_1, manual_param1))\n",
    "print(\"parameter_2 match:\", torch.allclose(parameter_2, manual_param2))\n",
    "print(\"Max difference param1:\", torch.max(torch.abs(parameter_1 - manual_param1)).item())\n",
    "print(\"Max difference param2:\", torch.max(torch.abs(parameter_2 - manual_param2)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3f0ff-64fd-4ac9-88f3-85c0f28b46d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3cb47b1-eae8-4f94-9e63-a596fc08c5c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ASGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42569170-2f60-4a99-89b1-39537c9e72ff",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Averaged Stochastic Gradient Descent (ASGD) is an optimization algorithm that enhances standard stochastic gradient descent by maintaining a running average of the parameter values over time. The key innovation is that ASGD computes parameter updates similarly to SGD but also keeps track of averaged parameters, which typically converge to better solutions. The averaging process begins after a specified number of steps (controlled by `t0`), and the learning rate is adjusted dynamically based on the number of steps taken. This averaging mechanism helps reduce the variance in the parameter estimates and can lead to improved generalization.\n",
    "\n",
    "In general, the init method in optim.ASGD has the following default arguments:\n",
    "\n",
    "```(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0, foreach=None, maximize=False, differentiable=False, capturable=False)```\n",
    "\n",
    "The default arguments ```differentiable```, ```foreach```, and ```capturable``` will be discussed later in more details. For now, if we denote the model's parameters at training step $t$ by $\\theta_t$ (starting with $t=0$), the learning rate by $\\gamma$, the decay term by $\\lambda$, the power for eta update by $\\alpha$, the averaging start point by $t_0$, and the weight decay by $\\omega$, then the optimization step in ASGD takes the following general form:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "g_t &= \\begin{cases}\n",
    "-\\nabla_\\theta f(\\theta_{t-1}) & \\text{if maximize} \\\\\n",
    "\\nabla_\\theta f(\\theta_{t-1}) & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "g_t &\\leftarrow g_t + \\omega \\theta_{t-1} \\quad \\text{if } \\omega \\neq 0 \\\\\n",
    "\\eta_t &= \\frac{1}{\\gamma(1 + \\lambda \\gamma t)^\\alpha} \\\\\n",
    "\\mu_t &= \\frac{1}{\\max(1, t - t_0)} \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\eta_t g_t \\\\\n",
    "\\bar{\\theta}_t &= \\begin{cases}\n",
    "\\theta_t & \\text{if } t \\leq t_0 \\\\\n",
    "(1 - \\mu_t)\\bar{\\theta}_{t-1} + \\mu_t \\theta_t & \\text{if } t > t_0\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "That is,\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "\\theta_t =& \\theta_{t-1} - \\eta_t g_t\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\eta_t =& \\frac{1}{\\gamma(1 + \\lambda \\gamma t)^\\alpha}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\bar{\\theta}_t =& \\begin{cases}\n",
    "\\theta_t & \\text{if } t \\leq t_0 \\\\\n",
    "(1 - \\mu_t)\\bar{\\theta}_{t-1} + \\mu_t \\theta_t & \\text{if } t > t_0\n",
    "\\end{cases}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\mu_t =& \\frac{1}{\\max(1, t - t_0)}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "g_t =& \\nabla_\\theta f(\\theta_{t-1}) + \\omega\\theta_{t-1}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "The ASGD is used similarly to the SGD.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9155e0f0-e63e-4329-98d5-135eee9d47ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b390a0d9-3ff7-4f1f-b2dc-24d873dfa966",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c831caa-81bc-4ce6-8ec8-54e8f0374981",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Adagrad (Adaptive Gradient Algorithm) is an optimization algorithm that adapts the learning rate for each parameter individually based on the historical gradients. The key innovation is that Adagrad accumulates the squared gradients over time and uses this accumulation to scale the learning rate differently for each parameter. Parameters that receive large gradients will have their learning rates reduced more aggressively, while parameters with small gradients will have relatively larger learning rates. This makes Adagrad particularly well-suited for dealing with sparse data and features that occur infrequently.\n",
    "\n",
    "In general, the init method in optim.Adagrad has the following default arguments:\n",
    "\n",
    "```(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10, foreach=None, maximize=False, differentiable=False, fused=None)```\n",
    "\n",
    "The default arguments ```differentiable```, ```foreach```, and ```fused``` will be discussed later in more details. For now, if we denote the model's parameters at training step $t$ by $\\theta_t$ (starting with $t=0$), the learning rate by $\\gamma$, the learning rate decay by $\\eta$, the weight decay by $\\lambda$, the initial accumulator value by $\\tau$, and the numerical stability term by $\\epsilon$, then the optimization step in Adagrad takes the following general form:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "\\text{state\\_sum}_0 &= \\tau \\\\\n",
    "g_t &= \\begin{cases}\n",
    "-\\nabla_\\theta f(\\theta_{t-1}) & \\text{if maximize} \\\\\n",
    "\\nabla_\\theta f(\\theta_{t-1}) & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "\\tilde{\\gamma}_t &= \\frac{\\gamma}{1 + (t-1)\\eta} \\\\\n",
    "g_t &\\leftarrow g_t + \\lambda \\theta_{t-1} \\quad \\text{if } \\lambda \\neq 0 \\\\\n",
    "\\text{state\\_sum}_t &= \\text{state\\_sum}_{t-1} + g_t^2 \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\frac{\\tilde{\\gamma}_t g_t}{\\sqrt{\\text{state\\_sum}_t} + \\epsilon}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "That is,\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "\\theta_t =& \\theta_{t-1} - \\frac{\\tilde{\\gamma}_t g_t}{\\sqrt{\\text{state\\_sum}_t} + \\epsilon}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\tilde{\\gamma}_t =& \\frac{\\gamma}{1 + (t-1)\\eta}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\text{state\\_sum}_t =& \\text{state\\_sum}_{t-1} + g_t^2 \\quad \\text{with } \\text{state\\_sum}_0 = \\tau\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "g_t =& \\nabla_\\theta f(\\theta_{t-1}) + \\lambda\\theta_{t-1}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a1958-0f4b-4558-b92a-f9a2d5559aab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb478c-cb04-482d-b6f4-e1c19bfe72aa",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size using a decaying average. The key innovation is that Adadelta also maintains a running average of the squared parameter updates, which it uses to adapt the learning rate. This eliminates the need to manually set an initial learning rate, though PyTorch's implementation still includes a learning rate scaling factor for flexibility.\n",
    "\n",
    "In general, the init method in optim.Adadelta has the following default arguments:\n",
    "\n",
    "```(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0, foreach=None, capturable=False, maximize=False, differentiable=False)```\n",
    "\n",
    "The default arguments ```differentiable```, ```foreach```, and ```capturable``` will be discussed later in more details. For now, if we denote the model's parameters at training step $t$ by $\\theta_t$ (starting with $t=0$), the learning rate by $\\gamma$, the decay rate by $\\rho$, the weight decay by $\\lambda$, and the numerical stability term by $\\epsilon$, then the optimization step in Adadelta takes the following general form:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "v_0 &= 0 \\quad \\text{(square avg)} \\\\\n",
    "u_0 &= 0 \\quad \\text{(accumulate variables)} \\\\\n",
    "g_t &= \\begin{cases}\n",
    "-\\nabla_\\theta f(\\theta_{t-1}) & \\text{if maximize} \\\\\n",
    "\\nabla_\\theta f(\\theta_{t-1}) & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "g_t &\\leftarrow g_t + \\lambda \\theta_{t-1} \\quad \\text{if } \\lambda \\neq 0 \\\\\n",
    "v_t &= v_{t-1}\\rho + g_t^2(1-\\rho) \\\\\n",
    "\\Delta x_t &= \\frac{\\sqrt{u_{t-1} + \\epsilon}}{\\sqrt{v_t + \\epsilon}} g_t \\\\\n",
    "u_t &= u_{t-1}\\rho + \\Delta x_t^2(1-\\rho) \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\gamma \\Delta x_t\n",
    "\\end{aligned}\n",
    "   $$\n",
    "That is,\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "\\theta_t =& \\theta_{t-1} - \\gamma \\Delta x_t\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\Delta x_t =& \\frac{\\sqrt{u_{t-1} + \\epsilon}}{\\sqrt{v_t + \\epsilon}} g_t\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "v_t =& v_{t-1}\\rho + g_t^2(1-\\rho) \\quad \\text{with } v_0 = 0\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "u_t =& u_{t-1}\\rho + \\Delta x_t^2(1-\\rho) \\quad \\text{with } u_0 = 0\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "g_t =& \\nabla_\\theta f(\\theta_{t-1}) + \\lambda\\theta_{t-1}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf22ed2-7cd2-4ca7-a12f-690abb38e6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82a931ae-e1af-42af-9c94-2337f2f5d76c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1962309-58f3-434d-a453-09f7f6b7f550",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "In general, the init method in optim.RMSprop has the following default arguments:\n",
    "\n",
    "```(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False, capturable=False, foreach=None, maximize=False, differentiable=False)```\n",
    "\n",
    "The default arguments ```differentiable```, ```foreach```, and ```capturable``` will be discussed later in more details. For now, if we denote the model's parameters at training step $t$ by $\\theta_t$ (starting with $t=0$), the learning rate by $\\gamma$, the smoothing constant by $\\alpha$, the weight decay by $\\lambda$, the momentum factor by $\\mu$, and the numerical stability term by $\\epsilon$, then the optimization step in RMSprop takes the following general form:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "v_0 &= 0 \\quad \\text{(square average)} \\\\\n",
    "b_0 &= 0 \\quad \\text{(buffer)} \\\\\n",
    "g_0^{ave} &= 0 \\\\\n",
    "g_t &= \\begin{cases}\n",
    "-\\nabla_\\theta f(\\theta_{t-1}) & \\text{if maximize} \\\\\n",
    "\\nabla_\\theta f(\\theta_{t-1}) & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "g_t &\\leftarrow g_t + \\lambda \\theta_{t-1} \\quad \\text{if } \\lambda \\neq 0 \\\\\n",
    "v_t &= \\alpha v_{t-1} + (1-\\alpha)g_t^2 \\\\\n",
    "g_t^{ave} &= \\alpha g_{t-1}^{ave} + (1-\\alpha)g_t \\quad \\text{(only if centered)} \\\\\n",
    "\\tilde{v}_t &= \\begin{cases}\n",
    "v_t - (g_t^{ave})^2 & \\text{if centered} \\\\\n",
    "v_t & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "b_t &= \\mu b_{t-1} + \\frac{g_t}{\\sqrt{\\tilde{v}_t+ \\epsilon} } \\quad \\text{(only if } \\mu > 0\\text{)} \\\\\n",
    "\\theta_t &= \\begin{cases}\n",
    "\\theta_{t-1} - \\gamma b_t & \\text{if } \\mu > 0 \\\\\n",
    "\\theta_{t-1} - \\frac{\\gamma g_t}{\\sqrt{\\tilde{v}_t+ \\epsilon}} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "That is,\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "\\theta_t =& \\begin{cases}\n",
    "\\theta_{t-1} - \\gamma b_t & \\text{if } \\mu > 0 \\\\\n",
    "\\theta_{t-1} - \\frac{\\gamma g_t}{\\sqrt{\\tilde{v}_t+ \\epsilon}} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "b_t =& \\mu b_{t-1} + \\frac{g_t}{\\sqrt{\\tilde{v}_t+ \\epsilon} } \\quad \\text{with } b_0 = 0 \\quad \\text{(only if } \\mu > 0\\text{)}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\tilde{v}_t =& \\begin{cases}\n",
    "v_t - (g_t^{ave})^2 & \\text{if centered} \\\\\n",
    "v_t & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "g_t^{ave} =& \\alpha g_{t-1}^{ave} + (1-\\alpha)g_t \\quad \\text{with } g_0^{ave} = 0 \\quad \\text{(only if centered)}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "v_t =& \\alpha v_{t-1} + (1-\\alpha)g_t^2 \\quad \\text{with } v_0 = 0\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "g_t =& \\nabla_\\theta f(\\theta_{t-1}) + \\lambda\\theta_{t-1}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b94c5f-64f3-434b-8680-a6d588488b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf847b4-2839-4b17-a6f3-af8611ef319b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e9287-75a6-401d-a8e1-de587762a41e",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is an optimization algorithm that combines ideas from RMSprop and momentum methods. The key innovation is that Adam computes adaptive learning rates for each parameter by maintaining exponentially decaying averages of both past gradients (first moment) and past squared gradients (second moment). Additionally, Adam includes bias correction terms to account for the initialization of these moment estimates at zero, which is particularly important during the initial training steps. An optional AMSGrad variant maintains the maximum of past second moments for improved convergence in some cases.\n",
    "\n",
    "In general, the init method in optim.Adam has the following default arguments:\n",
    "\n",
    "```(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None, decoupled_weight_decay=False)```\n",
    "\n",
    "The default arguments ```differentiable```, ```foreach```, ```capturable```, ```fused```, and ```decoupled_weight_decay``` will be discussed later in more details. For now, if we denote the model's parameters at training step $t$ by $\\theta_t$ (starting with $t=0$), the learning rate by $\\gamma$, the exponential decay rates for moment estimates by $\\beta_1$ and $\\beta_2$ (the `betas` tuple), the weight decay by $\\lambda$, and the numerical stability term by $\\epsilon$, then the optimization step in Adam takes the following general form:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "m_0 &= 0 \\quad \\text{(first moment)} \\\\\n",
    "v_0 &= 0 \\quad \\text{(second moment)} \\\\\n",
    "v_0^{max} &= 0 \\\\\n",
    "g_t &= \\begin{cases}\n",
    "-\\nabla_\\theta f(\\theta_{t-1}) & \\text{if maximize} \\\\\n",
    "\\nabla_\\theta f(\\theta_{t-1}) & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "g_t &\\leftarrow g_t + \\lambda \\theta_{t-1} \\quad \\text{if } \\lambda \\neq 0 \\\\\n",
    "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n",
    "v_t^{max} &= \\max(v_{t-1}^{max}, v_t) \\quad \\text{(only if amsgrad)} \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n",
    "\\hat{v}_t &= \\begin{cases}\n",
    "\\frac{v_t^{max}}{1-\\beta_2^t} & \\text{if amsgrad} \\\\\n",
    "\\frac{v_t}{1-\\beta_2^t} & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\frac{\\gamma \\hat{m}_t}{\\sqrt{\\hat{v}_t + \\epsilon}}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "That is,\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "\\theta_t =& \\theta_{t-1} - \\frac{\\gamma \\hat{m}_t}{\\sqrt{\\hat{v}_t + \\epsilon}}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\hat{m}_t =& \\frac{m_t}{1-\\beta_1^t}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\hat{v}_t =& \\begin{cases}\n",
    "\\frac{v_t^{max}}{1-\\beta_2^t} & \\text{if amsgrad} \\\\\n",
    "\\frac{v_t}{1-\\beta_2^t} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "v_t^{max} =& \\max(v_{t-1}^{max}, v_t) \\quad \\text{with } v_0^{max} = 0 \\quad \\text{(only if amsgrad)}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "m_t =& \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\quad \\text{with } m_0 = 0\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "v_t =& \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\quad \\text{with } v_0 = 0\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "g_t =& \\nabla_\\theta f(\\theta_{t-1}) + \\lambda\\theta_{t-1}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd1c15-df47-4cdf-8b33-4524033d0c35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b28168-dae0-4565-90ac-d6d0c7058301",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "AdamW (Adam with Decoupled Weight Decay) is a variant of the Adam optimizer that modifies how weight decay is applied. The key innovation is that weight decay is decoupled from the gradient-based optimization step and applied directly to the parameters. Unlike standard Adam where weight decay is added to the gradients (L2 regularization), AdamW applies weight decay as a separate multiplicative factor to the parameters themselves. This decoupling has been shown to improve generalization performance, particularly in deep learning applications.\n",
    "\n",
    "In general, the init method in optim.AdamW has the following default arguments:\n",
    "\n",
    "```(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False, maximize=False, foreach=None, capturable=False, differentiable=False, fused=None)```\n",
    "\n",
    "The default arguments ```differentiable```, ```foreach```, ```capturable```, and ```fused``` will be discussed later in more details. For now, if we denote the model's parameters at training step $t$ by $\\theta_t$ (starting with $t=0$), the learning rate by $\\gamma$, the exponential decay rates for moment estimates by $\\beta_1$ and $\\beta_2$ (the `betas` tuple), the weight decay by $\\lambda$, and the numerical stability term by $\\epsilon$, then the optimization step in AdamW takes the following general form:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "m_0 &= 0 \\quad \\text{(first moment)} \\\\\n",
    "v_0 &= 0 \\quad \\text{(second moment)} \\\\\n",
    "v_0^{max} &= 0 \\\\\n",
    "g_t &= \\begin{cases}\n",
    "-\\nabla_\\theta f(\\theta_{t-1}) & \\text{if maximize} \\\\\n",
    "\\nabla_\\theta f(\\theta_{t-1}) & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1} \\\\\n",
    "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n",
    "v_t^{max} &= \\max(v_{t-1}^{max}, v_t) \\quad \\text{(only if amsgrad)} \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n",
    "\\hat{v}_t &= \\begin{cases}\n",
    "\\frac{v_t^{max}}{1-\\beta_2^t} & \\text{if amsgrad} \\\\\n",
    "\\frac{v_t}{1-\\beta_2^t} & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "\\theta_t &\\leftarrow \\theta_t - \\frac{\\gamma \\hat{m}_t}{\\sqrt{\\hat{v}_t + \\epsilon}}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "That is,\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "\\theta_t =& \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1} - \\frac{\\gamma \\hat{m}_t}{\\sqrt{\\hat{v}_t + \\epsilon}}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\hat{m}_t =& \\frac{m_t}{1-\\beta_1^t}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "\\hat{v}_t =& \\begin{cases}\n",
    "\\frac{v_t^{max}}{1-\\beta_2^t} & \\text{if amsgrad} \\\\\n",
    "\\frac{v_t}{1-\\beta_2^t} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "v_t^{max} =& \\max(v_{t-1}^{max}, v_t) \\quad \\text{with } v_0^{max} = 0 \\quad \\text{(only if amsgrad)}\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "m_t =& \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\quad \\text{with } m_0 = 0\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "v_t =& \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2 \\quad \\text{with } v_0 = 0\n",
    "\\,,\n",
    "\\nonumber\n",
    "\\\\\n",
    "g_t =& \\nabla_\\theta f(\\theta_{t-1})\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b821c-0a30-42b2-8c6d-ccdcf342a76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
