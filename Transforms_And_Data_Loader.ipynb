{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d19cf3-e82e-4230-b443-80e4e301290e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c549210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, SequentialSampler, RandomSampler, SubsetRandomSampler, BatchSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8cac4a-5a41-45dc-994b-d5f4b5fea36b",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">Data does not always come in its final processed form that is required for training machine learning algorithms. We use **transforms** to perform some manipulation of the data and make it suitable for training. Moreover, we need an efficient way to **load** data in batches during training.\n",
    "\n",
    "PyTorch provides two fundamental data primitives:\n",
    "\n",
    "1. **`torch.utils.data.Dataset`**: An abstract class representing a dataset. It stores samples and their corresponding labels.\n",
    "2. **`torch.utils.data.DataLoader`**: Wraps an iterable around a `Dataset` to enable easy access to samples with support for batching, shuffling, and parallel loading.\n",
    "\n",
    "In this notebook, we will investigate:\n",
    "\n",
    "1. **Transforms**: How to preprocess and augment data using callable transforms\n",
    "2. **Dataset**: How to create custom datasets by subclassing `torch.utils.data.Dataset`\n",
    "3. **DataLoader**: How to efficiently load data in batches with various configurations\n",
    "4. **Samplers**: How to control the order in which data is loaded\n",
    "\n",
    "Throughout this notebook:\n",
    "- $B$ denotes the batch size\n",
    "- $N$ denotes the total number of samples in the dataset\n",
    "- $C$, $H$, $W$ denote channels, height, and width for image data\n",
    "\n",
    "\n",
    "**Overview**\n",
    "\n",
    "| Component | Purpose | Key Methods/Parameters | Primary Use Cases |\n",
    "|-----------|---------|----------------------|-------------------|\n",
    "| **Dataset** | Store and access individual samples | `__init__`, `__len__`, `__getitem__` | Custom data handling, preprocessing |\n",
    "| **DataLoader** | Batch, shuffle, and load data efficiently | `batch_size`, `shuffle`, `num_workers`, `collate_fn` | Training loops, batch iteration |\n",
    "| **Transforms** | Preprocess and augment data | Callable classes/functions | Normalization, augmentation, tensor conversion |\n",
    "| **Samplers** | Control data loading order | `SequentialSampler`, `RandomSampler`, `SubsetRandomSampler` | Custom sampling strategies |\n",
    "\n",
    "Detailed explanations of each component, including implementation examples, follow below.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e7703-33e6-4514-a1b2-c643ec1c4ccc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d181da9-5448-4c81-8fc8-d7c19009b817",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Transforms are **callable objects** (functions or classes with a `__call__` method) that take data as input and return transformed data. They are used to:\n",
    "\n",
    "1. **Convert data formats**: e.g., PIL Image → Tensor, NumPy array → Tensor\n",
    "2. **Normalize data**: Scale pixel values, standardize features\n",
    "3. **Augment data**: Random crops, flips, rotations for training robustness\n",
    "4. **Transform labels**: One-hot encoding, label smoothing\n",
    "\n",
    "In PyTorch, transforms are typically applied in two places:\n",
    "- `transform`: Applied to input features (e.g., images)\n",
    "- `target_transform`: Applied to labels/targets\n",
    "\n",
    "**Key Principle**: A transform is simply a callable that takes input and returns output:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\text{Transform}(\\text{input})\n",
    "$$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718bbb6-c3c8-4097-be17-ebf1835b0bda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Custom Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c531d4c9-6b37-4c6e-8da1-2be24342420f",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "A transform can be implemented as:\n",
    "1. **A simple function**: Takes input, returns output\n",
    "2. **A callable class**: Has `__init__` for parameters and `__call__` for the transformation\n",
    "\n",
    "Using a class is preferred when the transform has configurable parameters.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ef1346-d1f2-4925-a7ec-fbc010bcd29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original range: [-6.23, 27.08]\n",
      "Normalized range: [0.00, 1.00]\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Transform as a simple function\n",
    "def normalize_0_1(x):\n",
    "    \"\"\"Normalize tensor to [0, 1] range\"\"\"\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "# Test the function transform\n",
    "x = torch.randn(3, 4) * 10 + 5  # Random values\n",
    "x_normalized = normalize_0_1(x)\n",
    "print(f\"Original range: [{x.min():.2f}, {x.max():.2f}]\")\n",
    "print(f\"Normalized range: [{x_normalized.min():.2f}, {x_normalized.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c04041c-5316-4b37-ac90-5b851dee0bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize(mean=[0.48500001430511475, 0.4560000002384186, 0.4059999883174896], std=[0.2290000021457672, 0.2240000069141388, 0.22499999403953552])\n",
      "\n",
      "Original image stats per channel:\n",
      "  Channel 0: mean=0.454, std=0.257\n",
      "  Channel 1: mean=0.571, std=0.180\n",
      "  Channel 2: mean=0.445, std=0.319\n",
      "\n",
      "Normalized image stats per channel:\n",
      "  Channel 0: mean=-0.136, std=1.120\n",
      "  Channel 1: mean=0.512, std=0.804\n",
      "  Channel 2: mean=0.172, std=1.419\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Transform as a callable class (preferred for configurable transforms)\n",
    "# In this method, we normalize (each channel in a) tensor with given mean and std.\n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)  # Shape: (C, 1, 1) for broadcasting\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "    def __call__(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "    def __repr__(self):\n",
    "        return f\"Normalize(mean={self.mean.squeeze().tolist()}, std={self.std.squeeze().tolist()})\"\n",
    "\n",
    "# Test the class transform (ImageNet normalization values)\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "print(normalize)\n",
    "\n",
    "\n",
    "# Simulate a 3-channel image (C, H, W) = (3, 4, 4)\n",
    "image = torch.rand(3, 4, 4)\n",
    "normalized_image = normalize(image)\n",
    "print(f\"\\nOriginal image stats per channel:\")\n",
    "for c in range(3):\n",
    "    print(f\"  Channel {c}: mean={image[c].mean():.3f}, std={image[c].std():.3f}\")\n",
    "print(f\"\\nNormalized image stats per channel:\")\n",
    "for c in range(3):\n",
    "    print(f\"  Channel {c}: mean={normalized_image[c].mean():.3f}, std={normalized_image[c].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5f54f-9834-4991-b071-9a30c8e0ab3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Composing Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374761eb-2cde-44d2-904c-6609027850c9",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Often we need to apply multiple transforms in sequence. We can create a `Compose` class that chains transforms together:\n",
    "\n",
    "$$\n",
    "\\text{output} = T_n(T_{n-1}(...T_2(T_1(\\text{input}))))\n",
    "$$\n",
    "\n",
    "This is equivalent to `torchvision.transforms.Compose`.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43651e82-56dc-4cf8-a8ea-9143161826a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    \"\"\"Compose multiple transforms together.\n",
    "    \n",
    "    Args:\n",
    "        transforms: List of transforms to compose\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for transform in self.transforms:\n",
    "            x = transform(x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += f'\\n    {t},'\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd3b4009-9abe-44ab-a884-021f3c0a7e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor(),\n",
      "    RandomHorizontalFlip(p=0.5),\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define additional transforms\n",
    "class ToTensor:\n",
    "    \"\"\"Convert numpy array to PyTorch tensor.\"\"\"\n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            # Handle image data: (H, W, C) -> (C, H, W)\n",
    "            if x.ndim == 3:\n",
    "                x = x.transpose(2, 0, 1)\n",
    "            return torch.from_numpy(x).float()\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ToTensor()\"\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    \"\"\"Randomly flip tensor horizontally with given probability.\"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if torch.rand(1).item() < self.p:\n",
    "            return x.flip(-1)  # Flip along last dimension (width)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"RandomHorizontalFlip(p={self.p})\"\n",
    "\n",
    "# Compose multiple transforms\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6266e403-cd36-472d-93dc-2a03524d5336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: numpy array, shape=(32, 32, 3), dtype=float32\n",
      "Output: tensor, shape=torch.Size([3, 32, 32]), dtype=torch.float32\n",
      "Output range: [-1.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Test composed transform\n",
    "# Simulate a numpy image (H, W, C) with values in [0, 255]\n",
    "np_image = np.random.randint(0, 256, size=(32, 32, 3), dtype=np.uint8).astype(np.float32) / 255.0\n",
    "print(f\"Input: numpy array, shape={np_image.shape}, dtype={np_image.dtype}\")\n",
    "\n",
    "transformed = transform(np_image)\n",
    "print(f\"Output: tensor, shape={transformed.shape}, dtype={transformed.dtype}\")\n",
    "print(f\"Output range: [{transformed.min():.3f}, {transformed.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4fd8d-8996-46f6-8bba-1ccd00f3caf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Lambda Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152040c-f540-40f1-91b3-7479b2fc1c0c",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "For simple, one-off transforms, we can use a `Lambda` wrapper that applies any user-defined function:\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a4fa9c-0ab9-46ad-a82c-21e1d1f897a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda:\n",
    "    \"\"\"Apply a user-defined lambda function as a transform.\n",
    "    \n",
    "    Args:\n",
    "        lambd: Lambda function to apply\n",
    "    \"\"\"\n",
    "    def __init__(self, lambd):\n",
    "        self.lambd = lambd\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.lambd(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Lambda({self.lambd})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "479b2b8c-58eb-4705-948d-482a74697131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 3\n",
      "One-hot encoded: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Example: One-hot encoding for labels\n",
    "num_classes = 10\n",
    "one_hot_transform = Lambda(\n",
    "    lambda y: torch.zeros(num_classes, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)\n",
    ")\n",
    "\n",
    "# Test one-hot encoding\n",
    "label = 3\n",
    "one_hot_label = one_hot_transform(label)\n",
    "print(f\"Original label: {label}\")\n",
    "print(f\"One-hot encoded: {one_hot_label}\")\n",
    "print(f\"Shape: {one_hot_label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ce6dfd-6554-459a-a505-2e0de96dd6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: tensor([1., 1., 1., 1., 1.])\n",
      "With noise: tensor([1.1109, 0.9746, 1.0581, 0.9678, 0.9511])\n"
     ]
    }
   ],
   "source": [
    "# Another example: Adding noise to data\n",
    "add_noise = Lambda(lambda x: x + torch.randn_like(x) * 0.1)\n",
    "\n",
    "x = torch.ones(5)\n",
    "x_noisy = add_noise(x)\n",
    "print(f\"Original: {x}\")\n",
    "print(f\"With noise: {x_noisy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b1a17-2d5c-4088-8b0b-973a737360fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Common Transform Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ad34a-8aa6-450f-9cd1-f482b2358ea5",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Here we implement several commonly used transforms from scratch to understand their mechanics:\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c0b9e2a-fcdb-4e06-8ab2-4658519c7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize:\n",
    "    \"\"\"Resize tensor to given size using interpolation.\n",
    "    \n",
    "    Args:\n",
    "        size: Target size (H, W) or single int for square\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, int):\n",
    "            self.size = (size, size)\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x shape: (C, H, W) -> add batch dim -> (1, C, H, W)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = torch.nn.functional.interpolate(x, size=self.size, mode='bilinear', align_corners=False)\n",
    "        return x.squeeze(0)  # Remove batch dim\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Resize(size={self.size})\"\n",
    "\n",
    "class CenterCrop:\n",
    "    \"\"\"Crop the center of the tensor.\n",
    "    \n",
    "    Args:\n",
    "        size: Crop size (H, W) or single int for square\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, int):\n",
    "            self.size = (size, size)\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.shape\n",
    "        th, tw = self.size\n",
    "        \n",
    "        # Calculate crop coordinates\n",
    "        top = (h - th) // 2\n",
    "        left = (w - tw) // 2\n",
    "        \n",
    "        return x[:, top:top+th, left:left+tw]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"CenterCrop(size={self.size})\"\n",
    "\n",
    "class RandomCrop:\n",
    "    \"\"\"Randomly crop the tensor.\n",
    "    \n",
    "    Args:\n",
    "        size: Crop size (H, W) or single int for square\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, int):\n",
    "            self.size = (size, size)\n",
    "        else:\n",
    "            self.size = size\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.shape\n",
    "        th, tw = self.size\n",
    "        \n",
    "        # Random crop coordinates\n",
    "        top = torch.randint(0, h - th + 1, (1,)).item()\n",
    "        left = torch.randint(0, w - tw + 1, (1,)).item()\n",
    "        \n",
    "        return x[:, top:top+th, left:left+tw]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"RandomCrop(size={self.size})\"\n",
    "\n",
    "class StandardScaler:\n",
    "    \"\"\"Standardize features by removing the mean and scaling to unit variance.\n",
    "    \n",
    "    For each feature:\n",
    "        z = (x - mean) / std\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=None, std=None):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute mean and std from data.\"\"\"\n",
    "        self.mean = X.mean(dim=0)\n",
    "        self.std = X.std(dim=0)\n",
    "        return self\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise ValueError(\"Must call fit() first or provide mean and std\")\n",
    "        return (x - self.mean) / (self.std + 1e-8)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"StandardScaler(mean={self.mean}, std={self.std})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71390a0c-4434-4402-8e34-fb011be9e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([3, 64, 64])\n",
      "After Resize(32): torch.Size([3, 32, 32])\n",
      "After CenterCrop(48): torch.Size([3, 48, 48])\n",
      "After RandomCrop(48): torch.Size([3, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "# Test the transforms\n",
    "image = torch.rand(3, 64, 64)  # (C, H, W)\n",
    "\n",
    "resize = Resize(32)\n",
    "center_crop = CenterCrop(48)\n",
    "random_crop = RandomCrop(48)\n",
    "\n",
    "print(f\"Original shape: {image.shape}\")\n",
    "print(f\"After Resize(32): {resize(image).shape}\")\n",
    "print(f\"After CenterCrop(48): {center_crop(image).shape}\")\n",
    "print(f\"After RandomCrop(48): {random_crop(image).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758729d9-7bbb-4353-a262-d0fbbee09f9a",
   "metadata": {},
   "source": [
    "**Transforms Quick Reference**\n",
    "\n",
    "| Transform | Purpose | Input → Output |\n",
    "|-----------|---------|----------------|\n",
    "| `ToTensor` | Convert to PyTorch tensor | numpy/PIL → Tensor |\n",
    "| `Normalize` | Channel-wise normalization | Tensor → Tensor |\n",
    "| `Resize` | Resize to target size | (C,H,W) → (C,H',W') |\n",
    "| `CenterCrop` | Crop center region | (C,H,W) → (C,h,w) |\n",
    "| `RandomCrop` | Random position crop | (C,H,W) → (C,h,w) |\n",
    "| `RandomHorizontalFlip` | Flip with probability p | (C,H,W) → (C,H,W) |\n",
    "| `Compose` | Chain multiple transforms | Input → Output |\n",
    "| `Lambda` | Apply custom function | Input → Output |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e956c-945f-4c0d-95d3-3c26f45f92f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c77116f0-f247-4dce-93a2-ea96fede6cc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27535f56-92c0-4a80-b13e-c5c0dc717c1a",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The `Dataset` class is an abstract class representing a dataset. A custom dataset must inherit from `torch.utils.data.Dataset` and implement the following methods:\n",
    "\n",
    "1. **`__init__(self, ...)`**: Initialize the dataset (load data paths, set transforms, etc.)\n",
    "2. **`__len__(self)`**: Return the total number of samples in the dataset\n",
    "3. **`__getitem__(self, idx)`**: Return the sample (and label) at index `idx`\n",
    "\n",
    "PyTorch supports two types of datasets:\n",
    "\n",
    "| Type | Base Class | Access Pattern | Use Case |\n",
    "|------|------------|----------------|----------|\n",
    "| **Map-style** | `Dataset` | `dataset[idx]` | Random access, known size |\n",
    "| **Iterable-style** | `IterableDataset` | `iter(dataset)` | Streaming data, unknown size |\n",
    "\n",
    "We focus on **map-style** datasets, which are the most common.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8672b0-7604-41b3-a814-cf9e90f6bec1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basic Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed6674-0dc1-4c41-b35c-7b11b20568a0",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Let's create a simple dataset from in-memory data:\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bb87c13-baa5-4ddf-9b92-c82e2e5c2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"A simple dataset holding features and labels in memory.\n",
    "    \n",
    "    Args:\n",
    "        features: Tensor of shape (N, *) where N is number of samples\n",
    "        labels: Tensor of shape (N,) or (N, *)\n",
    "        transform: Optional transform to apply to features\n",
    "        target_transform: Optional transform to apply to labels\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels, transform=None, target_transform=None):\n",
    "        assert len(features) == len(labels), \"Features and labels must have same length\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return the sample and label at index idx.\"\"\"\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e5bb079-d8c2-46a1-b9af-4710a18d1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100\n",
      "First sample:\n",
      "  Features shape: torch.Size([10])\n",
      "  Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "N = 100  # Number of samples\n",
    "D = 10   # Feature dimension\n",
    "\n",
    "X = torch.randn(N, D)  # Features: (N, D)\n",
    "y = torch.randint(0, 5, (N,))  # Labels: (N,) with 5 classes\n",
    "\n",
    "# Create dataset without transforms\n",
    "dataset = SimpleDataset(X, y)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"First sample:\")\n",
    "x_0, y_0 = dataset[0]\n",
    "print(f\"  Features shape: {x_0.shape}\")\n",
    "print(f\"  Label: {y_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "784eb507-08b6-421c-94d2-ff3c272015a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transforms:\n",
      "  Features (standardized): mean=0.032, std=0.994\n",
      "  Label (one-hot): tensor([1., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/23jc2r615z79fzmhd0pd6nsh0000gn/T/ipykernel_34929/2807668459.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  one_hot = Lambda(lambda y: torch.zeros(5).scatter_(0, torch.tensor(y), 1.0))\n"
     ]
    }
   ],
   "source": [
    "# Create dataset with transforms\n",
    "scaler = StandardScaler().fit(X)  # Fit scaler on data\n",
    "one_hot = Lambda(lambda y: torch.zeros(5).scatter_(0, torch.tensor(y), 1.0))\n",
    "\n",
    "dataset_with_transforms = SimpleDataset(X, y, transform=scaler, target_transform=one_hot)\n",
    "\n",
    "x_0, y_0 = dataset_with_transforms[0]\n",
    "print(f\"With transforms:\")\n",
    "print(f\"  Features (standardized): mean={x_0.mean():.3f}, std={x_0.std():.3f}\")\n",
    "print(f\"  Label (one-hot): {y_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ecd821-3fd6-49fb-a164-67ef31df21a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Image Dataset from Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049e17c-a28c-45f7-9c42-44d756e561b3",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "A more realistic example: loading images from disk. This pattern is common when:\n",
    "- Dataset is too large to fit in memory\n",
    "- Images are stored in a directory structure\n",
    "- Labels are in a CSV/annotation file\n",
    "\n",
    "**Typical directory structure:**\n",
    "```\n",
    "data/\n",
    "├── images/\n",
    "│   ├── img001.jpg\n",
    "│   ├── img002.jpg\n",
    "│   └── ...\n",
    "└── labels.csv\n",
    "```\n",
    "\n",
    "**labels.csv format:**\n",
    "```\n",
    "filename,label\n",
    "img001.jpg,0\n",
    "img002.jpg,1\n",
    "...\n",
    "```\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31437f62-fa5f-4d9f-bc4c-26cea94f74a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3\n",
      "Image shape: torch.Size([3, 224, 224]), Label: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset for loading images from a directory with labels from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        annotations_file: Path to CSV file with columns [filename, label]\n",
    "        img_dir: Directory containing images\n",
    "        transform: Optional transform to apply to images\n",
    "        target_transform: Optional transform to apply to labels\n",
    "    \"\"\"\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        # In practice, use: self.img_labels = pd.read_csv(annotations_file)\n",
    "        # For this example, we'll simulate it\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        # Simulated annotations (filename, label)\n",
    "        self.img_labels = [\n",
    "            ('img001.jpg', 0),\n",
    "            ('img002.jpg', 1),\n",
    "            ('img003.jpg', 0),\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get filename and label\n",
    "        img_name, label = self.img_labels[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        # In practice, load image:\n",
    "        # from torchvision.io import read_image\n",
    "        # image = read_image(img_path)\n",
    "        \n",
    "        # For this example, simulate with random tensor\n",
    "        image = torch.rand(3, 224, 224)  # Simulated RGB image\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Demo\n",
    "img_dataset = ImageDataset(\n",
    "    annotations_file='labels.csv',\n",
    "    img_dir='data/images',\n",
    "    transform=Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(img_dataset)}\")\n",
    "img, label = img_dataset[0]\n",
    "print(f\"Image shape: {img.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de17e9-b7ef-4cd6-834c-54771802be30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CSV/Tabular Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242800f6-4825-4faa-9829-56f1ccc083b2",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "For tabular data (e.g., from CSV files), we create a dataset that loads and returns feature vectors:\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eb7da32-53a8-4a6e-8a3b-bbf37dfe350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 200\n",
      "Features: tensor([ 0.4967, -0.1383,  0.6477])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    \"\"\"Dataset for loading tabular data from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV file\n",
    "        feature_cols: List of column names/indices for features\n",
    "        label_col: Column name/index for labels\n",
    "        transform: Optional transform for features\n",
    "        target_transform: Optional transform for labels\n",
    "    \"\"\"\n",
    "    def __init__(self, data, feature_cols, label_col, transform=None, target_transform=None):\n",
    "        # In practice: self.data = pd.read_csv(csv_file)\n",
    "        # For this example, data is passed directly as numpy array\n",
    "        self.features = torch.tensor(data[:, feature_cols], dtype=torch.float32)\n",
    "        self.labels = torch.tensor(data[:, label_col], dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Create sample tabular data\n",
    "# Columns: [feature1, feature2, feature3, label]\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(200, 4)\n",
    "data[:, 3] = (data[:, 0] + data[:, 1] > 0).astype(int)  # Binary classification\n",
    "\n",
    "csv_dataset = CSVDataset(\n",
    "    data=data,\n",
    "    feature_cols=[0, 1, 2],  # First 3 columns are features\n",
    "    label_col=3              # Last column is label\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(csv_dataset)}\")\n",
    "x, y = csv_dataset[0]\n",
    "print(f\"Features: {x}\")\n",
    "print(f\"Label: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f0af3-6ca6-4b9c-9013-f012fc50e8da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sequence Dataset (for RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfa544-3f84-4f83-9f0a-93efaed016bb",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "For sequential data (time series, text), we often need to create overlapping windows or handle variable-length sequences:\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b42000a-4e15-43ed-8e62-8ae63f2eeae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequence length: 100\n",
      "Sequence length (input): 10\n",
      "Prediction length: 1\n",
      "Number of windows: 90\n",
      "\n",
      "Input shape: torch.Size([10, 3]) (seq_len, features)\n",
      "Target shape: torch.Size([1, 3]) (pred_len, features)\n"
     ]
    }
   ],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for creating sliding windows from sequential data.\n",
    "    \n",
    "    Given a sequence and window size, creates overlapping windows.\n",
    "    Each window is used to predict the next value(s).\n",
    "    \n",
    "    Args:\n",
    "        data: Tensor of shape (T, D) where T is sequence length, D is features\n",
    "        seq_len: Length of input sequence (window size)\n",
    "        pred_len: Length of prediction horizon (default: 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_len, pred_len=1):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Number of valid windows\n",
    "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: seq_len consecutive values starting at idx\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        # Target: pred_len values after the input window\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len]\n",
    "        return x, y\n",
    "\n",
    "# Create sample time series data\n",
    "T = 100  # Total sequence length\n",
    "D = 3    # Number of features\n",
    "time_series = torch.randn(T, D)\n",
    "\n",
    "seq_dataset = SequenceDataset(time_series, seq_len=10, pred_len=1)\n",
    "\n",
    "print(f\"Original sequence length: {T}\")\n",
    "print(f\"Sequence length (input): 10\")\n",
    "print(f\"Prediction length: 1\")\n",
    "print(f\"Number of windows: {len(seq_dataset)}\")\n",
    "\n",
    "x, y = seq_dataset[0]\n",
    "print(f\"\\nInput shape: {x.shape} (seq_len, features)\")\n",
    "print(f\"Target shape: {y.shape} (pred_len, features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c4892-39d0-45b4-b345-ff2ecfc6557c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d624bb0-2285-436a-b86b-6b0f4abe55a6",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The `DataLoader` class wraps a `Dataset` and provides:\n",
    "\n",
    "1. **Batching**: Group samples into mini-batches\n",
    "2. **Shuffling**: Randomize the order of samples each epoch\n",
    "3. **Parallel Loading**: Use multiple worker processes for faster data loading\n",
    "4. **Collation**: Combine individual samples into batched tensors\n",
    "\n",
    "**Constructor signature:**\n",
    "```python\n",
    "DataLoader(\n",
    "    dataset,           # Dataset to load from\n",
    "    batch_size=1,      # Number of samples per batch\n",
    "    shuffle=False,     # Whether to shuffle data each epoch\n",
    "    sampler=None,      # Custom sampler for loading order\n",
    "    batch_sampler=None,# Custom batch sampler\n",
    "    num_workers=0,     # Number of subprocesses for loading\n",
    "    collate_fn=None,   # Function to merge samples into batch\n",
    "    pin_memory=False,  # Copy tensors to CUDA pinned memory\n",
    "    drop_last=False,   # Drop last incomplete batch\n",
    ")\n",
    "```\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea31db-b467-4ae7-9944-a4f0b4ba12de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basic DataLoader Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27af93ca-d82d-48e7-95b5-e10f39cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100\n",
      "Batch size: 16\n",
      "Number of batches: 7\n",
      "Expected batches: ceil(100/16) = 7\n"
     ]
    }
   ],
   "source": [
    "# Create a simple dataset\n",
    "N = 100\n",
    "X = torch.randn(N, 5)\n",
    "y = torch.randint(0, 3, (N,))\n",
    "dataset = SimpleDataset(X, y)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Batch size: 16\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Expected batches: ceil({N}/16) = {(N + 15) // 16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70e65f54-8db9-4a77-8b6a-14ebf3cebafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: features shape = torch.Size([16, 5]), labels shape = torch.Size([16])\n",
      "Batch 1: features shape = torch.Size([16, 5]), labels shape = torch.Size([16])\n",
      "Batch 2: features shape = torch.Size([16, 5]), labels shape = torch.Size([16])\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Iterate through DataLoader\n",
    "for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}: features shape = {batch_x.shape}, labels shape = {batch_y.shape}\")\n",
    "    if batch_idx >= 2:  # Only show first 3 batches\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8e1acbb-ac7d-4e67-9ed1-959460ffa22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single batch:\n",
      "  Features: torch.Size([16, 5])\n",
      "  Labels: torch.Size([16])\n",
      "  Label values: tensor([1, 2, 0, 1, 1, 0, 1, 0, 2, 2, 2, 0, 0, 0, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch using next(iter(...))\n",
    "batch_x, batch_y = next(iter(dataloader))\n",
    "print(f\"Single batch:\")\n",
    "print(f\"  Features: {batch_x.shape}\")\n",
    "print(f\"  Labels: {batch_y.shape}\")\n",
    "print(f\"  Label values: {batch_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920937fe-be42-4485-97e7-ef0a122a64a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Key DataLoader Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057e9e6-eb4a-4075-9d91-8b6b227441fb",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `batch_size` | int | 1 | Number of samples per batch |\n",
    "| `shuffle` | bool | False | Reshuffle data at each epoch |\n",
    "| `num_workers` | int | 0 | Subprocesses for data loading (0 = main process) |\n",
    "| `pin_memory` | bool | False | Copy to CUDA pinned memory (faster GPU transfer) |\n",
    "| `drop_last` | bool | False | Drop last batch if incomplete |\n",
    "| `collate_fn` | callable | None | Custom function to create batches |\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e46a522-860b-4605-b91d-44306f832db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100, Batch size: 32\n",
      "drop_last=False: 4 batches\n",
      "drop_last=True:  3 batches\n",
      "\n",
      "Last batch size (drop_last=False): 4\n"
     ]
    }
   ],
   "source": [
    "# drop_last=True: Drops the last batch if it's smaller than batch_size\n",
    "N = 100\n",
    "batch_size = 32\n",
    "\n",
    "dataset = SimpleDataset(torch.randn(N, 5), torch.randint(0, 3, (N,)))\n",
    "\n",
    "loader_keep = DataLoader(dataset, batch_size=batch_size, drop_last=False)\n",
    "loader_drop = DataLoader(dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "print(f\"Dataset size: {N}, Batch size: {batch_size}\")\n",
    "print(f\"drop_last=False: {len(loader_keep)} batches\")\n",
    "print(f\"drop_last=True:  {len(loader_drop)} batches\")\n",
    "\n",
    "# Check last batch size\n",
    "for i, (x, y) in enumerate(loader_keep):\n",
    "    if i == len(loader_keep) - 1:\n",
    "        print(f\"\\nLast batch size (drop_last=False): {len(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb7713f3-85a1-4e9e-af97-5734b4fb9f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Labels: [1, 6, 8, 3, 5]\n",
      "  Labels: [9, 7, 2, 4, 0]\n",
      "\n",
      "Epoch 2:\n",
      "  Labels: [3, 2, 8, 7, 5]\n",
      "  Labels: [6, 9, 4, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# shuffle=True: Data is reshuffled at each epoch\n",
    "small_dataset = SimpleDataset(\n",
    "    torch.arange(10).float().unsqueeze(1),  # Features: 0-9\n",
    "    torch.arange(10)                         # Labels: 0-9\n",
    ")\n",
    "\n",
    "loader_shuffle = DataLoader(small_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "print(\"Epoch 1:\")\n",
    "for x, y in loader_shuffle:\n",
    "    print(f\"  Labels: {y.tolist()}\")\n",
    "\n",
    "print(\"\\nEpoch 2:\")\n",
    "for x, y in loader_shuffle:\n",
    "    print(f\"  Labels: {y.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714aef0-9f8d-4e1f-be04-41f76e034288",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Custom Collate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f260832-90df-40b8-bafc-34330441d73d",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The `collate_fn` parameter specifies how to merge a list of samples into a batch. The default collate function:\n",
    "1. Stacks tensors along a new dimension (creating the batch dimension)\n",
    "2. Handles nested structures (tuples, dicts, lists)\n",
    "\n",
    "Custom collate functions are useful when:\n",
    "- Samples have variable lengths (need padding)\n",
    "- Samples have complex structure\n",
    "- Special batch processing is needed\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "748a7bf3-2b6a-4a1c-8c54-f40c15fd3ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default collate result:\n",
      "  Features: tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "  Labels: tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Default collate behavior\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "# List of samples\n",
    "samples = [\n",
    "    (torch.tensor([1, 2, 3]), torch.tensor(0)),\n",
    "    (torch.tensor([4, 5, 6]), torch.tensor(1)),\n",
    "    (torch.tensor([7, 8, 9]), torch.tensor(2)),\n",
    "]\n",
    "\n",
    "batch = default_collate(samples)\n",
    "print(\"Default collate result:\")\n",
    "print(f\"  Features: {batch[0]}\")\n",
    "print(f\"  Labels: {batch[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4ff9252-8ba6-49d7-ab99-d972cd4c335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences shape: torch.Size([4, 6, 4])\n",
      "Original lengths: tensor([6, 6, 6, 3])\n",
      "Labels: tensor([0, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# Custom collate function for variable-length sequences\n",
    "def collate_with_padding(batch):\n",
    "    \"\"\"Collate function that pads sequences to the same length.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (sequence, label) tuples where sequences may have different lengths\n",
    "    \n",
    "    Returns:\n",
    "        padded_sequences: Tensor of shape (B, max_len, D)\n",
    "        lengths: Tensor of original lengths (B,)\n",
    "        labels: Tensor of labels (B,)\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Get lengths and max length\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    max_len = lengths.max().item()\n",
    "    \n",
    "    # Get feature dimension\n",
    "    feature_dim = sequences[0].shape[-1] if sequences[0].dim() > 1 else 1\n",
    "    \n",
    "    # Create padded tensor\n",
    "    batch_size = len(sequences)\n",
    "    padded = torch.zeros(batch_size, max_len, feature_dim)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq = seq.view(-1, feature_dim)  # Ensure 2D\n",
    "        padded[i, :len(seq)] = seq\n",
    "    \n",
    "    labels = torch.stack([torch.tensor(l) for l in labels])\n",
    "    \n",
    "    return padded, lengths, labels\n",
    "\n",
    "# Dataset with variable-length sequences\n",
    "class VariableLengthDataset(Dataset):\n",
    "    def __init__(self, n_samples=20):\n",
    "        self.data = []\n",
    "        for i in range(n_samples):\n",
    "            length = torch.randint(3, 10, (1,)).item()  # Random length 3-9\n",
    "            seq = torch.randn(length, 4)  # 4 features\n",
    "            label = i % 3\n",
    "            self.data.append((seq, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "var_dataset = VariableLengthDataset()\n",
    "var_loader = DataLoader(var_dataset, batch_size=4, collate_fn=collate_with_padding)\n",
    "\n",
    "padded_seqs, lengths, labels = next(iter(var_loader))\n",
    "print(f\"Padded sequences shape: {padded_seqs.shape}\")\n",
    "print(f\"Original lengths: {lengths}\")\n",
    "print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eaa2d9-8310-4e44-81e6-f2a1acbe453e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Multi-Process Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21439c0b-4d26-43e2-b0d3-b788a6243df5",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Setting `num_workers > 0` enables multi-process data loading. This is useful when:\n",
    "- Data loading is I/O bound (reading from disk)\n",
    "- Data preprocessing is CPU-intensive\n",
    "- You want to overlap data loading with GPU computation\n",
    "\n",
    "**Important considerations:**\n",
    "- `num_workers=0`: Data is loaded in the main process\n",
    "- `num_workers>0`: Data is loaded in separate worker processes\n",
    "- Workers are spawned at the start of each epoch\n",
    "- Each worker loads a subset of the data\n",
    "\n",
    "**Best practices:**\n",
    "- Start with `num_workers=0` for debugging\n",
    "- Typical values: 2-8 workers (depends on CPU cores)\n",
    "- Use `pin_memory=True` when using GPU\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79ac62e4-ddd5-46bb-a27b-c693bc949dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-process DataLoader created\n",
      "To use multi-process loading, set num_workers > 0\n",
      "Example: DataLoader(dataset, batch_size=16, num_workers=4, pin_memory=True)\n"
     ]
    }
   ],
   "source": [
    "# Example: Comparing loading times (conceptual - actual speedup depends on data loading cost)\n",
    "import time\n",
    "\n",
    "class SlowDataset(Dataset):\n",
    "    \"\"\"Dataset that simulates slow data loading.\"\"\"\n",
    "    def __init__(self, n_samples=100):\n",
    "        self.n_samples = n_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Simulate I/O delay (in practice, this would be disk read time)\n",
    "        # time.sleep(0.001)  # Uncomment to see effect\n",
    "        return torch.randn(3, 32, 32), torch.tensor(idx % 10)\n",
    "\n",
    "slow_dataset = SlowDataset(n_samples=100)\n",
    "\n",
    "# Single process (num_workers=0)\n",
    "loader_single = DataLoader(slow_dataset, batch_size=16, num_workers=0)\n",
    "\n",
    "# Note: Multi-process loading (num_workers>0) may not work in notebook environments\n",
    "# In a script, you would use:\n",
    "# loader_multi = DataLoader(slow_dataset, batch_size=16, num_workers=4)\n",
    "\n",
    "print(\"Single-process DataLoader created\")\n",
    "print(f\"To use multi-process loading, set num_workers > 0\")\n",
    "print(f\"Example: DataLoader(dataset, batch_size=16, num_workers=4, pin_memory=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87294dd-0cda-4a91-b4f9-80985bbaefdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87016a1f-9d1e-466e-8eeb-6219b5c814ab",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Samplers define the strategy to draw samples from the dataset. They are iterators that yield sample indices.\n",
    "\n",
    "**Built-in Samplers:**\n",
    "\n",
    "| Sampler | Description | Use Case |\n",
    "|---------|-------------|----------|\n",
    "| `SequentialSampler` | Yields indices in order (0, 1, 2, ...) | Evaluation/inference |\n",
    "| `RandomSampler` | Yields indices in random order | Training |\n",
    "| `SubsetRandomSampler` | Random sampling from a subset of indices | Train/val splits |\n",
    "| `WeightedRandomSampler` | Weighted random sampling | Class imbalance |\n",
    "| `BatchSampler` | Wraps another sampler to yield batches of indices | Custom batching |\n",
    "\n",
    "**Note:** When using a custom sampler, set `shuffle=False` (or don't set shuffle at all).\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c5452d6-0bbc-4335-ab1e-1153fc0d59c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialSampler indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "RandomSampler indices: [5, 2, 9, 4, 7, 6, 3, 0, 1, 8]\n",
      "SubsetRandomSampler indices: [2, 8, 4, 6, 0]\n"
     ]
    }
   ],
   "source": [
    "# Create a small dataset for demonstration\n",
    "dataset = SimpleDataset(\n",
    "    torch.arange(10).float().unsqueeze(1),\n",
    "    torch.arange(10)\n",
    ")\n",
    "\n",
    "# SequentialSampler: indices in order\n",
    "seq_sampler = SequentialSampler(dataset)\n",
    "print(\"SequentialSampler indices:\", list(seq_sampler))\n",
    "\n",
    "# RandomSampler: indices in random order\n",
    "rand_sampler = RandomSampler(dataset)\n",
    "print(\"RandomSampler indices:\", list(rand_sampler))\n",
    "\n",
    "# SubsetRandomSampler: random sampling from specific indices\n",
    "subset_indices = [0, 2, 4, 6, 8]  # Only even indices\n",
    "subset_sampler = SubsetRandomSampler(subset_indices)\n",
    "print(\"SubsetRandomSampler indices:\", list(subset_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35ce3de2-024f-4a82-b007-97a98c555edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches from SubsetRandomSampler:\n",
      "  Labels: [0, 8, 4]\n",
      "  Labels: [2, 6]\n"
     ]
    }
   ],
   "source": [
    "# Using sampler with DataLoader\n",
    "loader = DataLoader(dataset, batch_size=3, sampler=subset_sampler)\n",
    "\n",
    "print(\"Batches from SubsetRandomSampler:\")\n",
    "for x, y in loader:\n",
    "    print(f\"  Labels: {y.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c650f-d044-484b-a457-593e689410d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train/Validation Split with Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fb2b03e-ae3d-489c-a866-72da9869b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 100\n",
      "Training samples: 80\n",
      "Validation samples: 20\n",
      "\n",
      "Train batches: 5\n",
      "Val batches: 2\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "N = 100\n",
    "dataset = SimpleDataset(\n",
    "    torch.randn(N, 5),\n",
    "    torch.randint(0, 3, (N,))\n",
    ")\n",
    "\n",
    "# Create train/val split\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * N)\n",
    "\n",
    "# Shuffle indices\n",
    "indices = torch.randperm(N).tolist()\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "print(f\"Total samples: {N}\")\n",
    "print(f\"Training samples: {len(train_indices)}\")\n",
    "print(f\"Validation samples: {len(val_indices)}\")\n",
    "\n",
    "# Create samplers\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d920572c-6646-4548-8094-efaf2962a9bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Weighted Sampling for Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "113152bd-5c31-4e7b-8bb8-6e59c00db52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [90, 10]\n",
      "Class weights: [0.011111111380159855, 0.10000000149011612]\n",
      "\n",
      "Batch class distribution: [8, 12]\n",
      "Classes are now roughly balanced in each batch!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Create imbalanced dataset: class 0 has 90 samples, class 1 has 10 samples\n",
    "labels = torch.cat([torch.zeros(90), torch.ones(10)]).long()\n",
    "features = torch.randn(100, 5)\n",
    "imbalanced_dataset = SimpleDataset(features, labels)\n",
    "\n",
    "# Count class frequencies\n",
    "class_counts = torch.bincount(labels)\n",
    "print(f\"Class distribution: {class_counts.tolist()}\")\n",
    "\n",
    "# Calculate weights: inverse of class frequency\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "print(f\"Class weights: {class_weights.tolist()}\")\n",
    "\n",
    "# Assign weight to each sample based on its class\n",
    "sample_weights = class_weights[labels]\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),  # Number of samples to draw\n",
    "    replacement=True  # With replacement to allow oversampling\n",
    ")\n",
    "\n",
    "weighted_loader = DataLoader(imbalanced_dataset, batch_size=20, sampler=weighted_sampler)\n",
    "\n",
    "# Check class balance in a batch\n",
    "x, y = next(iter(weighted_loader))\n",
    "batch_counts = torch.bincount(y, minlength=2)\n",
    "print(f\"\\nBatch class distribution: {batch_counts.tolist()}\")\n",
    "print(\"Classes are now roughly balanced in each batch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371d437-ccf3-487d-8c14-e3e52b49d4c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Putting It All Together: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80de9e03-5d9e-4ce0-b5d5-015571ac1aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: Train Loss: 1.6265, Train Acc: 20.00% | Val Loss: 1.6276, Val Acc: 21.00%\n",
      "Epoch 2/3: Train Loss: 1.6022, Train Acc: 23.88% | Val Loss: 1.6330, Val Acc: 21.50%\n",
      "Epoch 3/3: Train Loss: 1.5887, Train Acc: 26.12% | Val Loss: 1.6303, Val Acc: 20.50%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 1. Create Dataset with transforms\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000, n_features=20, n_classes=5, transform=None):\n",
    "        self.features = torch.randn(n_samples, n_features)\n",
    "        self.labels = torch.randint(0, n_classes, (n_samples,))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "# 2. Create transforms\n",
    "class AddNoise:\n",
    "    def __init__(self, std=0.1):\n",
    "        self.std = std\n",
    "    def __call__(self, x):\n",
    "        return x + torch.randn_like(x) * self.std\n",
    "\n",
    "train_transform = Compose([\n",
    "    AddNoise(std=0.1),  # Data augmentation\n",
    "])\n",
    "\n",
    "# 3. Create datasets\n",
    "full_dataset = MyDataset(n_samples=1000, transform=train_transform)\n",
    "\n",
    "# 4. Create train/val split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "indices = torch.randperm(len(full_dataset)).tolist()\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# 5. Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    full_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=0,  # Use >0 in production\n",
    "    pin_memory=False  # Set True if using GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    full_dataset,\n",
    "    batch_size=32,\n",
    "    sampler=val_sampler\n",
    ")\n",
    "\n",
    "# 6. Create model, loss, optimizer\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(20, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 5)\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 7. Training loop\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_loss /= train_total\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_loss /= val_total\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89339e7-45a5-4caa-b625-cc4f46868eba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Quick Reference Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db5a71-f12f-4de2-86fa-e3311f266c21",
   "metadata": {},
   "source": [
    "**Dataset Methods**\n",
    "\n",
    "| Method | Required | Purpose |\n",
    "|--------|----------|--------|\n",
    "| `__init__` | Yes | Initialize dataset, load metadata |\n",
    "| `__len__` | Yes | Return total number of samples |\n",
    "| `__getitem__` | Yes | Return sample at given index |\n",
    "| `__getitems__` | No | Batch loading optimization (optional) |\n",
    "\n",
    "**DataLoader Parameters**\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `dataset` | Dataset | required | Dataset to load from |\n",
    "| `batch_size` | int | 1 | Samples per batch |\n",
    "| `shuffle` | bool | False | Randomize order each epoch |\n",
    "| `sampler` | Sampler | None | Custom sampling strategy |\n",
    "| `num_workers` | int | 0 | Parallel loading processes |\n",
    "| `collate_fn` | callable | None | Custom batch creation |\n",
    "| `pin_memory` | bool | False | Use CUDA pinned memory |\n",
    "| `drop_last` | bool | False | Drop incomplete final batch |\n",
    "\n",
    "**Sampler Types**\n",
    "\n",
    "| Sampler | Use Case |\n",
    "|---------|----------|\n",
    "| `SequentialSampler` | Evaluation (in order) |\n",
    "| `RandomSampler` | Training (shuffle) |\n",
    "| `SubsetRandomSampler` | Train/val splits |\n",
    "| `WeightedRandomSampler` | Class imbalance |\n",
    "| `BatchSampler` | Custom batch strategies |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8aeb56-d0c2-4e2a-a075-39a240331c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d75d8947-16b3-4d0d-a2d0-762ddb15b929",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ccce4-8b05-4fb3-8459-1fbebd76f6f0",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Key takeaways for working with PyTorch data loading:\n",
    "\n",
    "1. **Transforms** are callables that preprocess data. Use `Compose` to chain multiple transforms.\n",
    "\n",
    "2. **Dataset** is an abstract class requiring `__len__` and `__getitem__`. Use it to:\n",
    "   - Decouple data handling from model code\n",
    "   - Apply transforms lazily (on access)\n",
    "   - Handle various data sources (files, databases, etc.)\n",
    "\n",
    "3. **DataLoader** wraps Dataset for efficient training:\n",
    "   - `batch_size`: Start with 32 or 64, adjust based on GPU memory\n",
    "   - `shuffle=True`: Always shuffle training data\n",
    "   - `num_workers`: Use 2-8 for faster loading (0 for debugging)\n",
    "   - `pin_memory=True`: When using GPU\n",
    "   - `drop_last=True`: When batch size must be consistent\n",
    "\n",
    "4. **Samplers** control loading order:\n",
    "   - `SubsetRandomSampler`: For train/val splits\n",
    "   - `WeightedRandomSampler`: For class imbalance\n",
    "\n",
    "5. **Best Practices**:\n",
    "   - Load data lazily in `__getitem__`, not `__init__`\n",
    "   - Use `collate_fn` for variable-length data\n",
    "   - Profile data loading to find bottlenecks\n",
    "   - Consider `prefetch_factor` for overlapping data loading with computation\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab072f7d-3ee5-444c-855a-846bdfa41703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
