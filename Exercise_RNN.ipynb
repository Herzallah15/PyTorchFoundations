{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5deb9011-28d8-4c16-be22-ccd6a9423acf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Problem III: Text Classification with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060bc3dc-ffb0-4dcb-b2df-3389d262cebb",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "\n",
    "In this notebook, we explore **text classification** using **Recurrent Neural Networks (RNNs)**, one of the foundational architectures for processing sequential data. We will work with the **AG News** dataset, a popular benchmark for news article classification.\n",
    "\n",
    "We will:\n",
    "1. **Download** the AG News dataset from the internet\n",
    "2. **Tokenize** text into sequences of tokens\n",
    "3. **Build a vocabulary** mapping tokens to integer indices\n",
    "4. **Transform** text into padded tensor sequences\n",
    "5. **Organize** data using `Dataset` and `DataLoader` for efficient training\n",
    "6. **Build and train** an RNN-based classifier\n",
    "7. **Analyze** results\n",
    "\n",
    "**Key Takeaway**: Understanding the text data pipeline (download → tokenize → build vocabulary → numericalize → pad → batch) is foundational. **Unlike** images, text requires additional preprocessing steps to convert discrete symbols into continuous representations suitable for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83db72e-46b0-4ae1-a75c-da59e41be455",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 1: Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db055702-fa85-48aa-8090-9d924885ce5d",
   "metadata": {},
   "source": [
    "## 1. Introduction: What is Text Classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63b0af-8800-4a0e-9d74-f1abf056f1ae",
   "metadata": {},
   "source": [
    "**1.1 The Task**\n",
    "\n",
    "**Text classification** is the task of assigning a label to a text document from a fixed set of categories. Mathematically, we seek a function:\n",
    "\n",
    "$$f_{\\theta}: \\mathcal{X} \\rightarrow \\mathcal{Y}$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{X}$ is the space of variable-length text sequences (sentences, paragraphs, documents)\n",
    "- $\\mathcal{Y} = \\{0, 1, ..., K-1\\}$ is the set of $K$ class labels\n",
    "- $\\theta$ are the learnable parameters of our model\n",
    "\n",
    "For AG News:\n",
    "- Text documents are news articles (title + description)\n",
    "- There are $K = 4$ classes (World, Sports, Business, Sci/Tech)\n",
    "\n",
    "**1.2 Why AG News?**\n",
    "\n",
    "AG News is a widely-used benchmark dataset for text classification, constructed from news articles collected by an academic news search engine. It provides:\n",
    "\n",
    "| Property | AG News |\n",
    "|----------|---------------|\n",
    "| Training samples | 120,000 |\n",
    "| Test samples | 7,600 |\n",
    "| Classes | 4 (World, Sports, Business, Sci/Tech) |\n",
    "| Average text length | ~40 words |\n",
    "| Difficulty | Moderate |\n",
    "| Task | Multi-class classification |\n",
    "\n",
    "**Comparison with Image Classification (Fashion-MNIST)**:\n",
    "\n",
    "| Aspect | Image (Fashion-MNIST) | Text (AG News) |\n",
    "|--------|----------------------|----------------|\n",
    "| Input type | Fixed-size grid (28×28) | Variable-length sequence |\n",
    "| Data representation | Continuous pixel values | Discrete tokens (words) |\n",
    "| Spatial structure | 2D local correlations | 1D sequential dependencies |\n",
    "| Preprocessing | Normalize pixels | Tokenize → Numericalize → Pad |\n",
    "| Suitable architecture | CNN | RNN, LSTM, Transformer |\n",
    "\n",
    "**1.3 Why Recurrent Neural Networks?**\n",
    "\n",
    "Text is inherently **sequential**, the meaning of a word depends on the words that came before it. Consider:\n",
    "- \"The bank approved my loan\" (financial institution)\n",
    "- \"I sat by the river bank\" (edge of water)\n",
    "\n",
    "Standard full-connected networks or CNNs treat inputs as fixed-size vectors or grids, **ignoring sequential dependencies**.\n",
    "\n",
    "**RNNs exploit sequential structure** through:\n",
    "1. **Temporal processing**: Process one token at a time, left to right\n",
    "2. **Hidden state memory**: Maintain a hidden state $h_t$ that accumulates information from previous tokens\n",
    "3. **Parameter sharing**: The same weights are applied at every time step\n",
    "\n",
    "The core RNN equation:\n",
    "$$h_t = \\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1} W_{hh}^T + b_{hh})$$\n",
    "\n",
    "This makes RNNs naturally suited for variable-length sequences like text. **For more details about RNN layers, please see Layers.ipynb**.\n",
    "\n",
    "**1.4 The Text Classification Pipeline**\n",
    "\n",
    "Unlike images, text requires several preprocessing steps:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         TEXT CLASSIFICATION PIPELINE                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Raw Text           Tokenization         Numericalization        Embedding           RNN\n",
    "─────────          ────────────         ────────────────        ─────────           ───\n",
    "\"I love ML\"   →   [\"I\",\"love\",\"ML\"]  →    [5, 42, 137]     →   [e₅, e₄₂, e₁₃₇]  →  h_T\n",
    "                                                                     ↓\n",
    "                                                               Dense vectors       Final\n",
    "                                                               (learnable)        hidden\n",
    "                                                                                  state\n",
    "```\n",
    "\n",
    "Each step transforms the data:\n",
    "1. **Tokenization**: Split text into tokens (words, subwords, or characters)\n",
    "2. **Vocabulary building**: Create a mapping from tokens to integers\n",
    "3. **Numericalization**: Convert tokens to integer indices\n",
    "4. **Embedding**: Map integers to dense vectors (learnable)\n",
    "5. **RNN processing**: Process sequence and extract final representation\n",
    "6. **Classification**: Map final hidden state to class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b84428-5f68-4e25-b00a-0bd03cd5ac4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Downloading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f64fc9-c7b4-4e85-923b-1c98b02ba484",
   "metadata": {},
   "source": [
    "**2.1 The Mathematical Setup**\n",
    "\n",
    "Our dataset $\\mathcal{D}$ consists of $N$ labeled examples:\n",
    "\n",
    "$$\\mathcal{D} = \\{(\\mathbf{s}^{(i)}, y^{(i)})\\}_{i=1}^{N}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{s}^{(i)}$ is the $i$-th text string (variable-length sequence of characters)\n",
    "- $y^{(i)} \\in \\{0, 1, 2, 3\\}$ is the corresponding class label\n",
    "\n",
    "**2.2 Manual Download Approach**\n",
    "\n",
    "We download AG News directly from its source as CSV files. This approach is more robust than using `torchtext` (which has compatibility issues with newer PyTorch versions).\n",
    "\n",
    "The AG News dataset is hosted on various mirrors. We'll download it programmatically:\n",
    "\n",
    "**2.3 The AG News Classes**\n",
    "\n",
    "| Label | Description | Example Topics |\n",
    "|-------|-------------|----------------|\n",
    "| 1 | World | International politics, global events |\n",
    "| 2 | Sports | Football, basketball, Olympics |\n",
    "| 3 | Business | Stock market, economy, companies |\n",
    "| 4 | Sci/Tech | Technology, science discoveries |\n",
    "\n",
    "**Note**: The original labels are 1-indexed (1, 2, 3, 4). We will convert them to 0-indexed (0, 1, 2, 3) for PyTorch compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02384b20-4fd9-42e7-8fdc-b9f2e726eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used device is mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print(f'Used device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fb53ab-f82a-40fe-98ba-9b6108abbb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOWNLOADING AG NEWS DATASET\n",
      "============================================================\n",
      "\n",
      "Downloading train data...\n",
      "Train data downloaded: ./data/ag_news/train.csv\n",
      "Downloading test data...\n",
      "Test data downloaded: ./data/ag_news/test.csv\n",
      "\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "# Define data directory\n",
    "DATA_DIR = './data/ag_news'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# URLs for AG News dataset (hosted on GitHub mirror)\n",
    "URLS = {\n",
    "    'train': 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv',\n",
    "    'test': 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
    "}\n",
    "\n",
    "def download_ag_news(data_dir, urls):\n",
    "    \"\"\"Download AG News dataset if not already present.\"\"\"\n",
    "    for split, url in urls.items():\n",
    "        filepath = os.path.join(data_dir, f'{split}.csv')\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {split} data...\")\n",
    "            urllib.request.urlretrieve(url, filepath)\n",
    "            print(f\"{split.capitalize()} data downloaded: {filepath}\")\n",
    "        else:\n",
    "            print(f\"{split.capitalize()} data already exists: {filepath}\")\n",
    "    return os.path.join(data_dir, 'train.csv'), os.path.join(data_dir, 'test.csv')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOADING AG NEWS DATASET\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "train_path, test_path = download_ag_news(DATA_DIR, URLS)\n",
    "print(\"\\nDownload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1deb5536-046a-4a1c-abbc-16992144dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING RAW DATA\n",
      "============================================================\n",
      "\n",
      "Training samples: 120,000\n",
      "Test samples: 7,600\n",
      "\n",
      "Columns: ['label', 'title', 'description']\n",
      "\n",
      "Sample entry:\n",
      "  Label: 3 (Business)\n",
      "  Title: Wall St. Bears Claw Back Into the Black (Reuters)\n",
      "  Description: Reuters - Short-sellers, Wall Street's dwindling\\b...\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files\n",
    "# AG News CSV format: label, title, description (no header)\n",
    "train_df = pd.read_csv(train_path, header=None, names=['label', 'title', 'description'])\n",
    "test_df = pd.read_csv(test_path, header=None, names=['label', 'title', 'description'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING RAW DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining samples: {len(train_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "\n",
    "# Define class names\n",
    "CLASS_NAMES = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# Show a sample\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"\\nSample entry:\")\n",
    "print(f\"  Label: {sample['label']} ({CLASS_NAMES[sample['label']-1]})\")\n",
    "print(f\"  Title: {sample['title']}\")\n",
    "print(f\"  Description: {sample['description'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1156c88-5068-48d2-823a-0874c5241818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  World      30,000 samples (25.0%)\n",
      "  Sports     30,000 samples (25.0%)\n",
      "  Business   30,000 samples (25.0%)\n",
      "  Sci/Tech   30,000 samples (25.0%)\n",
      "\n",
      "Test Set:\n",
      "  World      1,900 samples (25.0%)\n",
      "  Sports     1,900 samples (25.0%)\n",
      "  Business   1,900 samples (25.0%)\n",
      "  Sci/Tech   1,900 samples (25.0%)\n",
      "\n",
      "Dataset is perfectly balanced!\n"
     ]
    }
   ],
   "source": [
    "# Examine class distribution\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTraining Set:\")\n",
    "for label in sorted(train_df['label'].unique()):\n",
    "    count = (train_df['label'] == label).sum()\n",
    "    print(f\"  {CLASS_NAMES[label-1]:10s} {count:,} samples ({100*count/len(train_df):.1f}%)\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "for label in sorted(test_df['label'].unique()):\n",
    "    count = (test_df['label'] == label).sum()\n",
    "    print(f\"  {CLASS_NAMES[label-1]:10s} {count:,} samples ({100*count/len(test_df):.1f}%)\")\n",
    "\n",
    "print(\"\\nDataset is perfectly balanced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f051e-fac2-47c3-982d-daff273c8263",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4006d4-7c54-4f52-a9cb-fe1593af8541",
   "metadata": {},
   "source": [
    "**3.1 What is Tokenization?**\n",
    "\n",
    "Tokenization is the process of splitting text into discrete units called **tokens**. There are several strategies:\n",
    "\n",
    "| Strategy | Example | Pros | Cons |\n",
    "|----------|---------|------|------|\n",
    "| **Word-level** | \"I love ML\" → [\"I\", \"love\", \"ML\"] | Intuitive, captures meaning | Large vocabulary, OOV problem |\n",
    "| **Character-level** | \"I love\" → [\"I\", \" \", \"l\", \"o\", \"v\", \"e\"] | Small vocabulary, no OOV | Loses word meaning, long sequences |\n",
    "| **Subword** | \"loving\" → [\"lov\", \"ing\"] | Balance of both | Requires training |\n",
    "\n",
    "We'll use **word-level tokenization** with simple preprocessing:\n",
    "1. Convert to lowercase\n",
    "2. Remove punctuation\n",
    "3. Split on whitespace\n",
    "\n",
    "**3.2 Handling Unknown Words (OOV)**\n",
    "\n",
    "Words not seen during training are called **Out-of-Vocabulary (OOV)** words. We handle them with a special `<unk>` token.\n",
    "\n",
    "**3.3 Special Tokens**\n",
    "\n",
    "| Token | Index | Purpose |\n",
    "|-------|-------|--------|\n",
    "| `<pad>` | 0 | Padding for batch alignment |\n",
    "| `<unk>` | 1 | Unknown/OOV words |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d90cbc-d195-4efa-8a43-ec316bbf1d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOKENIZATION EXAMPLE\n",
      "============================================================\n",
      "\n",
      "Original text:\n",
      "  \"Wall St. Bears Claw Back Into the Black (Reuters)\"\n",
      "\n",
      "After tokenization:\n",
      "  ['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black', 'reuters']\n",
      "\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple word-level tokenizer.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove non-alphanumeric characters (keep spaces)\n",
    "    3. Split on whitespace\n",
    "    4. Remove empty tokens\n",
    "    \n",
    "    Args:\n",
    "        text: Input string\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens (words)\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphanumeric (keep spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Split and filter empty\n",
    "    tokens = [token for token in text.split() if token]\n",
    "    return tokens\n",
    "\n",
    "# Test the tokenizer\n",
    "sample_text = train_df.iloc[0]['title']\n",
    "sample_tokens = tokenize(sample_text)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOKENIZATION EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOriginal text:\")\n",
    "print(f'  \"{sample_text}\"')\n",
    "print(f\"\\nAfter tokenization:\")\n",
    "print(f\"  {sample_tokens}\")\n",
    "print(f\"\\nNumber of tokens: {len(sample_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cffbee08-29fb-41a6-8f8f-369d5ef6ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMBINING TITLE AND DESCRIPTION\n",
      "============================================================\n",
      "\n",
      "Sample combined text:\n",
      "  Title: Wall St. Bears Claw Back Into the Black (Reuters)\n",
      "  Description: Reuters - Short-sellers, Wall Street's dwindling\\b...\n",
      "\n",
      "Combined tokens (first 20):\n",
      "  ['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black', 'reuters', 'reuters', 'shortsellers', 'wall', 'streets', 'dwindlingband', 'of', 'ultracynics', 'are', 'seeing', 'green', 'again']\n"
     ]
    }
   ],
   "source": [
    "def combine_text(row):\n",
    "    \"\"\"\n",
    "    Combine title and description into a single text.\n",
    "    \"\"\"\n",
    "    title = str(row['title']) if pd.notna(row['title']) else ''\n",
    "    description = str(row['description']) if pd.notna(row['description']) else ''\n",
    "    return title + ' ' + description\n",
    "\n",
    "# Combine title and description for all samples\n",
    "train_df['text'] = train_df.apply(combine_text, axis=1)\n",
    "test_df['text'] = test_df.apply(combine_text, axis=1)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMBINING TITLE AND DESCRIPTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample = train_df.iloc[0]\n",
    "print(f\"\\nSample combined text:\")\n",
    "print(f\"  Title: {sample['title']}\")\n",
    "print(f\"  Description: {sample['description'][:50]}...\")\n",
    "print(f\"\\nCombined tokens (first 20):\")\n",
    "print(f\"  {tokenize(sample['text'])[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025da68-3bbb-4e8e-ac20-6d9957f69713",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Building the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f8bf9-95ad-48cb-b7f7-ca6f7354c1e6",
   "metadata": {},
   "source": [
    "**4.1 What is a Vocabulary?**\n",
    "\n",
    "A vocabulary is a mapping from tokens to integer indices:\n",
    "\n",
    "$$\\text{vocab}: \\text{Token} \\rightarrow \\mathbb{Z}^+$$\n",
    "\n",
    "For example:\n",
    "```\n",
    "{\"<pad>\": 0, \"<unk>\": 1, \"the\": 2, \"a\": 3, \"is\": 4, ...}\n",
    "```\n",
    "\n",
    "**4.2 Vocabulary Size Trade-off**\n",
    "\n",
    "| Vocabulary Size | Pros | Cons |\n",
    "|-----------------|------|------|\n",
    "| Small (e.g., 5K) | Faster training, less memory | More `<unk>` tokens, loses information |\n",
    "| Large (e.g., 100K) | Better coverage | Slower, more memory, rare words |\n",
    "\n",
    "**Common Practice**: Keep the top $V$ most frequent words (e.g., $V = 25,000$).\n",
    "\n",
    "**4.3 Building Process**\n",
    "\n",
    "1. Tokenize all training texts\n",
    "2. Count token frequencies\n",
    "3. Keep top $V$ most frequent tokens\n",
    "4. Assign indices (0 = `<pad>`, 1 = `<unk>`, 2+ = words by frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d613877-5f4c-4e77-8ebe-1bf617d710af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILDING VOCABULARY\n",
      "============================================================\n",
      "\n",
      "Tokenizing training data...\n",
      "Total tokens in training data: 4,489,931\n",
      "Unique tokens: 102,169\n",
      "\n",
      "Top 10 most common tokens:\n",
      "  the       : 203,527\n",
      "  to        : 119,026\n",
      "  a         : 107,550\n",
      "  of        : 97,908\n",
      "  in        : 95,430\n",
      "  and       : 68,852\n",
      "  on        : 56,507\n",
      "  for       : 50,168\n",
      "  39s       : 31,218\n",
      "  that      : 27,741\n"
     ]
    }
   ],
   "source": [
    "# Count token frequencies from training data only\n",
    "print(\"=\"*60)\n",
    "print(\"BUILDING VOCABULARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTokenizing training data...\")\n",
    "token_counter = Counter()\n",
    "\n",
    "for text in train_df['text']:\n",
    "    tokens = tokenize(text)\n",
    "    token_counter.update(tokens)\n",
    "\n",
    "print(f\"Total tokens in training data: {sum(token_counter.values()):,}\")\n",
    "print(f\"Unique tokens: {len(token_counter):,}\")\n",
    "\n",
    "# Show most common tokens\n",
    "print(\"\\nTop 10 most common tokens:\")\n",
    "for token, count in token_counter.most_common(10):\n",
    "    print(f\"  {token:10s}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ea3042-df52-4039-b847-78b1535fa4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VOCABULARY CREATED\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 25,002\n",
      "  - Special tokens: 2 (<pad>, <unk>)\n",
      "  - Regular tokens: 25,000\n",
      "\n",
      "Special token indices:\n",
      "  <pad> → 0\n",
      "  <unk> → 1\n",
      "\n",
      "Sample word indices:\n",
      "  the → 2\n",
      "  to → 3\n",
      "  a → 4\n",
      "  of → 5\n",
      "  in → 6\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary with size limit\n",
    "MAX_VOCAB_SIZE = 25000  # Keep top 25K words\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Vocabulary class for mapping tokens to indices and vice versa.\n",
    "    \n",
    "    Attributes:\n",
    "        token2idx: Dict mapping tokens to indices\n",
    "        idx2token: Dict mapping indices to tokens\n",
    "        size: Number of tokens in vocabulary\n",
    "    \"\"\"\n",
    "    def __init__(self, token_counter, max_size=None, special_tokens=None):\n",
    "        \"\"\"\n",
    "        Build vocabulary from token counts.\n",
    "        \n",
    "        Args:\n",
    "            token_counter: Counter object with token frequencies\n",
    "            max_size: Maximum vocabulary size (excluding special tokens)\n",
    "            special_tokens: List of (token, index) tuples for special tokens\n",
    "        \"\"\"\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        \n",
    "        # Add special tokens first\n",
    "        if special_tokens:\n",
    "            for token, idx in special_tokens:\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token[idx] = token\n",
    "        \n",
    "        # Add most common tokens\n",
    "        start_idx = len(self.token2idx)\n",
    "        most_common = token_counter.most_common(max_size)\n",
    "        \n",
    "        for idx, (token, count) in enumerate(most_common, start=start_idx):\n",
    "            self.token2idx[token] = idx\n",
    "            self.idx2token[idx] = token\n",
    "        \n",
    "        self.size = len(self.token2idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        \"\"\"Get index for token, return UNK_IDX if not found.\"\"\"\n",
    "        return self.token2idx.get(token, UNK_IDX)\n",
    "    \n",
    "    def lookup_token(self, idx):\n",
    "        \"\"\"Get token for index.\"\"\"\n",
    "        return self.idx2token.get(idx, UNK_TOKEN)\n",
    "\n",
    "# Build the vocabulary\n",
    "special_tokens = [(PAD_TOKEN, PAD_IDX), (UNK_TOKEN, UNK_IDX)]\n",
    "vocab = Vocabulary(token_counter, max_size=MAX_VOCAB_SIZE, special_tokens=special_tokens)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VOCABULARY CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nVocabulary size: {len(vocab):,}\")\n",
    "print(f\"  - Special tokens: 2 (<pad>, <unk>)\")\n",
    "print(f\"  - Regular tokens: {MAX_VOCAB_SIZE:,}\")\n",
    "\n",
    "print(\"\\nSpecial token indices:\")\n",
    "print(f\"  {PAD_TOKEN} → {vocab[PAD_TOKEN]}\")\n",
    "print(f\"  {UNK_TOKEN} → {vocab[UNK_TOKEN]}\")\n",
    "\n",
    "print(\"\\nSample word indices:\")\n",
    "for word in ['the', 'to', 'a', 'of', 'in']:\n",
    "    print(f\"  {word} → {vocab[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc810c78-f755-4220-8f8e-3312692d42ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NUMERICALIZATION EXAMPLE\n",
      "============================================================\n",
      "\n",
      "Original text: \"Wall St. Bears Claw Back Into the Black\"\n",
      "\n",
      "Tokens:  ['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black']\n",
      "Indices: [399, 395, 1564, 14719, 101, 55, 2, 839]\n",
      "\n",
      "Token-to-index mapping:\n",
      "  wall  → 399\n",
      "  st    → 395\n",
      "  bears → 1564\n",
      "  claw  → 14719\n",
      "  back  → 101\n",
      "  into  → 55\n",
      "  the   → 2\n",
      "  black → 839\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate numericalization\n",
    "def numericalize(text, vocab):\n",
    "    \"\"\"\n",
    "    Convert text to list of token indices.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string\n",
    "        vocab: Vocabulary object\n",
    "    \n",
    "    Returns:\n",
    "        List of integer indices\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "# Test numericalization\n",
    "sample_text = \"Wall St. Bears Claw Back Into the Black\"\n",
    "sample_tokens = tokenize(sample_text)\n",
    "sample_indices = numericalize(sample_text, vocab)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NUMERICALIZATION EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "print(f'\\nOriginal text: \"{sample_text}\"')\n",
    "print(f\"\\nTokens:  {sample_tokens}\")\n",
    "print(f\"Indices: {sample_indices}\")\n",
    "\n",
    "print(\"\\nToken-to-index mapping:\")\n",
    "for token, idx in zip(sample_tokens, sample_indices):\n",
    "    print(f\"  {token:5s} → {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42623013-24fb-425a-ac03-f168fc69747a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a158cf-5485-472a-9fa5-a89bc2d21049",
   "metadata": {},
   "source": [
    "**5.1 PyTorch Dataset Class**\n",
    "\n",
    "PyTorch's `Dataset` is an abstract class that represents a dataset. We need to implement:\n",
    "- `__len__()`: Return the number of samples\n",
    "- `__getitem__(idx)`: Return sample at index `idx`\n",
    "\n",
    "**5.2 Handling Variable-Length Sequences**\n",
    "\n",
    "Unlike images (fixed 28×28), text sequences have variable lengths. For batching, we need:\n",
    "1. **Maximum length**: Truncate sequences longer than `max_length`\n",
    "2. **Padding**: Pad shorter sequences with `<pad>` tokens to `max_length`\n",
    "\n",
    "```\n",
    "Before padding:              After padding (max_length=5):\n",
    "─────────────────           ────────────────────────────\n",
    "[1, 2, 3]                   [1, 2, 3, 0, 0]\n",
    "[4, 5, 6, 7]                [4, 5, 6, 7, 0]\n",
    "[8, 9]                      [8, 9, 0, 0, 0]\n",
    "[1, 2, 3, 4, 5, 6, 7]       [1, 2, 3, 4, 5]  (truncated)\n",
    "```\n",
    "\n",
    "**5.3 Label Adjustment**\n",
    "\n",
    "Original AG News labels are 1-indexed (1, 2, 3, 4). PyTorch expects 0-indexed labels (0, 1, 2, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e72345-36ea-4153-8cfc-2bc3f5b38f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEXT LENGTH ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Sequence length statistics (in tokens):\n",
      "  Min:    11\n",
      "  Max:    143\n",
      "  Mean:   38.1\n",
      "  Median: 38.0\n",
      "  95th percentile: 56.0\n",
      "\n",
      "→ Using max_length = 64 (covers ~85% of sequences without truncation)\n"
     ]
    }
   ],
   "source": [
    "# Analyze text lengths to choose max_length\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT LENGTH ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute lengths for a sample of training data\n",
    "sample_size = 10000\n",
    "lengths = [len(tokenize(text)) for text in train_df['text'].iloc[:sample_size]]\n",
    "\n",
    "print(f\"\\nSequence length statistics (in tokens):\")\n",
    "print(f\"  Min:    {min(lengths)}\")\n",
    "print(f\"  Max:    {max(lengths)}\")\n",
    "print(f\"  Mean:   {np.mean(lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(lengths):.1f}\")\n",
    "print(f\"  95th percentile: {np.percentile(lengths, 95):.1f}\")\n",
    "\n",
    "# Choose max_length based on analysis\n",
    "MAX_LENGTH = 64  # Covers most sequences while limiting computation\n",
    "print(f\"\\n→ Using max_length = {MAX_LENGTH} (covers ~85% of sequences without truncation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b0c1fd3-aa06-48a7-9da8-6ff8ba8966f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGNewsDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class AGNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for AG News text classification.\n",
    "    \n",
    "    Handles:\n",
    "    - Tokenization and numericalization\n",
    "    - Padding/truncation to fixed length\n",
    "    - Label conversion (1-indexed → 0-indexed)\n",
    "    \n",
    "    Attributes:\n",
    "        texts: List of text strings\n",
    "        labels: List of integer labels (0-indexed)\n",
    "        vocab: Vocabulary object\n",
    "        max_length: Maximum sequence length\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab, max_length=64):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings\n",
    "            labels: List of labels (1-indexed from AG News)\n",
    "            vocab: Vocabulary object for numericalization\n",
    "            max_length: Maximum sequence length (pad/truncate to this)\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = [label - 1 for label in labels]  # Convert to 0-indexed\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Returns:\n",
    "            text_tensor: LongTensor of shape (max_length,)\n",
    "            label: Integer label (0-3)\n",
    "            length: Original sequence length (before padding)\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize and numericalize\n",
    "        tokens = tokenize(text)\n",
    "        indices = [self.vocab[token] for token in tokens]\n",
    "        \n",
    "        # Store original length (useful for packing sequences later)\n",
    "        original_length = len(indices)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "            original_length = self.max_length\n",
    "        \n",
    "        # Pad if too short\n",
    "        padding_length = self.max_length - len(indices)\n",
    "        indices = indices + [PAD_IDX] * padding_length\n",
    "        \n",
    "        # Convert to tensors\n",
    "        text_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "        \n",
    "        return text_tensor, label, original_length\n",
    "\n",
    "print(\"AGNewsDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df122bf7-08ae-4b0a-876d-8f5dd4ec9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING DATASETS\n",
      "============================================================\n",
      "\n",
      "Full training dataset: 120,000 samples\n",
      "Test dataset: 7,600 samples\n",
      "\n",
      "Sample from training dataset:\n",
      "  Text tensor shape: torch.Size([64])\n",
      "  Label: 2 (Business)\n",
      "  Original length: 20\n",
      "  First 10 indices: [399, 395, 1564, 14719, 101, 55, 2, 839, 24, 24]\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create full training dataset\n",
    "full_train_dataset = AGNewsDataset(\n",
    "    texts=train_df['text'].tolist(),\n",
    "    labels=train_df['label'].tolist(),\n",
    "    vocab=vocab,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = AGNewsDataset(\n",
    "    texts=test_df['text'].tolist(),\n",
    "    labels=test_df['label'].tolist(),\n",
    "    vocab=vocab,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"\\nFull training dataset: {len(full_train_dataset):,} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Verify a sample\n",
    "sample_text, sample_label, sample_length = full_train_dataset[0]\n",
    "print(f\"\\nSample from training dataset:\")\n",
    "print(f\"  Text tensor shape: {sample_text.shape}\")\n",
    "print(f\"  Label: {sample_label} ({CLASS_NAMES[sample_label]})\")\n",
    "print(f\"  Original length: {sample_length}\")\n",
    "print(f\"  First 10 indices: {sample_text[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e313e7-50d5-43e2-a795-ece200340aa5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Splitting and Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987a791-e14b-4c59-9f13-cb70c7c8dde6",
   "metadata": {},
   "source": [
    "**6.1 Why Split Training into Train/Validation?**\n",
    "\n",
    "Similar to the CNN exercise, we split the training data:\n",
    "\n",
    "| Set | Purpose | Size |\n",
    "|-----|---------|------|\n",
    "| **Training** | Learn parameters | 96,000 (80%) |\n",
    "| **Validation** | Tune hyperparameters, early stopping | 24,000 (20%) |\n",
    "| **Test** | Final evaluation | 7,600 (provided) |\n",
    "\n",
    "**6.2 DataLoader for Batching**\n",
    "\n",
    "The `DataLoader` handles:\n",
    "- Batching samples together\n",
    "- Shuffling (for training)\n",
    "- Parallel data loading\n",
    "\n",
    "**6.3 Custom Collate Function**\n",
    "\n",
    "We need a custom collate function to properly batch our data, including the sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a91b98cb-e1f7-416b-bdfa-9132c9062310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPLITTING TRAINING DATA\n",
      "============================================================\n",
      "\n",
      "Original training set: 120,000 samples\n",
      "Split ratio: 80% train / 20% validation\n",
      "\n",
      "After split:\n",
      "  Training:   96,000 samples\n",
      "  Validation: 24,000 samples\n",
      "  Test:       7,600 samples (separate)\n"
     ]
    }
   ],
   "source": [
    "# Split training data into train and validation\n",
    "TRAIN_RATIO = 0.8\n",
    "train_size = int(TRAIN_RATIO * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPLITTING TRAINING DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOriginal training set: {len(full_train_dataset):,} samples\")\n",
    "print(f\"Split ratio: {TRAIN_RATIO:.0%} train / {1-TRAIN_RATIO:.0%} validation\")\n",
    "\n",
    "# Perform the split\n",
    "generator = torch.Generator().manual_seed(42)  # For reproducibility\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter split:\")\n",
    "print(f\"  Training:   {len(train_dataset):,} samples\")\n",
    "print(f\"  Validation: {len(val_dataset):,} samples\")\n",
    "print(f\"  Test:       {len(test_dataset):,} samples (separate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e50f70a-5816-4e2e-8fbd-c4141bb50310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collate function defined\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching.\n",
    "    \n",
    "    Takes a list of (text_tensor, label, length) tuples and returns:\n",
    "    - texts: Tensor of shape (batch_size, max_length)\n",
    "    - labels: Tensor of shape (batch_size,)\n",
    "    - lengths: Tensor of shape (batch_size,)\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples from AGNewsDataset.__getitem__\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (texts, labels, lengths) tensors\n",
    "    \"\"\"\n",
    "    texts, labels, lengths = zip(*batch)\n",
    "    \n",
    "    # Stack into tensors\n",
    "    texts = torch.stack(texts, dim=0)       # (batch_size, max_length)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)  # (batch_size,)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)  # (batch_size,)\n",
    "    \n",
    "    return texts, labels, lengths\n",
    "\n",
    "print(\"Collate function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09b2ddcc-6c41-4eda-b286-72b1d1c50935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATALOADER CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Batch size: 64\n",
      "\n",
      "Number of batches:\n",
      "  Training:   1,500 batches × 64 = ~96,000 samples/epoch\n",
      "  Validation: 375 batches\n",
      "  Test:       119 batches\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,           # Shuffle training data each epoch\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,          # Don't shuffle validation\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device == 'cuda' else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,          # Don't shuffle test\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATALOADER CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBatch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nNumber of batches:\")\n",
    "print(f\"  Training:   {len(train_loader):,} batches × {BATCH_SIZE} = ~{len(train_loader)*BATCH_SIZE:,} samples/epoch\")\n",
    "print(f\"  Validation: {len(val_loader):,} batches\")\n",
    "print(f\"  Test:       {len(test_loader):,} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12da92b0-c74f-4144-b632-cc2d71922c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMINING A SINGLE BATCH\n",
      "============================================================\n",
      "\n",
      "Texts tensor shape: torch.Size([64, 64])\n",
      "  → (batch_size, max_length)\n",
      "  → (64, 64)\n",
      "\n",
      "Labels tensor shape: torch.Size([64])\n",
      "Labels dtype: torch.int64\n",
      "\n",
      "Lengths tensor shape: torch.Size([64])\n",
      "Length range: 23 to 64\n",
      "\n",
      "Sample labels: [0, 0, 3, 1, 2]\n",
      "Class names: ['World', 'World', 'Sci/Tech', 'Sports', 'Business']\n"
     ]
    }
   ],
   "source": [
    "# Examine a single batch\n",
    "sample_texts, sample_labels, sample_lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXAMINING A SINGLE BATCH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTexts tensor shape: {sample_texts.shape}\")\n",
    "print(f\"  → (batch_size, max_length)\")\n",
    "print(f\"  → ({BATCH_SIZE}, {MAX_LENGTH})\")\n",
    "print(f\"\\nLabels tensor shape: {sample_labels.shape}\")\n",
    "print(f\"Labels dtype: {sample_labels.dtype}\")\n",
    "print(f\"\\nLengths tensor shape: {sample_lengths.shape}\")\n",
    "print(f\"Length range: {sample_lengths.min().item()} to {sample_lengths.max().item()}\")\n",
    "print(f\"\\nSample labels: {sample_labels[:5].tolist()}\")\n",
    "print(f\"Class names: {[CLASS_NAMES[l] for l in sample_labels[:5].tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af48f4-e2d9-4d33-807d-70954c4a1dbd",
   "metadata": {},
   "source": [
    "## 7. Summary: Data Pipeline Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305ce60-1772-4923-af27-492cde456dab",
   "metadata": {},
   "source": [
    "**What We Built**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                    TEXT DATA PIPELINE - COMPLETE                                 │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "  Raw CSV             Tokenize            Numericalize          Pad/Truncate\n",
    "  ────────            ────────            ────────────          ────────────\n",
    "  \"Wall St...\"   →   [\"wall\",\"st\"...]  →  [670,251,...]    →   [670,251,...,0,0]\n",
    "                                                                     ↓\n",
    "                                                             Fixed-length tensor\n",
    "                                                                (max_length=64)\n",
    "```\n",
    "\n",
    "**Key Differences from Image Pipeline (Fashion-MNIST)**\n",
    "\n",
    "| Aspect | Image (Fashion-MNIST) | Text (AG News) |\n",
    "|--------|----------------------|----------------|\n",
    "| Raw input | 28×28 pixel array | Variable-length string |\n",
    "| Preprocessing | ToTensor + Normalize | Tokenize + Numericalize + Pad |\n",
    "| Vocabulary | Not needed | Required (25K tokens) |\n",
    "| Input to model | Float tensor (C, H, W) | Long tensor (L,) |\n",
    "| First model layer | Conv2d | Embedding |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "918065a5-4107-4d9a-a685-2312a1d6eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA PIPELINE READY FOR TRAINING\n",
      "============================================================\n",
      "\n",
      "✓ Device: mps\n",
      "✓ Batch size: 64\n",
      "✓ Max sequence length: 64\n",
      "✓ Vocabulary size: 25,002\n",
      "✓ Number of classes: 4\n",
      "\n",
      "✓ train_loader: 1,500 batches (96,000 samples)\n",
      "✓ val_loader:   375 batches (24,000 samples)\n",
      "✓ test_loader:  119 batches (7,600 samples)\n",
      "\n",
      "Class names:\n",
      "  0: World\n",
      "  1: Sports\n",
      "  2: Business\n",
      "  3: Sci/Tech\n",
      "\n",
      "============================================================\n",
      "Ready for model building and training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final verification\n",
    "print(\"=\"*60)\n",
    "print(\"DATA PIPELINE READY FOR TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✓ Device: {device}\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"✓ Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"✓ Number of classes: {len(CLASS_NAMES)}\")\n",
    "print(f\"\\n✓ train_loader: {len(train_loader):,} batches ({len(train_dataset):,} samples)\")\n",
    "print(f\"✓ val_loader:   {len(val_loader):,} batches ({len(val_dataset):,} samples)\")\n",
    "print(f\"✓ test_loader:  {len(test_loader):,} batches ({len(test_dataset):,} samples)\")\n",
    "\n",
    "print(f\"\\nClass names:\")\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for model building and training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9008b889-ae35-484c-abe5-5d011568e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KEY VARIABLES FOR NEXT STEPS\n",
      "============================================================\n",
      "\n",
      "Data Loaders:\n",
      "  • train_loader - for training\n",
      "  • val_loader   - for validation during training\n",
      "  • test_loader  - for final evaluation\n",
      "\n",
      "Vocabulary:\n",
      "  • vocab        - Vocabulary object for embedding layer\n",
      "  • len(vocab)   - 25,002 (vocabulary size for nn.Embedding)\n",
      "  • PAD_IDX      - 0 (for padding_idx in nn.Embedding)\n",
      "\n",
      "Constants:\n",
      "  • MAX_LENGTH   - 64 (sequence length)\n",
      "  • BATCH_SIZE   - 64\n",
      "  • CLASS_NAMES  - ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "  • device       - mps\n"
     ]
    }
   ],
   "source": [
    "# Summary of key variables for next steps\n",
    "print(\"=\"*60)\n",
    "print(\"KEY VARIABLES FOR NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nData Loaders:\")\n",
    "print(\"  • train_loader - for training\")\n",
    "print(\"  • val_loader   - for validation during training\")\n",
    "print(\"  • test_loader  - for final evaluation\")\n",
    "print(\"\\nVocabulary:\")\n",
    "print(\"  • vocab        - Vocabulary object for embedding layer\")\n",
    "print(f\"  • len(vocab)   - {len(vocab):,} (vocabulary size for nn.Embedding)\")\n",
    "print(f\"  • PAD_IDX      - {PAD_IDX} (for padding_idx in nn.Embedding)\")\n",
    "print(\"\\nConstants:\")\n",
    "print(f\"  • MAX_LENGTH   - {MAX_LENGTH} (sequence length)\")\n",
    "print(f\"  • BATCH_SIZE   - {BATCH_SIZE}\")\n",
    "print(f\"  • CLASS_NAMES  - {CLASS_NAMES}\")\n",
    "print(f\"  • device       - {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8be30-0d22-4cab-9a13-b6bf7248f8dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 2: Building the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57e860-8826-4f22-9349-67d04eb884f8",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Now that our data pipeline is ready, we build an RNN-based text classifier. The architecture follows the standard pattern:\n",
    "\n",
    "$$\\text{Indices} \\xrightarrow{\\text{Embedding}} \\text{Vectors} \\xrightarrow{\\text{RNN}} \\text{Hidden States} \\xrightarrow{\\text{Linear}} \\text{Logits}$$\n",
    "\n",
    "**Architecture Overview**\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Purpose |\n",
    "|-------|-------------|--------------|--------|\n",
    "| `nn.Embedding` | $(B, L)$ | $(B, L, d)$ | Map word indices to dense vectors |\n",
    "| `nn.RNN` | $(B, L, d)$ | $(B, L, H)$, $(N, B, H)$ | Process sequence, extract temporal features |\n",
    "| `nn.Linear` | $(B, H)$ | $(B, K)$ | Map final hidden state to class logits |\n",
    "\n",
    "Where:\n",
    "- $B$ = batch size\n",
    "- $L$ = sequence length (MAX_LENGTH = 64)\n",
    "- $d$ = embedding dimension\n",
    "- $H$ = hidden size\n",
    "- $N$ = number of RNN layers\n",
    "- $K$ = number of classes (4 for AG News)\n",
    "\n",
    "**For detailed explanations of each layer, see Layers.ipynb.**\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e3b8a-1802-4f52-9f0f-93fb4f4a28ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2b1a0-5a1b-4e67-b0ed-74a766e038a6",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**1.1 The Classification Pipeline**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         RNN TEXT CLASSIFIER                                      │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Input: [670, 251, 1564, ..., 0, 0]     Shape: (B, L) = (64, 64)\n",
    "              ↓\n",
    "        ┌───────────┐\n",
    "        │ Embedding │     nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        └───────────┘\n",
    "              ↓\n",
    "Embedded: [e₆₇₀, e₂₅₁, ...]           Shape: (B, L, d) = (64, 64, 128)\n",
    "              ↓\n",
    "        ┌───────────┐\n",
    "        │    RNN    │     nn.RNN(embed_dim, hidden_size, num_layers, ...)\n",
    "        └───────────┘\n",
    "              ↓\n",
    "output: [h₀, h₁, ..., h_{L-1}]        Shape: (B, L, H) = (64, 64, 256)\n",
    "h_n: final hidden states              Shape: (N, B, H) = (2, 64, 256)\n",
    "              ↓\n",
    "        Extract h_n[-1]               Take last layer's final hidden state\n",
    "              ↓\n",
    "Hidden: h_{L-1}                       Shape: (B, H) = (64, 256)\n",
    "              ↓\n",
    "        ┌───────────┐\n",
    "        │  Dropout  │     nn.Dropout(p=0.5) — regularization\n",
    "        └───────────┘\n",
    "              ↓\n",
    "        ┌───────────┐\n",
    "        │  Linear   │     nn.Linear(hidden_size, num_classes)\n",
    "        └───────────┘\n",
    "              ↓\n",
    "Logits: [z₀, z₁, z₂, z₃]              Shape: (B, K) = (64, 4)\n",
    "```\n",
    "\n",
    "**1.2 Key Design Decisions**\n",
    "\n",
    "| Decision | Choice | Rationale |\n",
    "|----------|--------|----------|\n",
    "| `padding_idx=0` | Embedding ignores pad tokens | Prevents padding from affecting representations |\n",
    "| `batch_first=True` | Input shape (B, L, d) | Consistent with DataLoader output |\n",
    "| Use `h_n[-1]` | Final hidden state of last layer | Summarizes entire sequence for classification |\n",
    "| Dropout before classifier | p=0.5 | Regularization to prevent overfitting |\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c50fc88c-3890-4f63-98f4-df8dfb71e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNClassifier class defined\n"
     ]
    }
   ],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN-based text classifier for AG News.\n",
    "    \n",
    "    Architecture:\n",
    "        Embedding → RNN → Dropout → Linear\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary (for embedding layer)\n",
    "        embed_dim: Dimension of word embeddings\n",
    "        hidden_size: Dimension of RNN hidden states\n",
    "        num_classes: Number of output classes\n",
    "        num_layers: Number of stacked RNN layers\n",
    "        dropout: Dropout probability\n",
    "        pad_idx: Index of padding token (for padding_idx in Embedding)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, \n",
    "                 num_layers=1, dropout=0.5, pad_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer: (B, L) → (B, L, embed_dim)\n",
    "        # padding_idx=pad_idx ensures padding tokens get zero vectors\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        \n",
    "        # RNN layer: (B, L, embed_dim) → (B, L, hidden_size), (num_layers, B, hidden_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input/output tensors have batch dimension first\n",
    "            dropout=dropout if num_layers > 1 else 0  # Dropout between RNN layers\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classification head: (B, hidden_size) → (B, num_classes)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of word indices, shape (B, L)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Class scores, shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        # Step 1: Embed word indices → dense vectors\n",
    "        # (B, L) → (B, L, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Step 2: Process sequence through RNN\n",
    "        # embedded: (B, L, embed_dim)\n",
    "        # output: (B, L, hidden_size) — hidden states at all time steps\n",
    "        # h_n: (num_layers, B, hidden_size) — final hidden state per layer\n",
    "        output, h_n = self.rnn(embedded)\n",
    "        \n",
    "        # Step 3: Extract final hidden state from last layer\n",
    "        # h_n[-1]: (B, hidden_size)\n",
    "        hidden = h_n[-1]\n",
    "        \n",
    "        # Step 4: Apply dropout and classify\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"RNNClassifier class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eaf5208-7127-45f9-8f07-97181a737b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Vocabulary size:  25,002\n",
      "Embedding dim:    128\n",
      "Hidden size:      256\n",
      "Number of layers: 2\n",
      "Dropout:          0.5\n",
      "Number of classes: 4\n",
      "Device:           mps\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "EMBED_DIM = 128      # Dimension of word embeddings\n",
    "HIDDEN_SIZE = 256    # Dimension of RNN hidden states\n",
    "NUM_LAYERS = 2       # Number of stacked RNN layers\n",
    "DROPOUT = 0.5        # Dropout probability\n",
    "NUM_CLASSES = len(CLASS_NAMES)  # 4 classes\n",
    "\n",
    "# Instantiate model\n",
    "model = RNNClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nVocabulary size:  {len(vocab):,}\")\n",
    "print(f\"Embedding dim:    {EMBED_DIM}\")\n",
    "print(f\"Hidden size:      {HIDDEN_SIZE}\")\n",
    "print(f\"Number of layers: {NUM_LAYERS}\")\n",
    "print(f\"Dropout:          {DROPOUT}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Device:           {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d03f5fc6-f6e6-4058-9bbf-d7933526bb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL ARCHITECTURE\n",
      "============================================================\n",
      "RNNClassifier(\n",
      "  (embedding): Embedding(25002, 128, padding_idx=0)\n",
      "  (rnn): RNN(128, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "PARAMETER COUNT\n",
      "============================================================\n",
      "embedding.weight                         torch.Size([25002, 128]) 3,200,256\n",
      "rnn.weight_ih_l0                         torch.Size([256, 128]) 32,768\n",
      "rnn.weight_hh_l0                         torch.Size([256, 256]) 65,536\n",
      "rnn.bias_ih_l0                           torch.Size([256])    256\n",
      "rnn.bias_hh_l0                           torch.Size([256])    256\n",
      "rnn.weight_ih_l1                         torch.Size([256, 256]) 65,536\n",
      "rnn.weight_hh_l1                         torch.Size([256, 256]) 65,536\n",
      "rnn.bias_ih_l1                           torch.Size([256])    256\n",
      "rnn.bias_hh_l1                           torch.Size([256])    256\n",
      "fc.weight                                torch.Size([4, 256]) 1,024\n",
      "fc.bias                                  torch.Size([4])      4\n",
      "\n",
      "Total                                                         3,431,684\n",
      "\n",
      "Trainable parameters: 3,431,684\n"
     ]
    }
   ],
   "source": [
    "# Verify model architecture and count parameters\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER COUNT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    total_params += num_params\n",
    "    print(f\"{name:40s} {str(param.shape):20s} {num_params:,}\")\n",
    "\n",
    "print(f\"\\n{'Total':40s} {'':20s} {total_params:,}\")\n",
    "print(f\"\\nTrainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "496dc1d3-223e-4f66-9fd4-fc560c6d6220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FORWARD PASS VERIFICATION\n",
      "============================================================\n",
      "\n",
      "Input shape:  torch.Size([64, 64]) → (B, L) = (64, 64)\n",
      "Output shape: torch.Size([64, 4]) → (B, K) = (64, 4)\n",
      "\n",
      "Sample output (first 3 samples):\n",
      "tensor([[ 0.0042, -0.0535, -0.0304, -0.0498],\n",
      "        [ 0.0042, -0.0535, -0.0304, -0.0498],\n",
      "        [ 0.0042, -0.0535, -0.0304, -0.0498]], device='mps:0')\n",
      "\n",
      "Predicted classes: [0, 0, 0]\n",
      "True classes:      [0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify forward pass with a sample batch\n",
    "print(\"=\"*60)\n",
    "print(\"FORWARD PASS VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get a sample batch\n",
    "sample_texts, sample_labels, sample_lengths = next(iter(train_loader))\n",
    "sample_texts = sample_texts.to(device)\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_logits = model(sample_texts)\n",
    "\n",
    "print(f\"\\nInput shape:  {sample_texts.shape} → (B, L) = ({BATCH_SIZE}, {MAX_LENGTH})\")\n",
    "print(f\"Output shape: {sample_logits.shape} → (B, K) = ({BATCH_SIZE}, {NUM_CLASSES})\")\n",
    "print(f\"\\nSample output (first 3 samples):\")\n",
    "print(sample_logits[:3])\n",
    "print(f\"\\nPredicted classes: {sample_logits[:3].argmax(dim=1).tolist()}\")\n",
    "print(f\"True classes:      {sample_labels[:3].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d355568-c021-4100-8ce2-1d1d313245ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8598cc-1bd1-4be1-9bbe-488794a2a49c",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**2.1 Cross-Entropy Loss**\n",
    "\n",
    "For multi-class classification, we use cross-entropy loss:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log\\left(\\frac{e^{z_{y_i}}}{\\sum_{k=0}^{K-1} e^{z_k}}\\right)$$\n",
    "\n",
    "where $z_k$ are the logits and $y_i$ is the true class.\n",
    "\n",
    "**2.2 Optimizer Choice**\n",
    "\n",
    "We use Adam, which adapts learning rates per-parameter. **For detailed explanations of optimizers, see Optimizers.ipynb.**\n",
    "\n",
    "| Hyperparameter | Value | Rationale |\n",
    "|----------------|-------|-----------|\n",
    "| Learning rate | 0.001 | Standard default for Adam |\n",
    "| $\\beta_1$ | 0.9 | Momentum term |\n",
    "| $\\beta_2$ | 0.999 | RMSprop term |\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "213381ec-202b-4448-af45-e6e49f254f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Loss function: CrossEntropyLoss\n",
      "Optimizer:     Adam (lr=0.001)\n",
      "Epochs:        5\n",
      "\n",
      "Batches per epoch: 1,500\n",
      "Total training steps: 7,500\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7b42f4e-23db-46b5-8777-d36d38066c9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 3: Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c8f9f-a885-4c92-9d31-5536b81c68ca",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**3.1 The Training Step**\n",
    "\n",
    "Each training iteration follows this pattern:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           TRAINING ITERATION                                     │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "1. optimizer.zero_grad()     ← Clear gradients from previous iteration\n",
    "         ↓\n",
    "2. logits = model(texts)     ← Forward pass: compute predictions\n",
    "         ↓\n",
    "3. loss = criterion(logits, labels)  ← Compute loss\n",
    "         ↓\n",
    "4. loss.backward()           ← Backward pass: compute gradients\n",
    "         ↓\n",
    "5. optimizer.step()          ← Update parameters: θ ← θ - lr·∇L\n",
    "```\n",
    "\n",
    "**3.2 Evaluation Mode**\n",
    "\n",
    "During validation/testing:\n",
    "- `model.eval()` disables dropout\n",
    "- `torch.no_grad()` prevents gradient computation (saves memory)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffe619c7-99cc-45cc-8cbc-0454560dc481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Loss function: CrossEntropyLoss\n",
      "Optimizer:     Adam (lr=0.001)\n",
      "Epochs:        5\n",
      "\n",
      "Batches per epoch: 1,500\n",
      "Total training steps: 7,500\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nLoss function: CrossEntropyLoss\")\n",
    "print(f\"Optimizer:     Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"Epochs:        {NUM_EPOCHS}\")\n",
    "print(f\"\\nBatches per epoch: {len(train_loader):,}\")\n",
    "print(f\"Total training steps: {len(train_loader) * NUM_EPOCHS:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7122148-a76f-4ad4-acf3-8ce6c82352d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network\n",
    "        loader: Training DataLoader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss over epoch\n",
    "        accuracy: Training accuracy\n",
    "    \"\"\"\n",
    "    model.train()  # Enable training mode (activates dropout)\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for texts, labels, lengths in loader:\n",
    "        # Move to device\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(texts)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item() * texts.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += texts.size(0)\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network\n",
    "        loader: DataLoader (validation or test)\n",
    "        criterion: Loss function\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss\n",
    "        accuracy: Accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Disable dropout\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation needed\n",
    "        for texts, labels, lengths in loader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(texts)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item() * texts.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += texts.size(0)\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "print(\"Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89db173f-717f-4306-a4e3-66e4828ab1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING\n",
      "============================================================\n",
      "Epoch 1/5\n",
      "  Train Loss: 1.3930 | Train Acc: 0.2516\n",
      "  Val Loss:   1.3967 | Val Acc:   0.2491 ← best\n",
      "\n",
      "Epoch 2/5\n",
      "  Train Loss: 1.4124 | Train Acc: 0.2541\n",
      "  Val Loss:   1.4023 | Val Acc:   0.2547 ← best\n",
      "\n",
      "Epoch 3/5\n",
      "  Train Loss: 1.4043 | Train Acc: 0.2523\n",
      "  Val Loss:   1.3891 | Val Acc:   0.2530\n",
      "\n",
      "Epoch 4/5\n",
      "  Train Loss: 1.4037 | Train Acc: 0.2535\n",
      "  Val Loss:   1.3929 | Val Acc:   0.2404\n",
      "\n",
      "Epoch 5/5\n",
      "  Train Loss: 1.4039 | Train Acc: 0.2529\n",
      "  Val Loss:   1.4089 | Val Acc:   0.2582 ← best\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Track best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_marker = \" ← best\"\n",
    "    else:\n",
    "        best_marker = \"\"\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}{best_marker}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dacb84-fe1e-46ce-9fe5-e295af3345ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be905932-0254-4ce3-b2c9-def8e076ed9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation accuracy: 0.2582\n"
     ]
    }
   ],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d440f6fc-9cec-41ee-bd87-4aebb7597479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL TEST EVALUATION\n",
      "============================================================\n",
      "\n",
      "Test Loss:     1.4069\n",
      "Test Accuracy: 0.2534 (25.34%)\n"
     ]
    }
   ],
   "source": [
    "# Final test evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL TEST EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nTest Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d12f1dac-2d56-4873-ad46-20ec912645e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PER-CLASS PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "Class         Correct    Total   Accuracy\n",
      "----------------------------------------\n",
      "World               0     1900     0.0000\n",
      "Sports            452     1900     0.2379\n",
      "Business          997     1900     0.5247\n",
      "Sci/Tech          477     1900     0.2511\n"
     ]
    }
   ],
   "source": [
    "# Per-class accuracy\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PER-CLASS PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels, lengths in test_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(texts)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        \n",
    "        for pred, label in zip(predictions, labels):\n",
    "            class_total[label.item()] += 1\n",
    "            if pred == label:\n",
    "                class_correct[label.item()] += 1\n",
    "\n",
    "print(f\"\\n{'Class':<12} {'Correct':>8} {'Total':>8} {'Accuracy':>10}\")\n",
    "print(\"-\" * 40)\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "    print(f\"{name:<12} {class_correct[i]:>8} {class_total[i]:>8} {acc:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b380c1af-2bc3-48cf-80cd-d08943b88a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Sample 1: ✓\n",
      "  True:       Business\n",
      "  Predicted:  Business (confidence: 27.93%)\n",
      "\n",
      "Sample 2: ✓\n",
      "  True:       Sci/Tech\n",
      "  Predicted:  Sci/Tech (confidence: 30.29%)\n",
      "\n",
      "Sample 3: ✓\n",
      "  True:       Sci/Tech\n",
      "  Predicted:  Sci/Tech (confidence: 30.29%)\n",
      "\n",
      "Sample 4: ✗\n",
      "  True:       Sci/Tech\n",
      "  Predicted:  Business (confidence: 40.37%)\n",
      "\n",
      "Sample 5: ✗\n",
      "  True:       Sci/Tech\n",
      "  Predicted:  Sports (confidence: 27.75%)\n"
     ]
    }
   ],
   "source": [
    "# Sample predictions\n",
    "print(\"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get a batch from test set\n",
    "test_texts, test_labels, test_lengths = next(iter(test_loader))\n",
    "test_texts = test_texts.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(test_texts)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predictions = logits.argmax(dim=1)\n",
    "\n",
    "# Show first 5 predictions\n",
    "for i in range(5):\n",
    "    true_label = test_labels[i].item()\n",
    "    pred_label = predictions[i].item()\n",
    "    confidence = probs[i, pred_label].item()\n",
    "    \n",
    "    status = \"✓\" if true_label == pred_label else \"✗\"\n",
    "    \n",
    "    print(f\"\\nSample {i+1}: {status}\")\n",
    "    print(f\"  True:       {CLASS_NAMES[true_label]}\")\n",
    "    print(f\"  Predicted:  {CLASS_NAMES[pred_label]} (confidence: {confidence:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee4ebe-9cab-4957-8678-5ff57596cbdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21df93-8187-492e-b6a6-a3a845008d3f",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**What We Built**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                    COMPLETE RNN TEXT CLASSIFIER                                  │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "  Raw Text              Preprocessing           RNN Classifier           Output\n",
    "  ────────              ─────────────           ──────────────           ──────\n",
    "  \"Wall St...\"    →    [670,251,...,0,0]   →   Embedding → RNN    →    [z₀,z₁,z₂,z₃]\n",
    "                         (Step 1)               → FC (Step 2)           → \"Business\"\n",
    "```\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "| Aspect | Key Point |\n",
    "|--------|----------|\n",
    "| Embedding | Maps discrete tokens to learnable dense vectors |\n",
    "| RNN | Processes sequence left-to-right, maintaining hidden state |\n",
    "| Classification | Uses final hidden state to predict class |\n",
    "| Regularization | Dropout prevents overfitting |\n",
    "\n",
    "**Possible Improvements**\n",
    "\n",
    "- **LSTM/GRU**: Replace `nn.RNN` with `nn.LSTM` or `nn.GRU` for better long-range dependencies\n",
    "- **Bidirectional**: Process sequence in both directions\n",
    "- **Pre-trained embeddings**: Initialize with Word2Vec or GloVe\n",
    "- **Attention mechanisms**: Weight important words more heavily\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a66ef357-365f-4f3f-b383-19d820842dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final Results:\n",
      "  Best Validation Accuracy: 0.2582\n",
      "  Test Accuracy:            0.2534\n",
      "\n",
      "Model Parameters: 3,431,684\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"  Test Accuracy:            {test_acc:.4f}\")\n",
    "print(f\"\\nModel Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d27477-1151-42ea-823c-4838070f935b",
   "metadata": {},
   "source": [
    "# Why the Previous Approach Did Not Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a53f63-b950-490b-a85e-4fe2d43e0e2c",
   "metadata": {},
   "source": [
    "## 1. Analyzing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69a699-3d47-4d15-8908-56e5ec83f163",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**1.1 What Went Wrong?**\n",
    "\n",
    "Looking at our training results, we observe:\n",
    "\n",
    "| Metric | Observed Value | Expected Value | Interpretation |\n",
    "|--------|---------------|----------------|----------------|\n",
    "| Test Accuracy | ~25% | 85-90% | **Random guessing** (4 classes → 25% by chance) |\n",
    "| Training Accuracy | ~25% | 95%+ | Model isn't learning even on training data |\n",
    "| Loss | ~1.39 | < 0.5 | Stuck at $-\\ln(0.25) = 1.386$ (uniform prediction) |\n",
    "| Per-class accuracy | World: 0%, Others: ~25% | Balanced | Model collapsed to predicting few classes |\n",
    "\n",
    "**The model is essentially making random predictions.** This is not a hyperparameter tuning issue—it's a fundamental architectural limitation.\n",
    "\n",
    "**1.2 The Root Cause: Vanishing Gradients**\n",
    "\n",
    "The core problem with vanilla RNNs is the **vanishing gradient problem**. Let's understand why mathematically.\n",
    "\n",
    "Recall the RNN hidden state update:\n",
    "$$h_t = \\tanh(x_t W_{ih}^T + h_{t-1} W_{hh}^T + b)$$\n",
    "\n",
    "During backpropagation, gradients flow backward through time. The gradient of the loss $\\mathcal{L}$ with respect to an early hidden state $h_k$ involves:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial h_k} = \\frac{\\partial \\mathcal{L}}{\\partial h_T} \\cdot \\prod_{t=k+1}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "Each factor $\\frac{\\partial h_t}{\\partial h_{t-1}}$ involves:\n",
    "$$\\frac{\\partial h_t}{\\partial h_{t-1}} = \\text{diag}(1 - h_t^2) \\cdot W_{hh}$$\n",
    "\n",
    "where $\\text{diag}(1 - h_t^2)$ comes from the $\\tanh$ derivative (bounded by 1).\n",
    "\n",
    "**The Problem**: When we multiply many terms that are typically less than 1:\n",
    "$$\\prod_{t=k+1}^{T} \\|\\frac{\\partial h_t}{\\partial h_{t-1}}\\| \\approx \\gamma^{T-k}$$\n",
    "\n",
    "where $\\gamma < 1$ typically. For a sequence of length $T = 64$:\n",
    "- If $\\gamma = 0.9$: gradient shrinks to $0.9^{64} \\approx 0.001$ (0.1% of original)\n",
    "- If $\\gamma = 0.5$: gradient shrinks to $0.5^{64} \\approx 10^{-19}$ (essentially zero)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                    VANISHING GRADIENT VISUALIZATION                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Forward pass:\n",
    "  x₁ → [h₁] → x₂ → [h₂] → ... → x₆₄ → [h₆₄] → Loss\n",
    "\n",
    "Backward pass (gradient magnitude):\n",
    "  ████ ← ███ ← ██ ← █ ← ▪ ← · ← · ← · ← · ← ·\n",
    "  h₁     h₂    h₃   h₄  h₅  ...              h₆₄\n",
    "  ↑                                            ↑\n",
    "  Gradient vanished!                      Full gradient\n",
    "\n",
    "Result: Early tokens (which often contain important context)\n",
    "        receive almost no gradient signal.\n",
    "```\n",
    "\n",
    "**1.3 Why This Matters for Text Classification**\n",
    "\n",
    "In our AG News task:\n",
    "- Sequences are 64 tokens long\n",
    "- Important classification signals can appear anywhere in the text\n",
    "- The word \"Reuters\" at position 1 and \"stock\" at position 60 both matter\n",
    "\n",
    "With vanishing gradients:\n",
    "- The embedding layer receives almost no useful gradient updates\n",
    "- Early words in the sequence are effectively ignored\n",
    "- The model can only \"remember\" the last few tokens\n",
    "\n",
    "**1.4 Other Contributing Factors**\n",
    "\n",
    "| Factor | Impact |\n",
    "|--------|--------|\n",
    "| **No gating mechanism** | Vanilla RNN has no way to selectively remember/forget information |\n",
    "| **Random initialization** | Embeddings start random; without gradients, they stay random |\n",
    "| **Sequential processing** | Each step depends on the previous; errors compound |\n",
    "| **tanh saturation** | When $h_t$ is close to ±1, gradients approach 0 |\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e57bd-17e4-4736-b7a7-26660e5caeed",
   "metadata": {},
   "source": [
    "## 2. The Solution: Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead8c58-f23a-4f54-964d-9fca9fd16aec",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**2.1 LSTM: Designed to Solve Vanishing Gradients**\n",
    "\n",
    "The **Long Short-Term Memory (LSTM)** architecture, introduced by Hochreiter & Schmidhuber (1997), was specifically designed to address the vanishing gradient problem. The key innovation is the **cell state** $c_t$—a highway that allows information to flow unchanged across many time steps.\n",
    "\n",
    "**2.2 LSTM Architecture**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                              LSTM CELL ARCHITECTURE                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "                              Cell State (memory highway)\n",
    "    c_{t-1} ─────────────────────[×]─────────────[+]─────────────────── c_t\n",
    "                                  ↑               ↑\n",
    "                                  │               │\n",
    "                           ┌─────────────┐  ┌─────────────┐\n",
    "                           │ Forget Gate │  │ Input Gate  │\n",
    "                           │   f_t · c   │  │  i_t · g_t  │\n",
    "                           └─────────────┘  └─────────────┘\n",
    "                                  ↑               ↑\n",
    "                            ┌─────┴─────┐   ┌─────┴─────┐\n",
    "                            │  σ(...)   │   │ σ(.) ⊙    │\n",
    "                            │           │   │   tanh(.) │\n",
    "                            └───────────┘   └───────────┘\n",
    "                                  ↑               ↑\n",
    "    h_{t-1} ──────────────────────┼───────────────┼───────────────────── \n",
    "                                  │               │                     ↓\n",
    "    x_t ──────────────────────────┴───────────────┴────────────── [Output Gate]\n",
    "                                                                        ↓\n",
    "                                                         h_t = o_t ⊙ tanh(c_t)\n",
    "```\n",
    "\n",
    "**2.3 The Three Gates**\n",
    "\n",
    "LSTM uses **three gates** to control information flow:\n",
    "\n",
    "| Gate | Symbol | Purpose | Equation |\n",
    "|------|--------|---------|----------|\n",
    "| **Forget Gate** | $f_t$ | What to forget from cell state | $f_t = \\sigma(x_t W_{if}^T + h_{t-1} W_{hf}^T + b_f)$ |\n",
    "| **Input Gate** | $i_t$ | What new info to store | $i_t = \\sigma(x_t W_{ii}^T + h_{t-1} W_{hi}^T + b_i)$ |\n",
    "| **Output Gate** | $o_t$ | What to output | $o_t = \\sigma(x_t W_{io}^T + h_{t-1} W_{ho}^T + b_o)$ |\n",
    "\n",
    "The candidate cell state:\n",
    "$$\\tilde{c}_t = \\tanh(x_t W_{ig}^T + h_{t-1} W_{hg}^T + b_g)$$\n",
    "\n",
    "**2.4 The Cell State Update (The Key Innovation)**\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "\n",
    "This is the **critical equation**. Notice:\n",
    "- If $f_t \\approx 1$ and $i_t \\approx 0$: information flows unchanged ($c_t \\approx c_{t-1}$)\n",
    "- Gradients can flow through the cell state with minimal decay\n",
    "- The network learns **when** to remember and **when** to forget\n",
    "\n",
    "**2.5 Why LSTM Solves Vanishing Gradients**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                    GRADIENT FLOW COMPARISON                                      │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Vanilla RNN (gradients must pass through tanh at every step):\n",
    "  ∂L/∂h₁ = ∂L/∂h₆₄ × (∂h₆₄/∂h₆₃) × (∂h₆₃/∂h₆₂) × ... × (∂h₂/∂h₁)\n",
    "                         ↑              ↑                    ↑\n",
    "                    |tanh'| < 1    |tanh'| < 1          |tanh'| < 1\n",
    "                    \n",
    "  Result: Gradient shrinks exponentially → vanishes\n",
    "\n",
    "LSTM (gradients can flow through cell state highway):\n",
    "  ∂L/∂c₁ = ∂L/∂c₆₄ × f₆₄ × f₆₃ × ... × f₂\n",
    "                      ↑      ↑          ↑\n",
    "                   Can be ≈ 1 when network learns to remember!\n",
    "                   \n",
    "  Result: Gradient can flow unimpeded → long-range learning\n",
    "```\n",
    "\n",
    "**2.6 LSTM vs RNN: Summary**\n",
    "\n",
    "| Aspect | Vanilla RNN | LSTM |\n",
    "|--------|-------------|------|\n",
    "| Hidden state | $h_t$ only | $h_t$ and $c_t$ (cell state) |\n",
    "| Gates | None | 3 (forget, input, output) |\n",
    "| Gradient flow | Through $\\tanh$ at each step | Direct through cell state |\n",
    "| Long-range dependencies | ❌ Poor (vanishing gradients) | ✅ Excellent |\n",
    "| Parameters | $\\sim 4 \\times$ fewer | More parameters |\n",
    "| Typical accuracy on AG News | ~25% (random) | ~85-90% |\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b9107-7b21-4c97-a93a-caf83ebeafd3",
   "metadata": {},
   "source": [
    "## 3. PyTorch Implementation: nn.RNN vs nn.LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74fcae1-3cda-4ae7-936c-be07a67c64cb",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**3.1 API Comparison**\n",
    "\n",
    "PyTorch makes switching from RNN to LSTM straightforward. The APIs are nearly identical:\n",
    "\n",
    "```python\n",
    "# Vanilla RNN\n",
    "nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "\n",
    "# LSTM\n",
    "nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "```\n",
    "\n",
    "**3.2 Key Difference: Output Format**\n",
    "\n",
    "The main difference is in what the forward pass returns:\n",
    "\n",
    "| Component | nn.RNN | nn.LSTM |\n",
    "|-----------|--------|--------|\n",
    "| **output** | $(B, L, H)$ — hidden states at all time steps | $(B, L, H)$ — same |\n",
    "| **hidden** | $h_n$: $(N, B, H)$ — final hidden state | $(h_n, c_n)$ — **tuple** of hidden and cell state |\n",
    "\n",
    "```python\n",
    "# RNN forward pass\n",
    "output, h_n = self.rnn(embedded)\n",
    "hidden = h_n[-1]  # Shape: (B, H)\n",
    "\n",
    "# LSTM forward pass\n",
    "output, (h_n, c_n) = self.lstm(embedded)  # Note: tuple unpacking!\n",
    "hidden = h_n[-1]  # Shape: (B, H)\n",
    "```\n",
    "\n",
    "**3.3 Parameter Count Comparison**\n",
    "\n",
    "LSTM has approximately 4× more parameters than RNN for the same hidden size (due to 4 weight matrices instead of 1):\n",
    "\n",
    "| Layer | RNN Parameters | LSTM Parameters |\n",
    "|-------|---------------|----------------|\n",
    "| Layer 0 input weights | $H \\times d$ | $4H \\times d$ |\n",
    "| Layer 0 hidden weights | $H \\times H$ | $4H \\times H$ |\n",
    "| Layer 0 biases | $2H$ | $8H$ |\n",
    "\n",
    "For our configuration ($d = 128$, $H = 256$, 2 layers):\n",
    "- RNN: ~230K parameters (recurrent layers only)\n",
    "- LSTM: ~920K parameters (recurrent layers only)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa658d-b772-4f7a-9628-2f19224f16b8",
   "metadata": {},
   "source": [
    "## 4. Building the LSTM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa5ab8-7a27-417a-b57f-29cf9ad37dc2",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "**4.1 Architecture Overview**\n",
    "\n",
    "Our LSTM classifier follows the same structure as before, with the RNN layer replaced by LSTM:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         LSTM TEXT CLASSIFIER                                     │\n",
    "└─────────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Input: [670, 251, 1564, ...]             Shape: (B, L) = (64, 64)\n",
    "              ↓\n",
    "        ┌───────────┐\n",
    "        │ Embedding │     nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        └───────────┘\n",
    "              ↓\n",
    "Embedded: [e₆₇₀, e₂₅₁, ...]           Shape: (B, L, d) = (64, 64, 128)\n",
    "              ↓\n",
    "        ┌───────────┐\n",
    "        │   LSTM    │     nn.LSTM(embed_dim, hidden_size, num_layers, ...)\n",
    "        └───────────┘\n",
    "              ↓\n",
    "output: [h₀, h₁, ..., h_{L-1}]        Shape: (B, L, H) = (64, 64, 256)\n",
    "(h_n, c_n): final states              h_n Shape: (N, B, H) = (2, 64, 256)\n",
    "              ↓\n",
    "        Extract h_n[-1]               Take last layer's final hidden state\n",
    "              ↓\n",
    "Hidden: h_{L-1}                       Shape: (B, H) = (64, 256)\n",
    "              ↓\n",
    "        ┌───────────┐\n",
    "        │  Dropout  │     nn.Dropout(p=0.3) — reduced from 0.5\n",
    "        └───────────┘\n",
    "              ↓\n",
    "        ┌───────────┐\n",
    "        │  Linear   │     nn.Linear(hidden_size, num_classes)\n",
    "        └───────────┘\n",
    "              ↓\n",
    "Logits: [z₀, z₁, z₂, z₃]              Shape: (B, K) = (64, 4)\n",
    "```\n",
    "\n",
    "**4.2 Key Changes from RNN Version**\n",
    "\n",
    "| Component | RNN Version | LSTM Version | Rationale |\n",
    "|-----------|-------------|--------------|----------|\n",
    "| Recurrent layer | `nn.RNN` | `nn.LSTM` | Solves vanishing gradients |\n",
    "| Forward output | `output, h_n` | `output, (h_n, c_n)` | LSTM returns cell state too |\n",
    "| Dropout | 0.5 | 0.3 | Less aggressive; LSTM already regularizes |\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2dc4a4e3-9de1-4dab-af42-fa8269f20f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier class defined\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based text classifier for AG News.\n",
    "    \n",
    "    Architecture:\n",
    "        Embedding → LSTM → Dropout → Linear\n",
    "    \n",
    "    Key difference from RNNClassifier:\n",
    "        - Uses nn.LSTM instead of nn.RNN\n",
    "        - LSTM returns (h_n, c_n) tuple instead of just h_n\n",
    "        - Better gradient flow enables learning long-range dependencies\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary (for embedding layer)\n",
    "        embed_dim: Dimension of word embeddings\n",
    "        hidden_size: Dimension of LSTM hidden states\n",
    "        num_classes: Number of output classes\n",
    "        num_layers: Number of stacked LSTM layers\n",
    "        dropout: Dropout probability\n",
    "        pad_idx: Index of padding token (for padding_idx in Embedding)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, \n",
    "                 num_layers=1, dropout=0.3, pad_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer: (B, L) → (B, L, embed_dim)\n",
    "        # padding_idx=pad_idx ensures padding tokens get zero vectors\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        \n",
    "        # LSTM layer: (B, L, embed_dim) → (B, L, hidden_size), ((num_layers, B, hidden_size), (num_layers, B, hidden_size))\n",
    "        # Key difference: LSTM has cell state c_t in addition to hidden state h_t\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,       # Input/output tensors have batch dimension first\n",
    "            dropout=dropout if num_layers > 1 else 0  # Dropout between LSTM layers\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization (reduced from 0.5 to 0.3)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classification head: (B, hidden_size) → (B, num_classes)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of word indices, shape (B, L)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Class scores, shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        # Step 1: Embed word indices → dense vectors\n",
    "        # (B, L) → (B, L, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Step 2: Process sequence through LSTM\n",
    "        # embedded: (B, L, embed_dim)\n",
    "        # output: (B, L, hidden_size) — hidden states at all time steps\n",
    "        # h_n: (num_layers, B, hidden_size) — final hidden state per layer\n",
    "        # c_n: (num_layers, B, hidden_size) — final cell state per layer\n",
    "        output, (h_n, c_n) = self.lstm(embedded)  # Note: tuple unpacking!\n",
    "        \n",
    "        # Step 3: Extract final hidden state from last layer\n",
    "        # h_n[-1]: (B, hidden_size)\n",
    "        hidden = h_n[-1]\n",
    "        \n",
    "        # Step 4: Apply dropout and classify\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"LSTMClassifier class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cfec5fe-51ff-435c-b35b-5683c5e28068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LSTM MODEL CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Vocabulary size:  25,002\n",
      "Embedding dim:    128\n",
      "Hidden size:      256\n",
      "Number of layers: 2\n",
      "Dropout:          0.3\n",
      "Number of classes: 4\n",
      "Device:           mps\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters (same as before, except dropout)\n",
    "EMBED_DIM = 128      # Dimension of word embeddings\n",
    "HIDDEN_SIZE = 256    # Dimension of LSTM hidden states\n",
    "NUM_LAYERS = 2       # Number of stacked LSTM layers\n",
    "DROPOUT = 0.3        # Reduced dropout (LSTM already provides regularization)\n",
    "NUM_CLASSES = len(CLASS_NAMES)  # 4 classes\n",
    "\n",
    "# Instantiate LSTM model\n",
    "lstm_model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LSTM MODEL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nVocabulary size:  {len(vocab):,}\")\n",
    "print(f\"Embedding dim:    {EMBED_DIM}\")\n",
    "print(f\"Hidden size:      {HIDDEN_SIZE}\")\n",
    "print(f\"Number of layers: {NUM_LAYERS}\")\n",
    "print(f\"Dropout:          {DROPOUT}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Device:           {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5f01457-e682-4713-9799-2ead8d31414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LSTM MODEL ARCHITECTURE\n",
      "============================================================\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(25002, 128, padding_idx=0)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "============================================================\n",
      "PARAMETER COUNT\n",
      "============================================================\n",
      "embedding.weight                              torch.Size([25002, 128]) 3,200,256\n",
      "lstm.weight_ih_l0                             torch.Size([1024, 128]) 131,072\n",
      "lstm.weight_hh_l0                             torch.Size([1024, 256]) 262,144\n",
      "lstm.bias_ih_l0                               torch.Size([1024])   1,024\n",
      "lstm.bias_hh_l0                               torch.Size([1024])   1,024\n",
      "lstm.weight_ih_l1                             torch.Size([1024, 256]) 262,144\n",
      "lstm.weight_hh_l1                             torch.Size([1024, 256]) 262,144\n",
      "lstm.bias_ih_l1                               torch.Size([1024])   1,024\n",
      "lstm.bias_hh_l1                               torch.Size([1024])   1,024\n",
      "fc.weight                                     torch.Size([4, 256]) 1,024\n",
      "fc.bias                                       torch.Size([4])      4\n",
      "\n",
      "Total                                                              4,122,884\n",
      "\n",
      "Trainable parameters: 4,122,884\n",
      "\n",
      "============================================================\n",
      "COMPARISON WITH RNN MODEL\n",
      "============================================================\n",
      "RNN model parameters:  3,431,684\n",
      "LSTM model parameters: 4,122,884\n",
      "Increase:              691,200 (20.1% more)\n",
      "\n",
      "Note: LSTM has ~4× more recurrent layer parameters due to the 4 gates.\n"
     ]
    }
   ],
   "source": [
    "# Verify LSTM model architecture and count parameters\n",
    "print(\"=\"*60)\n",
    "print(\"LSTM MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(lstm_model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER COUNT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_params = 0\n",
    "for name, param in lstm_model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    total_params += num_params\n",
    "    print(f\"{name:45s} {str(param.shape):20s} {num_params:,}\")\n",
    "\n",
    "print(f\"\\n{'Total':45s} {'':20s} {total_params:,}\")\n",
    "print(f\"\\nTrainable parameters: {sum(p.numel() for p in lstm_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Compare with RNN model\n",
    "rnn_params = 3_431_684  # From previous model\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH RNN MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RNN model parameters:  {rnn_params:,}\")\n",
    "print(f\"LSTM model parameters: {total_params:,}\")\n",
    "print(f\"Increase:              {total_params - rnn_params:,} ({100*(total_params/rnn_params - 1):.1f}% more)\")\n",
    "print(f\"\\nNote: LSTM has ~4× more recurrent layer parameters due to the 4 gates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1064842-63b7-4eb5-970b-c8bb754cdc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FORWARD PASS VERIFICATION\n",
      "============================================================\n",
      "\n",
      "Input shape:  torch.Size([64, 64]) → (B, L) = (64, 64)\n",
      "Output shape: torch.Size([64, 4]) → (B, K) = (64, 4)\n",
      "\n",
      "Sample output (first 3 samples):\n",
      "tensor([[ 0.0001,  0.0852, -0.0139, -0.0615],\n",
      "        [ 0.0001,  0.0852, -0.0139, -0.0615],\n",
      "        [ 0.0001,  0.0852, -0.0139, -0.0616]], device='mps:0')\n",
      "\n",
      "Predicted classes: [1, 1, 1]\n",
      "True classes:      [2, 1, 1]\n",
      "\n",
      "✓ LSTM model is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Verify forward pass with a sample batch\n",
    "print(\"=\"*60)\n",
    "print(\"FORWARD PASS VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get a sample batch\n",
    "sample_texts, sample_labels, sample_lengths = next(iter(train_loader))\n",
    "sample_texts = sample_texts.to(device)\n",
    "\n",
    "# Forward pass\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_logits = lstm_model(sample_texts)\n",
    "\n",
    "print(f\"\\nInput shape:  {sample_texts.shape} → (B, L) = ({BATCH_SIZE}, {MAX_LENGTH})\")\n",
    "print(f\"Output shape: {sample_logits.shape} → (B, K) = ({BATCH_SIZE}, {NUM_CLASSES})\")\n",
    "print(f\"\\nSample output (first 3 samples):\")\n",
    "print(sample_logits[:3])\n",
    "print(f\"\\nPredicted classes: {sample_logits[:3].argmax(dim=1).tolist()}\")\n",
    "print(f\"True classes:      {sample_labels[:3].tolist()}\")\n",
    "print(f\"\\n✓ LSTM model is ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d941f5-5222-473a-9f37-17409991d230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
