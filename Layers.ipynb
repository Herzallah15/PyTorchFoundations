{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8a6232-fbdd-466a-a420-4fc50b79008a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115725270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a0ead-bc14-4d64-99a4-d89ae7c7a1ef",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">Neural network layers are the fundamental building blocks that transform input data into meaningful representations. Each layer type is designed for specific data structures and tasks. The **shape of the input and output tensors** plays a crucial role in understanding how these layers operate.\n",
    "\n",
    "In what follows, we investigate the most common layer types in PyTorch:\n",
    "\n",
    "1. **nn.Linear**: Fully connected layers for general-purpose transformations\n",
    "2. **nn.Conv1d**: 1D convolutions for sequential/temporal data\n",
    "3. **nn.Conv2d**: 2D convolutions for image data\n",
    "4. **nn.Conv3d**: 3D convolutions for volumetric/video data\n",
    "5. **nn.ConvTranspose1d**: 1D transposed convolutions for upsampling sequential data\n",
    "6. **nn.ConvTranspose2d**: 2D transposed convolutions for image upsampling/generation\n",
    "7. **nn.ConvTranspose3d**: 3D transposed convolutions for volumetric upsampling\n",
    "8. **nn.RNN**: Recurrent layers for sequential data with temporal dependencies \n",
    "\n",
    "Throughout this notebook:\n",
    "- $B$ denotes the batch size\n",
    "- $C_{\\text{in}}$ and $C_{\\text{out}}$ denote input and output channels/features\n",
    "- Spatial dimensions are denoted by $L$ (length), $H$ (height), $W$ (width), $D$ (depth)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19674a-3b68-45ec-923f-911e7192c272",
   "metadata": {},
   "source": [
    "**Overview for Linear and Convolutional Layers**\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Primary Use Cases | Key Parameters |\n",
    "|-------|-------------|--------------|-------------------|----------------|\n",
    "| **nn.Linear** | $(*, H_{\\text{in}})$ | $(*, H_{\\text{out}})$ | MLPs, classification heads, dense connections, Transformer projections | `in_features`, `out_features`, `bias` |\n",
    "| **nn.Conv1d** | $(B, C_{\\text{in}}, L)$ | $(B, C_{\\text{out}}, L_{\\text{out}})$ | Time series, audio, text (1D sequences) | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding` |\n",
    "| **nn.Conv2d** | $(B, C_{\\text{in}}, H, W)$ | $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Images, feature maps, 2D spatial data | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups` |\n",
    "| **nn.Conv3d** | $(B, C_{\\text{in}}, D, H, W)$ | $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Videos, medical imaging (CT/MRI), 3D point clouds | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups` |\n",
    "\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Primary Use Cases | Key Parameters |\n",
    "|-------|-------------|--------------|-------------------|----------------|\n",
    "| **nn.ConvTranspose1d** | $(B, C_{\\text{in}}, L)$ | $(B, C_{\\text{out}}, L_{\\text{out}})$ | Audio generation, sequence upsampling, WaveNet decoders | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `output_padding` |\n",
    "| **nn.ConvTranspose2d** | $(B, C_{\\text{in}}, H, W)$ | $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Image generation (GANs), semantic segmentation decoders, autoencoders | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `output_padding` |\n",
    "| **nn.ConvTranspose3d** | $(B, C_{\\text{in}}, D, H, W)$ | $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Video generation, 3D medical image reconstruction, volumetric upsampling | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `output_padding` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a099d-0351-443a-a5c3-1071a9914497",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "    \n",
    "**Overview for Recurrent Layers** \n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed to process **sequential data** where the order of elements matters. Unlike feedforward networks that treat each input independently, RNNs maintain a **hidden state** that captures information from previous time steps.\n",
    "\n",
    "Throughout this notebook:\n",
    "- $L$ denotes the sequence length (number of time steps)\n",
    "- $H_{\\text{in}}$ denotes the input size (features per time step)\n",
    "- $H_{\\text{out}}$ denotes the hidden size (dimension of hidden state)\n",
    "- $t$ denotes the time step index\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Primary Use Cases | Key Parameters |\n",
    "|-------|-------------|--------------|-------------------|----------------|\n",
    "| **nn.RNN** | $X$: $(B, L, H_{\\text{in}})$, $h_0$: $(D \\cdot N_{\\text{layers}}, B, H_{\\text{out}})$ | output: $(B, L, D \\cdot H_{\\text{out}})$, $h_n$: $(D \\cdot N_{\\text{layers}}, B, H_{\\text{out}})$ | Sequence modeling, time series, NLP | `input_size`, `hidden_size`, `num_layers`, `batch_first`, `bidirectional` |\n",
    "| **nn.RNNCell** | $x$: $(B, H_{\\text{in}})$, $h$: $(B, H_{\\text{out}})$ | $h'$: $(B, H_{\\text{out}})$ | Custom RNN loops, single time step | `input_size`, `hidden_size` |\n",
    "| **nn.LSTM** | $X$: $(B, L, H_{\\text{in}})$, $(h_0, c_0)$: $(D \\cdot N_{\\text{layers}}, B, H_{\\text{out}})$ | output: $(B, L, D \\cdot H_{\\text{out}})$, $(h_n, c_n)$: $(D \\cdot N_{\\text{layers}}, B, H_{\\text{out}})$ | Long sequences, language modeling, translation | `input_size`, `hidden_size`, `num_layers`, `batch_first`, `bidirectional` |\n",
    "| **nn.GRU** | $X$: $(B, L, H_{\\text{in}})$, $h_0$: $(D \\cdot N_{\\text{layers}}, B, H_{\\text{out}})$ | output: $(B, L, D \\cdot H_{\\text{out}})$, $h_n$: $(D \\cdot N_{\\text{layers}}, B, H_{\\text{out}})$ | Similar to LSTM with fewer parameters | `input_size`, `hidden_size`, `num_layers`, `batch_first`, `bidirectional` |\n",
    "\n",
    "**Note**: Shapes shown assume `batch_first=True`. With `batch_first=False`, swap $B$ and $L$ in input/output tensors (but not in hidden states).\n",
    "\n",
    "**Key Output Interpretation**:\n",
    "- `output`: Hidden states from the **last layer only**, at **all time steps**\n",
    "- `h_n`: Hidden states from **all layers**, at the **last time step only**\n",
    "- For single-layer RNN: `h_n[0] == output[:, -1, :]`\n",
    "- For multi-layer RNN: `h_n[-1] == output[:, -1, :]`\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7cf48c-0fcd-4f7e-b181-73ce91529756",
   "metadata": {},
   "source": [
    "Detailed explanations of each layer, including mathematical formulations and implementation examples, follow below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913c152-b2d7-401b-b573-3dc034a3956d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184a791-9132-4eb1-9ea3-1b2dbb235a57",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "### Where is it used?\n",
    "\n",
    "The `nn.Linear` layer (also known as a fully connected or dense layer) applies a linear transformation to the incoming data. It is the most fundamental building block in neural networks and is used in:\n",
    "\n",
    "- **Multi-Layer Perceptrons (MLPs)**: The backbone of simple feedforward networks\n",
    "- **Classification heads**: Final layers that map features to class logits\n",
    "- **Transformer architectures**: Q, K, V projections and feed-forward networks\n",
    "- **Autoencoders**: Encoding and decoding dense representations\n",
    "- **Regression tasks**: Mapping features to continuous outputs\n",
    "\n",
    "### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(*, H_{\\text{in}})$ where $*$ means any number of dimensions and $H_{\\text{in}}$ is the number of input features\n",
    "- **Output**: $(*, H_{\\text{out}})$ where all dimensions except the last remain unchanged\n",
    "\n",
    "**Important**: The linear transformation is applied to the **last dimension** only.\n",
    "\n",
    "### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "- `in_features` (int): Size of each input sample (required)\n",
    "- `out_features` (int): Size of each output sample (required)\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- \n",
    "### Mathematical Formulation\n",
    "\n",
    "The linear layer applies the following transformation:\n",
    "\n",
    "$$\n",
    "{y_{\\rm out}}^{i,j,\\cdots, k} = \\omega_{k z} \\, {x_{\\rm in}}^{i,j,\\cdots, z} + b^{k}\n",
    "$$\n",
    "\n",
    "\n",
    "where the indices \"$i,j,\\cdots$\" represent an arbitrary dimension, i.e., $*$. This makes the layer **batch-agnostic**. As we see,\n",
    "- $x$ is the input tensor of shape $(*, H_{\\text{in}})$\n",
    "- $\\omega$ is the weight matrix of shape $(H_{\\text{out}}, H_{\\text{in}})$\n",
    "- $b$ is the bias vector of shape $(H_{\\text{out}})$\n",
    "- $y$ is the output tensor of shape $(*, H_{\\text{out}})$\n",
    "- **Notice**, that, all elements in the arbitrary dimension \"$i,j,\\cdots$\" get exactly the same biases and weights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "Weights are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k = \\frac{1}{H_{\\text{in}}}$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62e6aaa-46a3-4f58-b540-ccab2157f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([30, 20])\n",
      "Bias shape: torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Linear usage\n",
    "# Create a linear layer that maps 20 input features to 30 output features\n",
    "linear = nn.Linear(20, 30)\n",
    "\n",
    "# Check the weight and bias shapes\n",
    "print(f\"Weight shape: {linear.weight.shape}\")  # (out_features, in_features)\n",
    "print(f\"Bias shape: {linear.bias.shape}\")      # (out_features,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67d299c4-0284-4d7a-b5e5-01cb687c227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 20])\n",
      "Output shape: torch.Size([128, 30])\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple 2D input (batch_size, in_features)\n",
    "batch_size = 128\n",
    "in_features = 20\n",
    "out_features = 30\n",
    "\n",
    "linear = nn.Linear(in_features, out_features)\n",
    "models_weights = linear.weight\n",
    "models_biases = linear.bias\n",
    "x = torch.randn(batch_size, in_features)\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Lets now try to reproduce the output manually:\n",
    "\n",
    "output_manual = torch.einsum('bi,oi->bo', x, models_weights) + models_biases.unsqueeze(0).expand(batch_size, -1)\n",
    "print(f'Outputs match: {torch.all(output_manual==output).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b58cd32-e699-4cd3-be86-d44e5e910935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has bias: False\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Without bias\n",
    "linear_no_bias = nn.Linear(20, 30, bias=False)\n",
    "print(f\"Has bias: {linear_no_bias.bias is not None}\")\n",
    "\n",
    "# Verify the transformation manually\n",
    "x = torch.randn(5, 20)\n",
    "output_layer = linear_no_bias(x)\n",
    "output_manual = x @ linear_no_bias.weight.T  # y = x @ W^T\n",
    "\n",
    "print(f\"Outputs match: {torch.allclose(output_layer, output_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13656ff2-0304-4bba-92b6-d911349b9487",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb845e-958c-4d18-ac16-ffdd8708b9b6",
   "metadata": {},
   "source": [
    "Before we start discussing the types of convolutional layers, we briefly discuss some of the important padding types appearing in these layers:\n",
    "\n",
    "\n",
    "\n",
    "| Padding | Conv1d | Conv2d | Conv3d | Meaning |\n",
    "|---------|--------|--------|--------|---------|\n",
    "| `padding=0` | `0` | `(0,0)` | `(0,0,0)` | No padding, output shrinks |\n",
    "| `padding='valid'` | `0` | `(0,0)` | `(0,0,0)` | Equivalent to `padding=0` |\n",
    "| `padding='same'` | auto | auto | auto | Output size = Input size (requires `stride=1`) |\n",
    "| `padding=k//2` | `k//2` | `(k//2, k//2)` | `(k//2, k//2, k//2)` | Preserves size for odd kernel $k$ with `stride=1`, `dilation=1` |\n",
    "\n",
    "**Common recipe to preserve spatial dimensions:**\n",
    "\n",
    "For `kernel_size=k`, `stride=1`, `dilation=1`:\n",
    "```python\n",
    "# These are equivalent:\n",
    "padding = 'same'\n",
    "padding = k // 2   # only works for odd k\n",
    "```\n",
    "\n",
    "**Examples preserving size:**\n",
    "\n",
    "| Kernel | Padding |\n",
    "|--------|---------|\n",
    "| 3 | 1 |\n",
    "| 5 | 2 |\n",
    "| 7 | 3 |\n",
    "\n",
    "**General formula:** `padding = (kernel_size - 1) // 2` for odd kernels with `stride=1`, `dilation=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd3a3e-f067-4665-9c67-47d3cea77666",
   "metadata": {},
   "source": [
    "### nn.Conv1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6b5e4-7460-4208-a79a-f9afa8a79915",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.Conv1d` layer applies a 1D convolution over an input signal composed of several input planes. It is primarily used for:\n",
    "\n",
    "- **Time series analysis**: Stock prices, sensor data, weather patterns\n",
    "- **Audio processing**: Speech recognition, music classification\n",
    "- **Text/NLP**: Character-level or word-level sequence modeling\n",
    "- **Signal processing**: Any 1D sequential data\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, L_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $L_{\\text{in}}$ = length of the input sequence\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, L_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels (number of filters)\n",
    "  - $L_{\\text{out}} = \\left\\lfloor\\frac{L_{\\text{in}} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor$\n",
    "\n",
    "#### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "          dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int): Stride of the convolution. Default: `1`\n",
    "- `padding` (int or str): Zero-padding added to both sides. Default: `0`. Can be `'same'` or `'valid'`\n",
    "- `dilation` (int): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): `'zeros'`, `'reflect'`, `'replicate'`, or `'circular'`. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, \\text{kernel\\_size})$\n",
    "\n",
    "\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, L)$ and output of size $(B, C_{\\text{out}}, L_{\\text{out}})$, the convolution from $x_{\\rm in}[b, c, i]$ to $y_{\\rm out}[b, c, i] $ is performed as follows:\n",
    "\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, i] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in} -1} \\sum_{k=0}^{K -1}~{\\tilde x}_{\\rm in}[b,~ c_{\\rm in},~ s\\cdot i+~d\\cdot k] \\omega[c_{\\rm out}, c_{\\rm in}, k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* For `groups`$ > 1$ and, `in_channels % groups`$~=~$ `out_channels % groups`$=0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, i] = \\sum_{j=0}^{C_{\\rm in}/g -1} \\sum_{k=0}^{K -1}~{\\tilde x}_{\\rm in}[b,~ m(c_{\\rm out})\\cdot \\frac{C_{\\rm in}}{g} + j,~ s\\cdot i+~d\\cdot k] \\omega[c_{\\rm out}, j, k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ${\\tilde x}_{\\rm in}$ is the padded input tensor, it is simply equal to $x_{\\rm in}$ if padding is zero. Moreover, $g = $ `groups`, $s = $ `stride`, $d = $ `dilation` , and $m(c_{\\rm out}) = {\\rm floor}( c_{out} \\cdot  g / C_{out} )$.\n",
    "\n",
    "\n",
    "\n",
    "**Remark**: When `groups` $=1$, each filter combines information from all input channels. Otherwise, channels are processed in isolated groups, reducing computation and parameters but limiting cross-channel interaction.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0b7daae-f2d9-4635-8b7b-0a0c813d89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([33, 16, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Conv1d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3\n",
    "conv1d = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv1d.weight.shape}\")  # (out_channels, in_channels, kernel_size)\n",
    "print(f\"Bias shape: {conv1d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aef7343c-7a6b-470d-b2f7-0aa8d0f6314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 48])\n",
      "Expected L_out: 48\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic convolution without padding\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "out_channels = 33\n",
    "kernel_size = 3\n",
    "L_in = 50  # Input sequence length\n",
    "\n",
    "conv1d = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "x = torch.randn(batch_size, in_channels, L_in)\n",
    "output = conv1d(x)\n",
    "\n",
    "# Calculate expected output length: L_out = (L_in + 2*padding - dilation*(kernel_size-1) - 1) / stride + 1\n",
    "L_out_expected = (L_in + 2*0 - 1*(kernel_size-1) - 1) // 1 + 1\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected L_out: {L_out_expected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf0b2f29-7ca5-4397-86db-edfce62a1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 50])\n",
      "Length preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Convolution with padding to maintain spatial dimension\n",
    "# For kernel_size=3, padding=1 maintains the length (with stride=1)\n",
    "conv1d_same = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, padding=1)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_same(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Length preserved: {x.shape[2] == output.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0cdef7b-db36-4c24-86ec-062b584ba662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 50])\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using padding='same' (requires stride=1)\n",
    "conv1d_same_str = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=5, padding='same')\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_same_str(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b44ee49-aaa2-4797-9aa8-0f15c580aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 24])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Convolution with stride > 1 (downsampling)\n",
    "conv1d_stride = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, stride=2)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_stride(x)\n",
    "\n",
    "# L_out = (50 + 2*0 - 1*(3-1) - 1) // 2 + 1 = (50 - 2 - 1) // 2 + 1 = 47//2 + 1 = 23 + 1 = 24\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8463110e-6e6a-4ded-84f3-9160e63f4877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 46])\n",
      "Effective kernel size with dilation=2: 5\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Dilated convolution (for larger receptive field)\n",
    "conv1d_dilated = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, dilation=2)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_dilated(x)\n",
    "\n",
    "# L_out = (50 + 2*0 - 2*(3-1) - 1) / 1 + 1 = (50 - 4 - 1) + 1 = 46\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Effective kernel size with dilation=2: {2*(3-1)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf59a20-95b9-434e-b493-0bb69e661f1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nn.Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1760984-8345-4afd-9d9e-0fb3d61e3eaa",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.Conv2d` layer applies a 2D convolution over an input signal composed of several input planes. It is the backbone of computer vision and is used in:\n",
    "\n",
    "- **Image classification**: CNNs for recognizing objects in images\n",
    "- **Object detection**: YOLO, Faster R-CNN, etc.\n",
    "- **Semantic segmentation**: U-Net, DeepLab, etc.\n",
    "- **Image generation**: GANs, VAEs\n",
    "- **Feature extraction**: Any 2D spatial data processing\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels (e.g., 3 for RGB images)\n",
    "  - $H_{\\text{in}}$ = height of the input\n",
    "  - $W_{\\text{in}}$ = width of the input\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels (number of filters)\n",
    "  - $H_{\\text{out}} = \\left\\lfloor\\frac{H_{\\text{in}} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$\n",
    "  - $W_{\\text{out}} = \\left\\lfloor\\frac{W_{\\text{in}} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor$\n",
    "\n",
    "#### Default Arguments\n",
    "```python\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "          dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int or tuple): Stride of the convolution. Default: `1`\n",
    "- `padding` (int, tuple, or str): Zero-padding added to both sides. Default: `0`. Can be `'same'` or `'valid'`\n",
    "- `dilation` (int or tuple): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): `'zeros'`, `'reflect'`, `'replicate'`, or `'circular'`. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, \\text{kernel\\_size}[0], \\text{kernel\\_size}[1])$\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$, the convolution from $x_{\\rm in}[b, c, h, w]$ to $y_{\\rm out}[b, c, h, w]$ is performed as follows:\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, h, w] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in} -1} \\sum_{k_h=0}^{K_h -1} \\sum_{k_w=0}^{K_w -1}~{\\tilde x}_{\\rm in}[b,~ c_{\\rm in},~ s_h h+d_h k_h,~ s_w w + d_w k_w] \\cdot \\omega[c_{\\rm out}, c_{\\rm in}, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* For `groups`$ > 1$ and, `in_channels % groups`$~=~$ `out_channels % groups`$=0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, h, w] = \\sum_{j=0}^{C_{\\rm in}/g -1} \\sum_{k_h=0}^{K_h -1} \\sum_{k_w=0}^{K_w -1}~{\\tilde x}_{\\rm in}[b,~ m(c_{\\rm out})\\frac{C_{\\rm in}}{g} + j,~ s_h h+d_h \\cdot k_h,~ s_w \\cdot w + d_w \\cdot k_w] \\cdot \\omega[c_{\\rm out}, j, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ${\\tilde x}_{\\rm in}$ is the padded input tensor (equal to $x_{\\rm in}$ if padding is zero). Moreover, $g = $ `groups`, $(s_h, s_w) = $ `stride`, $(d_h, d_w) = $ `dilation`, $(K_h, K_w) = $ `kernel_size`, and $m(c_{\\rm out}) = {\\rm floor}( c_{\\rm out} \\cdot g / C_{\\rm out} )$.\n",
    "\n",
    "**Remarks**:\n",
    "1. When `groups` $=1$, each filter combines information from all input channels. Otherwise, channels are processed in isolated groups, reducing computation and parameters but limiting cross-channel interaction.\n",
    "2. If `kernel_size = k` is just a number, then PyTorch automatically assumes that $(K_h, K_w) = (k, k)$.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7b128f8-027a-4813-9789-c58536435884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([33, 16, 3, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Conv2d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3x3\n",
    "conv2d = nn.Conv2d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv2d.weight.shape}\")  # (out_channels, in_channels, kH, kW)\n",
    "print(f\"Bias shape: {conv2d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61d1f869-6b7c-4aae-ad42-87c4790d9f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 24, 49])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Square kernels and equal stride\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "H, W = 50, 100\n",
    "\n",
    "conv2d = nn.Conv2d(16, 33, kernel_size=3, stride=2)\n",
    "x = torch.randn(batch_size, in_channels, H, W)\n",
    "output = conv2d(x)\n",
    "\n",
    "# H_out = (50 + 2*0 - 1*(3-1) - 1) / 2 + 1 = 24\n",
    "# W_out = (100 + 2*0 - 1*(3-1) - 1) / 2 + 1 = 49\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd9059dd-9f38-4efa-b14c-953fbf49f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 28, 100])\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Non-square kernels and unequal stride with padding\n",
    "conv2d = nn.Conv2d(16, 33, kernel_size=(3, 5), stride=(2, 1), padding=(4, 2))\n",
    "x = torch.randn(20, 16, 50, 100)\n",
    "output = conv2d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c233100c-c32f-4bc4-9f8f-c33174810412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Output shape: torch.Size([1, 64, 224, 224])\n",
      "Spatial dimensions preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Maintaining spatial dimensions with appropriate padding\n",
    "# For kernel_size=3 and stride=1, padding=1 preserves dimensions\n",
    "conv2d_same = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "x = torch.randn(1, 3, 224, 224)  # Typical ImageNet input\n",
    "output = conv2d_same(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Spatial dimensions preserved: {x.shape[2:] == output.shape[2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8804a96c-1f34-4154-9bb9-90b32aa3f7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 26, 100])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Dilated convolution\n",
    "conv2d_dilated = nn.Conv2d(16, 33, kernel_size=(3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "x = torch.randn(20, 16, 50, 100)\n",
    "output = conv2d_dilated(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cce597ec-b623-40b1-8df3-bb8dd314b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depthwise conv weight shape: torch.Size([32, 1, 3, 3])\n",
      "Input shape: torch.Size([1, 32, 64, 64])\n",
      "Output shape: torch.Size([1, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Depthwise convolution (groups = in_channels)\n",
    "# Each input channel is convolved separately with its own filter\n",
    "in_channels = 32\n",
    "conv2d_depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "\n",
    "print(f\"Depthwise conv weight shape: {conv2d_depthwise.weight.shape}\")\n",
    "# Note: each filter only sees 1 input channel (in_channels/groups = 32/32 = 1)\n",
    "\n",
    "x = torch.randn(1, in_channels, 64, 64)\n",
    "output = conv2d_depthwise(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61e131f4-214f-4a72-ab15-a37020a8c7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard conv weight shape: torch.Size([8, 4, 3, 3])\n",
      "Grouped conv weight shape: torch.Size([8, 2, 3, 3])\n",
      "Depthwise conv weight shape: torch.Size([4, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Understanding groups parameter more deeply\n",
    "# Standard conv: each output channel sees ALL input channels\n",
    "# groups=2: split into 2 parallel convs, each seeing half the input channels\n",
    "\n",
    "# Standard convolution\n",
    "conv_standard = nn.Conv2d(4, 8, kernel_size=3, groups=1)\n",
    "print(f\"Standard conv weight shape: {conv_standard.weight.shape}\")\n",
    "# Shape: (8, 4, 3, 3) - each of 8 filters sees all 4 input channels\n",
    "\n",
    "# Grouped convolution with groups=2\n",
    "conv_grouped = nn.Conv2d(4, 8, kernel_size=3, groups=2)\n",
    "print(f\"Grouped conv weight shape: {conv_grouped.weight.shape}\")\n",
    "# Shape: (8, 2, 3, 3) - each of 8 filters sees only 2 input channels (4/2)\n",
    "\n",
    "# Depthwise convolution (groups = in_channels)\n",
    "conv_depthwise = nn.Conv2d(4, 4, kernel_size=3, groups=4)\n",
    "print(f\"Depthwise conv weight shape: {conv_depthwise.weight.shape}\")\n",
    "# Shape: (4, 1, 3, 3) - each of 4 filters sees only 1 input channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "094d0667-849a-4715-92b2-ec099bf8a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
      "  Calculated: (64, 64), Actual: torch.Size([64, 64])\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 2, 'padding': 1}\n",
      "  Calculated: (32, 32), Actual: torch.Size([32, 32])\n",
      "  Match: True\n",
      "Config: {'kernel_size': 5, 'stride': 2, 'padding': 2}\n",
      "  Calculated: (32, 32), Actual: torch.Size([32, 32])\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 2}\n",
      "  Calculated: (62, 62), Actual: torch.Size([62, 62])\n",
      "  Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula\n",
    "def conv2d_output_size(H_in, W_in, kernel_size, stride=1, padding=0, dilation=1):\n",
    "    \"\"\"Calculate Conv2d output dimensions\"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation)\n",
    "    \n",
    "    H_out = (H_in + 2*padding[0] - dilation[0]*(kernel_size[0]-1) - 1) // stride[0] + 1\n",
    "    W_out = (W_in + 2*padding[1] - dilation[1]*(kernel_size[1]-1) - 1) // stride[1] + 1\n",
    "    return H_out, W_out\n",
    "\n",
    "# Test with various configurations\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': 5, 'stride': 2, 'padding': 2},\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 2},\n",
    "]\n",
    "\n",
    "H_in, W_in = 64, 64\n",
    "x = torch.randn(1, 3, H_in, W_in)\n",
    "\n",
    "for config in configs:\n",
    "    conv = nn.Conv2d(3, 16, **config)\n",
    "    output = conv(x)\n",
    "    H_out_calc, W_out_calc = conv2d_output_size(H_in, W_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: ({H_out_calc}, {W_out_calc}), Actual: {output.shape[2:]}\")\n",
    "    print(f\"  Match: {(H_out_calc, W_out_calc) == tuple(output.shape[2:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004df0b8-2300-47a6-87dd-8fa3fc4b6e3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Conv3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dd36d-d58e-436f-8d34-c063eaa3316a",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.Conv3d` layer applies a 3D convolution over an input signal composed of several input planes. It is used for:\n",
    "\n",
    "- **Video analysis**: Action recognition, video classification\n",
    "- **Medical imaging**: CT scans, MRI volumes, 3D ultrasound\n",
    "- **3D object recognition**: Point cloud processing, voxel-based analysis\n",
    "- **Spatiotemporal data**: Any data with 3 spatial/temporal dimensions\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, D_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $D_{\\text{in}}$ = depth of the input (e.g., number of frames in video, slices in CT scan)\n",
    "  - $H_{\\text{in}}$ = height of the input\n",
    "  - $W_{\\text{in}}$ = width of the input\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels (number of filters)\n",
    "  - $D_{\\text{out}} = \\left\\lfloor\\frac{D_{\\text{in}} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$\n",
    "  - $H_{\\text{out}} = \\left\\lfloor\\frac{H_{\\text{in}} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor$\n",
    "  - $W_{\\text{out}} = \\left\\lfloor\\frac{W_{\\text{in}} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor$\n",
    "\n",
    "#### Default Arguments\n",
    "```python\n",
    "nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "          dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input volume (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int or tuple): Stride of the convolution. Default: `1`\n",
    "- `padding` (int, tuple, or str): Zero-padding added to all sides. Default: `0`. Can be `'same'` or `'valid'`\n",
    "- `dilation` (int or tuple): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): `'zeros'`, `'reflect'`, `'replicate'`, or `'circular'`. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, \\text{kernel\\_size}[0], \\text{kernel\\_size}[1], \\text{kernel\\_size}[2])$\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, D_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$, the convolution from $x_{\\rm in}[b, c, d, h, w]$ to $y_{\\rm out}[b, c, d, h, w]$ is performed as follows:\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, d, h, w] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in} -1} \\sum_{k_d=0}^{K_d -1} \\sum_{k_h=0}^{K_h -1} \\sum_{k_w=0}^{K_w -1}~{\\tilde x}_{\\rm in}[b,~ c_{\\rm in},~ s_d \\cdot d + d_d \\cdot k_d,~ s_h \\cdot h + d_h \\cdot k_h,~ s_w \\cdot w + d_w \\cdot k_w] \\cdot \\omega[c_{\\rm out}, c_{\\rm in}, k_d, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* For `groups`$ > 1$ and, `in_channels % groups`$~=~$ `out_channels % groups`$=0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, d, h, w] = \\sum_{j=0}^{C_{\\rm in}/g -1} \\sum_{k_d=0}^{K_d -1} \\sum_{k_h=0}^{K_h -1} \\sum_{k_w=0}^{K_w -1}~{\\tilde x}_{\\rm in}[b,~ m(c_{\\rm out})\\frac{C_{\\rm in}}{g} + j,~ s_d \\cdot d + d_d\\cdot  k_d,~ s_h \\cdot h + d_h \\cdot k_h,~ s_w\\cdot  w + d_w\\cdot  k_w] \\cdot \\omega[c_{\\rm out}, j, k_d, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ${\\tilde x}_{\\rm in}$ is the padded input tensor (equal to $x_{\\rm in}$ if padding is zero). Moreover, $g = $ `groups`, $(s_d, s_h, s_w) = $ `stride`, $(d_d, d_h, d_w) = $ `dilation`, $(K_d, K_h, K_w) = $ `kernel_size`, and $m(c_{\\rm out}) = \\lfloor c_{\\rm out} \\cdot g / C_{\\rm out} \\rfloor$.\n",
    "\n",
    "**Remarks**:\n",
    "1. When `groups` $=1$, each filter combines information from all input channels. Otherwise, channels are processed in isolated groups, reducing computation and parameters but limiting cross-channel interaction.\n",
    "2. If `kernel_size = k` is just a number, then PyTorch automatically assumes that $(K_d, K_h, K_w) = (k, k, k)$.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e405e332-e46f-49c3-9935-5d1fe77fd6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([33, 16, 3, 3, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Conv3d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3x3x3\n",
    "conv3d = nn.Conv3d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv3d.weight.shape}\")  # (out_channels, in_channels, kD, kH, kW)\n",
    "print(f\"Bias shape: {conv3d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f492e4d-5df9-457b-9946-324e171df263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 10, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 4, 24, 49])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Cubic kernels and equal stride\n",
    "conv3d = nn.Conv3d(16, 33, kernel_size=3, stride=2)\n",
    "x = torch.randn(20, 16, 10, 50, 100)  # (batch, channels, depth, height, width)\n",
    "output = conv3d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee98c502-aeff-459f-9972-9b6e52244732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 10, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 8, 50, 99])\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Non-cubic kernels and unequal stride with padding\n",
    "# kernel_size=(depth, height, width), stride=(sD, sH, sW), padding=(pD, pH, pW)\n",
    "conv3d = nn.Conv3d(16, 33, kernel_size=(3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n",
    "x = torch.randn(20, 16, 10, 50, 100)\n",
    "output = conv3d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7afc1698-64f6-4025-ba53-7c4045515b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (video): torch.Size([4, 3, 16, 112, 112])\n",
      "Output shape: torch.Size([4, 64, 16, 112, 112])\n",
      "Dimensions preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Video processing (batch, channels, frames, height, width)\n",
    "# A typical video input: 3 RGB channels, 16 frames, 112x112 resolution\n",
    "batch_size = 4\n",
    "channels = 3\n",
    "frames = 16\n",
    "height = width = 112\n",
    "\n",
    "conv3d_video = nn.Conv3d(channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "video_input = torch.randn(batch_size, channels, frames, height, width)\n",
    "output = conv3d_video(video_input)\n",
    "\n",
    "print(f\"Input shape (video): {video_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Dimensions preserved: {video_input.shape[2:] == output.shape[2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58538fa3-8575-432c-8dfb-df7c0e3b0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT scan input shape: torch.Size([1, 1, 32, 128, 128])\n",
      "Feature map output shape: torch.Size([1, 16, 32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Medical imaging (CT scan volume)\n",
    "# Input: single channel (grayscale), 32 slices, 128x128 resolution\n",
    "conv3d_medical = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n",
    "ct_scan = torch.randn(1, 1, 32, 128, 128)  # (batch, channel, slices, height, width)\n",
    "output = conv3d_medical(ct_scan)\n",
    "\n",
    "print(f\"CT scan input shape: {ct_scan.shape}\")\n",
    "print(f\"Feature map output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c643439a-2f67-4f6b-926e-f7dbd5c72358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
      "  Calculated: (16, 64, 64), Actual: (16, 64, 64)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': 3, 'stride': 2, 'padding': 1}\n",
      "  Calculated: (8, 32, 32), Actual: (8, 32, 32)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': (3, 5, 5), 'stride': (1, 2, 2), 'padding': (1, 2, 2)}\n",
      "  Calculated: (16, 32, 32), Actual: (16, 32, 32)\n",
      "  Match: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula for Conv3d\n",
    "def conv3d_output_size(D_in, H_in, W_in, kernel_size, stride=1, padding=0, dilation=1):\n",
    "    \"\"\"Calculate Conv3d output dimensions\"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size, kernel_size)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation, dilation)\n",
    "    \n",
    "    D_out = (D_in + 2*padding[0] - dilation[0]*(kernel_size[0]-1) - 1) // stride[0] + 1\n",
    "    H_out = (H_in + 2*padding[1] - dilation[1]*(kernel_size[1]-1) - 1) // stride[1] + 1\n",
    "    W_out = (W_in + 2*padding[2] - dilation[2]*(kernel_size[2]-1) - 1) // stride[2] + 1\n",
    "    return D_out, H_out, W_out\n",
    "\n",
    "# Test\n",
    "D_in, H_in, W_in = 16, 64, 64\n",
    "x = torch.randn(1, 3, D_in, H_in, W_in)\n",
    "\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': (3, 5, 5), 'stride': (1, 2, 2), 'padding': (1, 2, 2)},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    conv = nn.Conv3d(3, 16, **config)\n",
    "    output = conv(x)\n",
    "    D_out_calc, H_out_calc, W_out_calc = conv3d_output_size(D_in, H_in, W_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: ({D_out_calc}, {H_out_calc}, {W_out_calc}), Actual: {tuple(output.shape[2:])}\")\n",
    "    print(f\"  Match: {(D_out_calc, H_out_calc, W_out_calc) == tuple(output.shape[2:])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55972c90-f365-4dd0-808a-ca8e8555c105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26cbbd43-b45f-4221-afae-ec1a2d374fa7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2426c73-5afa-455d-885c-4a1c91bdc121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LAYER COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "nn.Linear(512, 256):\n",
      "  Input shape:  (32, 512) -> (batch, in_features)\n",
      "  Output shape: (32, 256) -> (batch, out_features)\n",
      "  Weight shape: (256, 512) -> (out_features, in_features)\n",
      "  Parameters:   131,328\n",
      "\n",
      "nn.Conv1d(64, 128, kernel_size=3, padding=1):\n",
      "  Input shape:  (32, 64, 100) -> (batch, in_channels, length)\n",
      "  Output shape: (32, 128, 100) -> (batch, out_channels, length)\n",
      "  Weight shape: (128, 64, 3) -> (out_ch, in_ch, kernel)\n",
      "  Parameters:   24,704\n",
      "\n",
      "nn.Conv2d(64, 128, kernel_size=3, padding=1):\n",
      "  Input shape:  (32, 64, 56, 56) -> (batch, in_channels, H, W)\n",
      "  Output shape: (32, 128, 56, 56) -> (batch, out_channels, H, W)\n",
      "  Weight shape: (128, 64, 3, 3) -> (out_ch, in_ch, kH, kW)\n",
      "  Parameters:   73,856\n",
      "\n",
      "nn.Conv3d(64, 128, kernel_size=3, padding=1):\n",
      "  Input shape:  (4, 64, 16, 56, 56) -> (batch, in_channels, D, H, W)\n",
      "  Output shape: (4, 128, 16, 56, 56) -> (batch, out_channels, D, H, W)\n",
      "  Weight shape: (128, 64, 3, 3, 3) -> (out_ch, in_ch, kD, kH, kW)\n",
      "  Parameters:   221,312\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison of all layers\n",
    "print(\"=\" * 80)\n",
    "print(\"LAYER COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# nn.Linear\n",
    "linear = nn.Linear(512, 256)\n",
    "x_linear = torch.randn(32, 512)\n",
    "print(f\"\\nnn.Linear(512, 256):\")\n",
    "print(f\"  Input shape:  {tuple(x_linear.shape)} -> (batch, in_features)\")\n",
    "print(f\"  Output shape: {tuple(linear(x_linear).shape)} -> (batch, out_features)\")\n",
    "print(f\"  Weight shape: {tuple(linear.weight.shape)} -> (out_features, in_features)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in linear.parameters()):,}\")\n",
    "\n",
    "# nn.Conv1d\n",
    "conv1d = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "x_conv1d = torch.randn(32, 64, 100)\n",
    "print(f\"\\nnn.Conv1d(64, 128, kernel_size=3, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_conv1d.shape)} -> (batch, in_channels, length)\")\n",
    "print(f\"  Output shape: {tuple(conv1d(x_conv1d).shape)} -> (batch, out_channels, length)\")\n",
    "print(f\"  Weight shape: {tuple(conv1d.weight.shape)} -> (out_ch, in_ch, kernel)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv1d.parameters()):,}\")\n",
    "\n",
    "# nn.Conv2d\n",
    "conv2d = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "x_conv2d = torch.randn(32, 64, 56, 56)\n",
    "print(f\"\\nnn.Conv2d(64, 128, kernel_size=3, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_conv2d.shape)} -> (batch, in_channels, H, W)\")\n",
    "print(f\"  Output shape: {tuple(conv2d(x_conv2d).shape)} -> (batch, out_channels, H, W)\")\n",
    "print(f\"  Weight shape: {tuple(conv2d.weight.shape)} -> (out_ch, in_ch, kH, kW)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv2d.parameters()):,}\")\n",
    "\n",
    "# nn.Conv3d\n",
    "conv3d = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "x_conv3d = torch.randn(4, 64, 16, 56, 56)\n",
    "print(f\"\\nnn.Conv3d(64, 128, kernel_size=3, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_conv3d.shape)} -> (batch, in_channels, D, H, W)\")\n",
    "print(f\"  Output shape: {tuple(conv3d(x_conv3d).shape)} -> (batch, out_channels, D, H, W)\")\n",
    "print(f\"  Weight shape: {tuple(conv3d.weight.shape)} -> (out_ch, in_ch, kD, kH, kW)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv3d.parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e4833cb-4dc2-4ef6-ade2-a88f244c9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY FORMULAS FOR OUTPUT SIZE CALCULATION\n",
      "============================================================\n",
      "\n",
      "nn.Linear:\n",
      "  Output: (*, out_features)\n",
      "  The transformation is applied to the LAST dimension only\n",
      "\n",
      "nn.Conv1d/Conv2d/Conv3d:\n",
      "  For each spatial dimension:\n",
      "  L_out = floor((L_in + 2*padding - dilation*(kernel_size-1) - 1) / stride + 1)\n",
      "\n",
      "Special cases:\n",
      "  - padding='same' with stride=1: output size = input size\n",
      "  - padding=k//2 with odd kernel k and stride=1: preserves size\n",
      "  - stride=2 with appropriate padding: halves spatial dimensions\n"
     ]
    }
   ],
   "source": [
    "# Key formulas for output size calculation\n",
    "print(\"KEY FORMULAS FOR OUTPUT SIZE CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"nn.Linear:\")\n",
    "print(\"  Output: (*, out_features)\")\n",
    "print(\"  The transformation is applied to the LAST dimension only\")\n",
    "print()\n",
    "print(\"nn.Conv1d/Conv2d/Conv3d:\")\n",
    "print(\"  For each spatial dimension:\")\n",
    "print(\"  L_out = floor((L_in + 2*padding - dilation*(kernel_size-1) - 1) / stride + 1)\")\n",
    "print()\n",
    "print(\"Special cases:\")\n",
    "print(\"  - padding='same' with stride=1: output size = input size\")\n",
    "print(\"  - padding=k//2 with odd kernel k and stride=1: preserves size\")\n",
    "print(\"  - stride=2 with appropriate padding: halves spatial dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c9bf3-11ab-43ef-bea0-65e64cf05029",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transposed Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cfbe0-4714-4ed4-9d17-a809b3e83038",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">Transposed convolutional layers (often called \"deconvolutions\" or \"fractionally strided convolutions\") are essential building blocks for **upsampling** in neural networks. They perform the **inverse spatial transformation** of regular convolutions, where convolutions typically reduce spatial dimensions, transposed convolutions increase them.\n",
    "\n",
    "**Why Transposed Convolutions?**\n",
    "\n",
    "\n",
    "\n",
    "Regular convolutions with `stride > 1` **downsample** spatial dimensions. For many applications (image generation, segmentation, autoencoders), we need to **upsample**, to go from a smaller spatial representation back to a larger one.\n",
    "\n",
    "**Key insight**: A transposed convolution is not literally the inverse of a convolution. Instead, it is the **transpose of the convolution operation viewed as a matrix multiplication**. If a convolution maps from a space of dimension $N$ to a space of dimension $M$, the transposed convolution maps from $M$ back to $N$.\n",
    "\n",
    "Concretely:\n",
    "- **Convolution** ($\\text{stride}=2$): Input $L_{\\text{in}} \\to$ Output $L_{\\text{out}} \\approx L_{\\text{in}}/2$ (downsampling)\n",
    "- **Transposed Convolution** ($\\text{stride}=2$): Input $L_{\\text{in}} \\to$ Output $L_{\\text{out}} \\approx 2 \\cdot L_{\\text{in}}$ (upsampling)\n",
    "\n",
    "The `output_padding` parameter resolves the ambiguity that arises because multiple input sizes can map to the same output size under convolution.\n",
    "\n",
    "In what follows, we investigate the transposed convolutional layer types in PyTorch:\n",
    "\n",
    "1. **nn.ConvTranspose1d**: 1D transposed convolutions for sequential/temporal upsampling\n",
    "2. **nn.ConvTranspose2d**: 2D transposed convolutions for image upsampling\n",
    "3. **nn.ConvTranspose3d**: 3D transposed convolutions for volumetric/video upsampling\n",
    "\n",
    "Throughout this notebook:\n",
    "- $B$ denotes the batch size\n",
    "- $C_{\\text{in}}$ and $C_{\\text{out}}$ denote input and output channels/features\n",
    "- Spatial dimensions are denoted by $L$ (length), $H$ (height), $W$ (width), $D$ (depth)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4cce7-d999-49c3-abf7-0e2812f722cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Before we start discussing the types of transposed convolutional layers, we briefly discuss the key parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `stride` | Controls the **upsampling factor**. Unlike in `Conv`, larger stride means **larger** output. |\n",
    "| `padding` | Reduces the output size (removes elements from the output). |\n",
    "| `output_padding` | Adds extra elements to one side of the output to resolve size ambiguity. Must be `< stride`. |\n",
    "| `dilation` | Spacing between kernel elements (same as in regular convolution). |\n",
    "\n",
    "**Output size formula** (for each spatial dimension):\n",
    "\n",
    "$$\n",
    "L_{\\text{out}} = (L_{\\text{in}} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation} \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1\n",
    "$$\n",
    "\n",
    "**Common recipe for exact $2\\times$ upsampling** (doubling spatial dimensions):\n",
    "\n",
    "```python\n",
    "# For exact 2x upsampling:\n",
    "nn.ConvTranspose2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
    "# or\n",
    "nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2, padding=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62599735-707b-48bb-9f35-5535e2dfc5c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nn.ConvTranspose1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2174ff-c0bf-45a8-9b0c-dbbaa4e45a0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.ConvTranspose1d` layer applies a 1D transposed convolution over an input signal composed of several input planes. It is primarily used for:\n",
    "\n",
    "- **Audio generation**: WaveNet decoders, audio super-resolution\n",
    "- **Sequence upsampling**: Increasing temporal resolution of time series\n",
    "- **1D autoencoders**: Decoder networks for sequential data\n",
    "- **Signal reconstruction**: Upsampling compressed signal representations\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, L_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $L_{\\text{in}}$ = length of the input sequence\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, L_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels\n",
    "  - $L_{\\text{out}} = (L_{\\text{in}} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation} \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1$\n",
    "\n",
    "#### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                   output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int): Stride of the convolution (controls upsampling factor). Default: `1`\n",
    "- `padding` (int): Zero-padding added to both sides of the input. **Reduces** output size. Default: `0`\n",
    "- `output_padding` (int): Additional size added to one side of the output. Must be `< stride`. Default: `0`\n",
    "- `dilation` (int): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): Only `'zeros'` is supported. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{in}}, \\frac{C_{\\text{out}}}{\\text{groups}}, \\text{kernel\\_size})$\n",
    "\n",
    "**Note**: The weight shape is **transposed** compared to `nn.Conv1d` which has shape $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, K)$.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, L_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, L_{\\text{out}})$, the transposed convolution from $x_{\\rm in}[b, c, i]$ to $y_{\\rm out}[b, c, j]$ can be understood as follows:\n",
    "\n",
    "The transposed convolution is defined as the **gradient of a convolution with respect to its input**. Equivalently, it can be viewed as:\n",
    "\n",
    "1. **Insert zeros** between input elements (determined by `stride`)\n",
    "2. **Pad** the expanded input\n",
    "3. Apply a **regular convolution** with the spatially flipped kernel\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, j] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{k=0}^{K-1} \\tilde{x}_{\\rm in}[b, c_{\\rm in}, j + p - d \\cdot k] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, K - 1 - k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}_{\\rm in}$ is the input after zero-insertion (inserting $s-1$ zeros between each element) and appropriate padding adjustment.\n",
    "\n",
    "Alternatively, the operation can be expressed as a **summation over input positions** that contribute to each output position:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, j] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{i=0}^{L_{\\rm in}-1} \\sum_{k=0}^{K-1} \\mathbf{1}_{[j = s \\cdot i + d \\cdot k - p + p_{\\rm out}]} \\cdot x_{\\rm in}[b, c_{\\rm in}, i] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $s = $ `stride`, $d = $ `dilation`, $p = $ `padding`, $p_{\\rm out} = $ `output_padding`, and $\\mathbf{1}_{[\\cdot]}$ is the indicator function.\n",
    "\n",
    "* For `groups`$ > 1$, the channels are split into groups analogously to regular convolutions.\n",
    "\n",
    "**Remark**: The key intuition is that each input element \"broadcasts\" its value to multiple output positions through the kernel, with the kernel flipped spatially.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "445472ad-02ea-45ff-ad80-9fe423b49c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([16, 33, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.ConvTranspose1d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3\n",
    "conv_transpose1d = nn.ConvTranspose1d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv_transpose1d.weight.shape}\")  # (in_channels, out_channels, kernel_size)\n",
    "print(f\"Bias shape: {conv_transpose1d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca602d0b-9680-4422-9a79-8ca944f1e764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 52])\n",
      "Expected L_out: 52\n",
      "Note: Output is LARGER than input: True\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic transposed convolution (upsampling)\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "out_channels = 33\n",
    "kernel_size = 3\n",
    "L_in = 50  # Input sequence length\n",
    "\n",
    "conv_transpose1d = nn.ConvTranspose1d(in_channels, out_channels, kernel_size)\n",
    "x = torch.randn(batch_size, in_channels, L_in)\n",
    "output = conv_transpose1d(x)\n",
    "\n",
    "# Calculate expected output length: L_out = (L_in - 1) * stride - 2*padding + dilation*(kernel_size-1) + output_padding + 1\n",
    "# With defaults: stride=1, padding=0, dilation=1, output_padding=0\n",
    "L_out_expected = (L_in - 1) * 1 - 2*0 + 1*(kernel_size-1) + 0 + 1\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected L_out: {L_out_expected}\")\n",
    "print(f\"Note: Output is LARGER than input: {L_out_expected > x.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a3f6995-a969-4f91-838c-83b8fcf1520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 100])\n",
      "Upsampling factor: 2.0x\n"
     ]
    }
   ],
   "source": [
    "# Example 2: 2x upsampling with stride=2\n",
    "conv_transpose1d_upsample = nn.ConvTranspose1d(in_channels=16, out_channels=33, kernel_size=4, stride=2, padding=1)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv_transpose1d_upsample(x)\n",
    "\n",
    "# L_out = (50-1)*2 - 2*1 + 1*(4-1) + 0 + 1 = 98 - 2 + 3 + 1 = 100\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Upsampling factor: {output.shape[2] / x.shape[2]}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4acab6e8-95f2-4424-aa9a-59953e8a3a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 100])\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using output_padding to resolve ambiguity\n",
    "# When stride > 1, multiple input sizes can produce the same output size after Conv1d\n",
    "# output_padding helps specify the exact output size we want\n",
    "\n",
    "conv_transpose1d_op = nn.ConvTranspose1d(in_channels=16, out_channels=33, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv_transpose1d_op(x)\n",
    "\n",
    "# L_out = (50-1)*2 - 2*1 + 1*(3-1) + 1 + 1 = 98 - 2 + 2 + 1 + 1 = 100\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbbe174f-0932-417b-b324-f1f9fc0a91c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 16, 100])\n",
      "After Conv1d (downsample): torch.Size([1, 32, 50])\n",
      "After ConvTranspose1d (upsample): torch.Size([1, 16, 100])\n",
      "Spatial dimension restored: True\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Comparison with Conv1d (inverse relationship)\n",
    "# ConvTranspose1d can reverse the spatial transformation of Conv1d\n",
    "\n",
    "L_original = 100\n",
    "x_original = torch.randn(1, 16, L_original)\n",
    "\n",
    "# Downsample with Conv1d\n",
    "conv1d_down = nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1)\n",
    "x_down = conv1d_down(x_original)\n",
    "\n",
    "# Upsample with ConvTranspose1d (matching parameters)\n",
    "conv_transpose1d_up = nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "x_up = conv_transpose1d_up(x_down)\n",
    "\n",
    "print(f\"Original shape: {x_original.shape}\")\n",
    "print(f\"After Conv1d (downsample): {x_down.shape}\")\n",
    "print(f\"After ConvTranspose1d (upsample): {x_up.shape}\")\n",
    "print(f\"Spatial dimension restored: {x_original.shape[2] == x_up.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc282ca4-58f1-4071-8e7c-9e3773310b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 54])\n",
      "Effective kernel size with dilation=2: 5\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Dilated transposed convolution\n",
    "conv_transpose1d_dilated = nn.ConvTranspose1d(in_channels=16, out_channels=33, kernel_size=3, dilation=2)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv_transpose1d_dilated(x)\n",
    "\n",
    "# L_out = (50-1)*1 - 2*0 + 2*(3-1) + 0 + 1 = 49 + 4 + 1 = 54\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Effective kernel size with dilation=2: {2*(3-1)+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06a9239b-da05-46db-acae-fc4924085005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0}\n",
      "  Calculated: 52, Actual: 52\n",
      "  Match: True\n",
      "Config: {'kernel_size': 4, 'stride': 2, 'padding': 1}\n",
      "  Calculated: 100, Actual: 100\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 2, 'padding': 1, 'output_padding': 1}\n",
      "  Calculated: 100, Actual: 100\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0, 'dilation': 2}\n",
      "  Calculated: 54, Actual: 54\n",
      "  Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula for ConvTranspose1d\n",
    "def conv_transpose1d_output_size(L_in, kernel_size, stride=1, padding=0, dilation=1, output_padding=0):\n",
    "    \"\"\"Calculate ConvTranspose1d output length\"\"\"\n",
    "    L_out = (L_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    return L_out\n",
    "\n",
    "# Test with various configurations\n",
    "L_in = 50\n",
    "x = torch.randn(1, 16, L_in)\n",
    "\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0},\n",
    "    {'kernel_size': 4, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1, 'output_padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0, 'dilation': 2},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    conv_t = nn.ConvTranspose1d(16, 33, **config)\n",
    "    output = conv_t(x)\n",
    "    L_out_calc = conv_transpose1d_output_size(L_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: {L_out_calc}, Actual: {output.shape[2]}\")\n",
    "    print(f\"  Match: {L_out_calc == output.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c04965-4736-4a9f-8c16-a6903b33bbbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nn.ConvTranspose2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fe3dc-071b-4d30-929c-d9c3160d46db",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.ConvTranspose2d` layer applies a 2D transposed convolution over an input image composed of several input planes. It is the backbone of image upsampling and is used in:\n",
    "\n",
    "- **Generative Adversarial Networks (GANs)**: Generator networks that upsample from latent vectors to images\n",
    "- **Semantic segmentation**: Decoder networks in U-Net, FCN, etc.\n",
    "- **Image super-resolution**: Upscaling low-resolution images\n",
    "- **Autoencoders**: Decoder part for image reconstruction\n",
    "- **Style transfer**: Reconstructing styled images at original resolution\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $H_{\\text{in}}$ = height of the input\n",
    "  - $W_{\\text{in}}$ = width of the input\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels\n",
    "  - $H_{\\text{out}} = (H_{\\text{in}} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1$\n",
    "  - $W_{\\text{out}} = (W_{\\text{in}} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1$\n",
    "\n",
    "#### Default Arguments\n",
    "```python\n",
    "nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                   output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input image (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int or tuple): Stride of the convolution (controls upsampling factor). Default: `1`\n",
    "- `padding` (int or tuple): Zero-padding added to both sides. **Reduces** output size. Default: `0`\n",
    "- `output_padding` (int or tuple): Additional size added to one side of output. Must be `< stride`. Default: `0`\n",
    "- `dilation` (int or tuple): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): Only `'zeros'` is supported. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{in}}, \\frac{C_{\\text{out}}}{\\text{groups}}, \\text{kernel\\_size}[0], \\text{kernel\\_size}[1])$\n",
    "\n",
    "**Note**: The weight shape is **transposed** compared to `nn.Conv2d` which has shape $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, K_h, K_w)$.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$, the transposed convolution from $x_{\\rm in}[b, c, h, w]$ to $y_{\\rm out}[b, c, h', w']$ is performed as follows:\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, h', w'] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1} \\tilde{x}_{\\rm in}[b, c_{\\rm in}, h' + p_h - d_h k_h, w' + p_w - d_w k_w] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, K_h - 1 - k_h, K_w - 1 - k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}_{\\rm in}$ is the input after zero-insertion (inserting $s_h - 1$ and $s_w - 1$ zeros between elements along height and width respectively).\n",
    "\n",
    "Equivalently, the operation can be expressed as a **summation over input positions**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, h', w'] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{h=0}^{H_{\\rm in}-1} \\sum_{w=0}^{W_{\\rm in}-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1} \\mathbf{1}_{\\substack{[h' = s_h h + d_h k_h - p_h + p_{\\rm out,h}] \\\\ [w' = s_w w + d_w k_w - p_w + p_{\\rm out,w}]}} \\cdot x_{\\rm in}[b, c_{\\rm in}, h, w] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $(s_h, s_w) = $ `stride`, $(d_h, d_w) = $ `dilation`, $(p_h, p_w) = $ `padding`, $(p_{\\rm out,h}, p_{\\rm out,w}) = $ `output_padding`, $(K_h, K_w) = $ `kernel_size`, and $\\mathbf{1}_{[\\cdot]}$ is the indicator function.\n",
    "\n",
    "* For `groups`$ > 1$, the channels are split into groups analogously to regular convolutions.\n",
    "\n",
    "**Remarks**:\n",
    "1. Each input pixel \"scatters\" its influence to a $K_h \\times K_w$ region in the output, weighted by the kernel.\n",
    "2. If `kernel_size = k` is just a number, then PyTorch automatically assumes $(K_h, K_w) = (k, k)$.\n",
    "3. The `output_padding` only adds to one side (not both), which is why it must be strictly less than `stride`.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "796c0e79-4822-4d40-b31d-54f9302c38f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([16, 33, 3, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.ConvTranspose2d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3x3\n",
    "conv_transpose2d = nn.ConvTranspose2d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv_transpose2d.weight.shape}\")  # (in_channels, out_channels, kH, kW)\n",
    "print(f\"Bias shape: {conv_transpose2d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "586182ab-f7f7-4290-b010-b7dfb88fb19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 52, 102])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic transposed convolution (slight upsampling)\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "H, W = 50, 100\n",
    "\n",
    "conv_transpose2d = nn.ConvTranspose2d(16, 33, kernel_size=3)\n",
    "x = torch.randn(batch_size, in_channels, H, W)\n",
    "output = conv_transpose2d(x)\n",
    "\n",
    "# H_out = (50-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 52\n",
    "# W_out = (100-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 102\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da3ab54a-8c9a-49d9-b578-2f4753ddf92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 32, 32])\n",
      "Output shape: torch.Size([20, 33, 64, 64])\n",
      "Exact 2x upsampling: True\n"
     ]
    }
   ],
   "source": [
    "# Example 2: 2x upsampling (common in GANs and decoders)\n",
    "# kernel_size=4, stride=2, padding=1 gives exact 2x upsampling\n",
    "conv_transpose2d_2x = nn.ConvTranspose2d(16, 33, kernel_size=4, stride=2, padding=1)\n",
    "x = torch.randn(20, 16, 32, 32)\n",
    "output = conv_transpose2d_2x(x)\n",
    "\n",
    "# H_out = (32-1)*2 - 2*1 + 1*(4-1) + 0 + 1 = 62 - 2 + 3 + 1 = 64\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Exact 2x upsampling: {output.shape[2] == 2 * x.shape[2] and output.shape[3] == 2 * x.shape[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e81c327-260d-4c3b-9373-2670b2f2d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 32, 64])\n",
      "Output shape: torch.Size([20, 33, 63, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Non-square kernels and unequal stride with padding\n",
    "conv_transpose2d_nonsq = nn.ConvTranspose2d(16, 33, kernel_size=(3, 5), stride=(2, 1), padding=(1, 2))\n",
    "x = torch.randn(20, 16, 32, 64)\n",
    "output = conv_transpose2d_nonsq(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "abb08c26-a496-4a48-a939-942ca14fc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 32, 32])\n",
      "Output shape: torch.Size([20, 33, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Using output_padding to resolve ambiguity\n",
    "# Two different input sizes (63 and 64) would produce the same output (32) with Conv2d(stride=2)\n",
    "# output_padding helps specify we want 64, not 63\n",
    "\n",
    "conv_transpose2d_op = nn.ConvTranspose2d(16, 33, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "x = torch.randn(20, 16, 32, 32)\n",
    "output = conv_transpose2d_op(x)\n",
    "\n",
    "# H_out = (32-1)*2 - 2*1 + 1*(3-1) + 1 + 1 = 62 - 2 + 2 + 1 + 1 = 64\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa579660-8f09-477d-bc4e-3a36737c97b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape: torch.Size([1, 512, 4, 4])\n",
      "Generated image shape: torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example 5: GAN-style generator upsampling chain\n",
    "# Starting from a small spatial size and progressively upsampling\n",
    "\n",
    "class SimpleGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each layer doubles spatial dimensions: 4 -> 8 -> 16 -> 32 -> 64\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # 4 -> 8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 8 -> 16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 16 -> 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),     # 32 -> 64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "generator = SimpleGenerator()\n",
    "latent = torch.randn(1, 512, 4, 4)  # Starting from 4x4 feature map\n",
    "generated_image = generator(latent)\n",
    "\n",
    "print(f\"Latent shape: {latent.shape}\")\n",
    "print(f\"Generated image shape: {generated_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82d6083c-4c71-4df9-ac03-4a321d5aa25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 3, 64, 64])\n",
      "After Conv2d (downsample): torch.Size([1, 64, 32, 32])\n",
      "After ConvTranspose2d (upsample): torch.Size([1, 3, 64, 64])\n",
      "Spatial dimensions restored: True\n"
     ]
    }
   ],
   "source": [
    "# Example 6: Comparison of Conv2d and ConvTranspose2d (inverse spatial transform)\n",
    "H_original, W_original = 64, 64\n",
    "x_original = torch.randn(1, 3, H_original, W_original)\n",
    "\n",
    "# Downsample with Conv2d\n",
    "conv2d_down = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
    "x_down = conv2d_down(x_original)\n",
    "\n",
    "# Upsample with ConvTranspose2d (matching parameters)\n",
    "conv_transpose2d_up = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "x_up = conv_transpose2d_up(x_down)\n",
    "\n",
    "print(f\"Original shape: {x_original.shape}\")\n",
    "print(f\"After Conv2d (downsample): {x_down.shape}\")\n",
    "print(f\"After ConvTranspose2d (upsample): {x_up.shape}\")\n",
    "print(f\"Spatial dimensions restored: {x_original.shape[2:] == x_up.shape[2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e8b9df5-6a21-4cbf-b6ce-049865419f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0}\n",
      "  Calculated: (34, 34), Actual: (34, 34)\n",
      "  Match: True\n",
      "Config: {'kernel_size': 4, 'stride': 2, 'padding': 1}\n",
      "  Calculated: (64, 64), Actual: (64, 64)\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 2, 'padding': 1, 'output_padding': 1}\n",
      "  Calculated: (64, 64), Actual: (64, 64)\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0, 'dilation': 2}\n",
      "  Calculated: (36, 36), Actual: (36, 36)\n",
      "  Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula for ConvTranspose2d\n",
    "def conv_transpose2d_output_size(H_in, W_in, kernel_size, stride=1, padding=0, dilation=1, output_padding=0):\n",
    "    \"\"\"Calculate ConvTranspose2d output dimensions\"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation)\n",
    "    if isinstance(output_padding, int):\n",
    "        output_padding = (output_padding, output_padding)\n",
    "    \n",
    "    H_out = (H_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + output_padding[0] + 1\n",
    "    W_out = (W_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + output_padding[1] + 1\n",
    "    return H_out, W_out\n",
    "\n",
    "# Test with various configurations\n",
    "H_in, W_in = 32, 32\n",
    "x = torch.randn(1, 16, H_in, W_in)\n",
    "\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0},\n",
    "    {'kernel_size': 4, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1, 'output_padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0, 'dilation': 2},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    conv_t = nn.ConvTranspose2d(16, 33, **config)\n",
    "    output = conv_t(x)\n",
    "    H_out_calc, W_out_calc = conv_transpose2d_output_size(H_in, W_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: ({H_out_calc}, {W_out_calc}), Actual: {tuple(output.shape[2:])}\")\n",
    "    print(f\"  Match: {(H_out_calc, W_out_calc) == tuple(output.shape[2:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a78ac6-7a2a-4085-97c2-567647921afa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nn.ConvTranspose3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148712d9-a9ea-4d39-97cf-2a21419a89d9",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.ConvTranspose3d` layer applies a 3D transposed convolution over an input signal composed of several input planes. It is used for:\n",
    "\n",
    "- **Video generation**: Upsampling in video GANs, video prediction models\n",
    "- **Medical image reconstruction**: Upsampling in 3D U-Net for CT/MRI segmentation\n",
    "- **3D object generation**: Voxel-based generative models\n",
    "- **Volumetric super-resolution**: Increasing resolution of 3D data\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, D_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $D_{\\text{in}}$ = depth of the input (e.g., number of frames, slices)\n",
    "  - $H_{\\text{in}}$ = height of the input\n",
    "  - $W_{\\text{in}}$ = width of the input\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels\n",
    "  - $D_{\\text{out}} = (D_{\\text{in}} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1$\n",
    "  - $H_{\\text{out}} = (H_{\\text{in}} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1$\n",
    "  - $W_{\\text{out}} = (W_{\\text{in}} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2] \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1$\n",
    "\n",
    "#### Default Arguments\n",
    "```python\n",
    "nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                   output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input volume (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int or tuple): Stride of the convolution (controls upsampling factor). Default: `1`\n",
    "- `padding` (int or tuple): Zero-padding added to all sides. **Reduces** output size. Default: `0`\n",
    "- `output_padding` (int or tuple): Additional size added to one side. Must be `< stride`. Default: `0`\n",
    "- `dilation` (int or tuple): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): Only `'zeros'` is supported. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{in}}, \\frac{C_{\\text{out}}}{\\text{groups}}, \\text{kernel\\_size}[0], \\text{kernel\\_size}[1], \\text{kernel\\_size}[2])$\n",
    "\n",
    "**Note**: The weight shape is **transposed** compared to `nn.Conv3d` which has shape $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, K_d, K_h, K_w)$.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, D_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$, the transposed convolution from $x_{\\rm in}[b, c, d, h, w]$ to $y_{\\rm out}[b, c, d', h', w']$ is performed as follows:\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, d', h', w'] = &\\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{k_d=0}^{K_d-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1} \\\\\n",
    "&\\tilde{x}_{\\rm in}[b, c_{\\rm in}, d' + p_d - d_d k_d, h' + p_h - d_h k_h, w' + p_w - d_w k_w] \\\\\n",
    "&\\cdot \\omega[c_{\\rm in}, c_{\\rm out}, K_d - 1 - k_d, K_h - 1 - k_h, K_w - 1 - k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}_{\\rm in}$ is the input after zero-insertion (inserting $s_d - 1$, $s_h - 1$, and $s_w - 1$ zeros between elements along depth, height, and width respectively).\n",
    "\n",
    "Equivalently, as a **summation over input positions**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, d', h', w'] = &\\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{d=0}^{D_{\\rm in}-1} \\sum_{h=0}^{H_{\\rm in}-1} \\sum_{w=0}^{W_{\\rm in}-1} \\sum_{k_d=0}^{K_d-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1} \\\\\n",
    "&\\mathbf{1}_{\\substack{[d' = s_d d + d_d k_d - p_d + p_{\\rm out,d}] \\\\ [h' = s_h h + d_h k_h - p_h + p_{\\rm out,h}] \\\\ [w' = s_w w + d_w k_w - p_w + p_{\\rm out,w}]}} \\cdot x_{\\rm in}[b, c_{\\rm in}, d, h, w] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, k_d, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $(s_d, s_h, s_w) = $ `stride`, $(d_d, d_h, d_w) = $ `dilation`, $(p_d, p_h, p_w) = $ `padding`, $(p_{\\rm out,d}, p_{\\rm out,h}, p_{\\rm out,w}) = $ `output_padding`, $(K_d, K_h, K_w) = $ `kernel_size`, and $\\mathbf{1}_{[\\cdot]}$ is the indicator function.\n",
    "\n",
    "* For `groups`$ > 1$, the channels are split into groups analogously to regular convolutions.\n",
    "\n",
    "**Remarks**:\n",
    "1. Each input voxel \"scatters\" its influence to a $K_d \\times K_h \\times K_w$ region in the output.\n",
    "2. If `kernel_size = k` is just a number, then PyTorch automatically assumes $(K_d, K_h, K_w) = (k, k, k)$.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "255f93eb-25d3-4510-bcd0-3410c21e447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([16, 33, 3, 3, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.ConvTranspose3d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3x3x3\n",
    "conv_transpose3d = nn.ConvTranspose3d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv_transpose3d.weight.shape}\")  # (in_channels, out_channels, kD, kH, kW)\n",
    "print(f\"Bias shape: {conv_transpose3d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ed1f29a-b6c0-44ad-b461-cf4a904805fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 16, 10, 50, 100])\n",
      "Output shape: torch.Size([4, 33, 12, 52, 102])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic transposed convolution (slight upsampling)\n",
    "conv_transpose3d = nn.ConvTranspose3d(16, 33, kernel_size=3)\n",
    "x = torch.randn(4, 16, 10, 50, 100)  # (batch, channels, depth, height, width)\n",
    "output = conv_transpose3d(x)\n",
    "\n",
    "# D_out = (10-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 12\n",
    "# H_out = (50-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 52\n",
    "# W_out = (100-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 102\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c156b2bf-d5a8-4229-962f-f72d96b8233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 16, 8, 16, 16])\n",
      "Output shape: torch.Size([4, 33, 16, 32, 32])\n",
      "Exact 2x upsampling in all dims: True\n"
     ]
    }
   ],
   "source": [
    "# Example 2: 2x upsampling in all dimensions\n",
    "conv_transpose3d_2x = nn.ConvTranspose3d(16, 33, kernel_size=4, stride=2, padding=1)\n",
    "x = torch.randn(4, 16, 8, 16, 16)\n",
    "output = conv_transpose3d_2x(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Exact 2x upsampling in all dims: {all(o == 2*i for o, i in zip(output.shape[2:], x.shape[2:]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b6a18dd-1217-4d89-850f-25b224eabd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 16, 16, 32, 32])\n",
      "Output shape: torch.Size([4, 33, 16, 64, 64])\n",
      "Temporal dim preserved: True\n",
      "Spatial dims doubled: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Non-cubic kernels and unequal stride (e.g., upsample only spatially, not temporally)\n",
    "# kernel_size=(depth, height, width), stride=(sD, sH, sW), padding=(pD, pH, pW)\n",
    "conv_transpose3d_spatial = nn.ConvTranspose3d(16, 33, kernel_size=(3, 4, 4), stride=(1, 2, 2), padding=(1, 1, 1))\n",
    "x = torch.randn(4, 16, 16, 32, 32)  # Video: 16 frames, 32x32 resolution\n",
    "output = conv_transpose3d_spatial(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Temporal dim preserved: {x.shape[2] == output.shape[2]}\")\n",
    "print(f\"Spatial dims doubled: {output.shape[3] == 2*x.shape[3] and output.shape[4] == 2*x.shape[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64cbc4dd-fd4b-4dd9-ae1a-1f9d76c4f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck features: torch.Size([1, 128, 8, 8, 8])\n",
      "Skip connection: torch.Size([1, 64, 16, 16, 16])\n",
      "Decoder output: torch.Size([1, 64, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Medical imaging - 3D U-Net decoder block\n",
    "# Upsampling feature maps in a 3D segmentation network\n",
    "\n",
    "class UNet3DDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.ConvTranspose3d(in_channels, out_channels, \n",
    "                                            kernel_size=2, stride=2)  # 2x upsample\n",
    "        self.conv = nn.Conv3d(out_channels * 2, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip_connection], dim=1)  # Skip connection from encoder\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Simulate decoder block usage\n",
    "decoder_block = UNet3DDecoderBlock(128, 64)\n",
    "bottleneck_features = torch.randn(1, 128, 8, 8, 8)  # From encoder bottleneck\n",
    "skip_features = torch.randn(1, 64, 16, 16, 16)      # Skip connection from encoder\n",
    "output = decoder_block(bottleneck_features, skip_features)\n",
    "\n",
    "print(f\"Bottleneck features: {bottleneck_features.shape}\")\n",
    "print(f\"Skip connection: {skip_features.shape}\")\n",
    "print(f\"Decoder output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b21d8dd-2f35-40b5-b7c6-68e6c14ffea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape: torch.Size([1, 256, 2, 4, 4])\n",
      "Generated video shape: torch.Size([1, 3, 16, 32, 32])\n",
      "(batch, RGB channels, frames, height, width)\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Video generation - upsampling from latent to video frames\n",
    "# Starting from a compressed spatiotemporal representation\n",
    "\n",
    "class VideoGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Progressively upsample: (2,4,4) -> (4,8,8) -> (8,16,16) -> (16,32,32)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "generator = VideoGenerator()\n",
    "latent = torch.randn(1, 256, 2, 4, 4)  # Compressed representation\n",
    "video = generator(latent)\n",
    "\n",
    "print(f\"Latent shape: {latent.shape}\")\n",
    "print(f\"Generated video shape: {video.shape}\")\n",
    "print(f\"(batch, RGB channels, frames, height, width)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d817fd49-ae70-4cd1-938e-4edd78dcc7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0}\n",
      "  Calculated: (10, 18, 18), Actual: (10, 18, 18)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': 4, 'stride': 2, 'padding': 1}\n",
      "  Calculated: (16, 32, 32), Actual: (16, 32, 32)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': (3, 4, 4), 'stride': (1, 2, 2), 'padding': (1, 1, 1)}\n",
      "  Calculated: (8, 32, 32), Actual: (8, 32, 32)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': 2, 'stride': 2, 'padding': 0}\n",
      "  Calculated: (16, 32, 32), Actual: (16, 32, 32)\n",
      "  Match: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula for ConvTranspose3d\n",
    "def conv_transpose3d_output_size(D_in, H_in, W_in, kernel_size, stride=1, padding=0, dilation=1, output_padding=0):\n",
    "    \"\"\"Calculate ConvTranspose3d output dimensions\"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size, kernel_size)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation, dilation)\n",
    "    if isinstance(output_padding, int):\n",
    "        output_padding = (output_padding, output_padding, output_padding)\n",
    "    \n",
    "    D_out = (D_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + output_padding[0] + 1\n",
    "    H_out = (H_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + output_padding[1] + 1\n",
    "    W_out = (W_in - 1) * stride[2] - 2 * padding[2] + dilation[2] * (kernel_size[2] - 1) + output_padding[2] + 1\n",
    "    return D_out, H_out, W_out\n",
    "\n",
    "# Test\n",
    "D_in, H_in, W_in = 8, 16, 16\n",
    "x = torch.randn(1, 16, D_in, H_in, W_in)\n",
    "\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0},\n",
    "    {'kernel_size': 4, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': (3, 4, 4), 'stride': (1, 2, 2), 'padding': (1, 1, 1)},\n",
    "    {'kernel_size': 2, 'stride': 2, 'padding': 0},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    conv_t = nn.ConvTranspose3d(16, 33, **config)\n",
    "    output = conv_t(x)\n",
    "    D_out_calc, H_out_calc, W_out_calc = conv_transpose3d_output_size(D_in, H_in, W_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: ({D_out_calc}, {H_out_calc}, {W_out_calc}), Actual: {tuple(output.shape[2:])}\")\n",
    "    print(f\"  Match: {(D_out_calc, H_out_calc, W_out_calc) == tuple(output.shape[2:])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec2f32-0350-4d35-a0df-692ab081d158",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6250dad-c2af-4ace-8d5f-cf3770cf7485",
   "metadata": {},
   "source": [
    "#### Note on the Mathematical Formulation of Transposed Convolutions\n",
    "\n",
    "Throughout this notebook, we presented two equivalent formulations for transposed convolutions:\n",
    "\n",
    "**Formulation 1 (Flipped Kernel View):**\n",
    "$$\n",
    "y_{\\rm out}[\\ldots, j] = \\sum_{c_{\\rm in}, k} \\tilde{x}_{\\rm in}[\\ldots, j + p - d \\cdot k] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, K - 1 - k] + \\beta[c_{\\rm out}]\n",
    "$$\n",
    "\n",
    "**Formulation 2 (Scatter/Adjoint View):**\n",
    "$$\n",
    "y_{\\rm out}[\\ldots, j] = \\sum_{c_{\\rm in}, i, k} \\mathbf{1}_{[j = s \\cdot i + d \\cdot k - p + p_{\\rm out}]} \\cdot x_{\\rm in}[\\ldots, i] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, k] + \\beta[c_{\\rm out}]\n",
    "$$\n",
    "\n",
    "These formulations are **mathematically equivalent** via the change of variables $k' = K - 1 - k$. However, they represent different conceptual interpretations:\n",
    "\n",
    "- **Flipped Kernel View**: The transposed convolution can be understood as inserting zeros between input elements, then applying a regular convolution with a spatially flipped kernel. This interpretation arises naturally when deriving the transposed convolution as the gradient of a forward convolution.\n",
    "\n",
    "- **Scatter/Adjoint View**: Each input element \"scatters\" or \"broadcasts\" its value to multiple output positions, weighted by the kernel elements. This is the **interpretation that matches PyTorch's actual implementation** — the kernel weights are used directly without spatial flipping.\n",
    "\n",
    "Lets verify this behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9e544fc6-f52d-47d9-a5be-d5d20969adc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3.]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a ConvTranspose1d with a known kernel\n",
    "conv_t = nn.ConvTranspose1d(1, 1, kernel_size=3, bias=False)\n",
    "conv_t.weight.data = torch.tensor([[[1., 2., 3.]]])  # kernel = [1, 2, 3]\n",
    "\n",
    "# Single input\n",
    "x = torch.tensor([[[1.]]])  # single value at position 0\n",
    "output = conv_t(x)\n",
    "print(output)  # tensor([[[1., 2., 3.]]])  — NOT [3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aacb162f-f8cd-47e3-89e1-25aa12968cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRANSPOSED CONVOLUTIONAL LAYER COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "nn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1):\n",
      "  Input shape:  (32, 64, 50) -> (batch, in_channels, length)\n",
      "  Output shape: (32, 128, 100) -> (batch, out_channels, length_upsampled)\n",
      "  Weight shape: (64, 128, 4) -> (in_ch, out_ch, kernel)\n",
      "  Parameters:   32,896\n",
      "\n",
      "nn.ConvTranspose2d(64, 128, kernel_size=4, stride=2, padding=1):\n",
      "  Input shape:  (32, 64, 28, 28) -> (batch, in_channels, H, W)\n",
      "  Output shape: (32, 128, 56, 56) -> (batch, out_channels, H_up, W_up)\n",
      "  Weight shape: (64, 128, 4, 4) -> (in_ch, out_ch, kH, kW)\n",
      "  Parameters:   131,200\n",
      "\n",
      "nn.ConvTranspose3d(64, 128, kernel_size=4, stride=2, padding=1):\n",
      "  Input shape:  (4, 64, 8, 14, 14) -> (batch, in_channels, D, H, W)\n",
      "  Output shape: (4, 128, 16, 28, 28) -> (batch, out_channels, D_up, H_up, W_up)\n",
      "  Weight shape: (64, 128, 4, 4, 4) -> (in_ch, out_ch, kD, kH, kW)\n",
      "  Parameters:   524,416\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison of all transposed convolutional layers\n",
    "print(\"=\" * 80)\n",
    "print(\"TRANSPOSED CONVOLUTIONAL LAYER COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# nn.ConvTranspose1d\n",
    "conv_t1d = nn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "x_1d = torch.randn(32, 64, 50)\n",
    "print(f\"\\nnn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_1d.shape)} -> (batch, in_channels, length)\")\n",
    "print(f\"  Output shape: {tuple(conv_t1d(x_1d).shape)} -> (batch, out_channels, length_upsampled)\")\n",
    "print(f\"  Weight shape: {tuple(conv_t1d.weight.shape)} -> (in_ch, out_ch, kernel)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv_t1d.parameters()):,}\")\n",
    "\n",
    "# nn.ConvTranspose2d\n",
    "conv_t2d = nn.ConvTranspose2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "x_2d = torch.randn(32, 64, 28, 28)\n",
    "print(f\"\\nnn.ConvTranspose2d(64, 128, kernel_size=4, stride=2, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_2d.shape)} -> (batch, in_channels, H, W)\")\n",
    "print(f\"  Output shape: {tuple(conv_t2d(x_2d).shape)} -> (batch, out_channels, H_up, W_up)\")\n",
    "print(f\"  Weight shape: {tuple(conv_t2d.weight.shape)} -> (in_ch, out_ch, kH, kW)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv_t2d.parameters()):,}\")\n",
    "\n",
    "# nn.ConvTranspose3d\n",
    "conv_t3d = nn.ConvTranspose3d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "x_3d = torch.randn(4, 64, 8, 14, 14)\n",
    "print(f\"\\nnn.ConvTranspose3d(64, 128, kernel_size=4, stride=2, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_3d.shape)} -> (batch, in_channels, D, H, W)\")\n",
    "print(f\"  Output shape: {tuple(conv_t3d(x_3d).shape)} -> (batch, out_channels, D_up, H_up, W_up)\")\n",
    "print(f\"  Weight shape: {tuple(conv_t3d.weight.shape)} -> (in_ch, out_ch, kD, kH, kW)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv_t3d.parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b753f269-1fc9-4e32-8d94-68548751e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY FORMULAS FOR OUTPUT SIZE CALCULATION\n",
      "============================================================\n",
      "\n",
      "nn.ConvTranspose1d/ConvTranspose2d/ConvTranspose3d:\n",
      "  For each spatial dimension:\n",
      "  L_out = (L_in - 1) * stride - 2*padding + dilation*(kernel_size-1) + output_padding + 1\n",
      "\n",
      "Common recipes for exact 2x upsampling:\n",
      "  - kernel_size=4, stride=2, padding=1 -> exact doubling\n",
      "  - kernel_size=2, stride=2, padding=0 -> exact doubling\n",
      "  - kernel_size=3, stride=2, padding=1, output_padding=1 -> exact doubling\n",
      "\n",
      "Key differences from regular Conv:\n",
      "  - stride > 1 INCREASES output size (upsampling)\n",
      "  - padding DECREASES output size\n",
      "  - Weight shape is (in_ch, out_ch, ...) not (out_ch, in_ch, ...)\n"
     ]
    }
   ],
   "source": [
    "# Key formulas for output size calculation\n",
    "print(\"KEY FORMULAS FOR OUTPUT SIZE CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"nn.ConvTranspose1d/ConvTranspose2d/ConvTranspose3d:\")\n",
    "print(\"  For each spatial dimension:\")\n",
    "print(\"  L_out = (L_in - 1) * stride - 2*padding + dilation*(kernel_size-1) + output_padding + 1\")\n",
    "print()\n",
    "print(\"Common recipes for exact 2x upsampling:\")\n",
    "print(\"  - kernel_size=4, stride=2, padding=1 -> exact doubling\")\n",
    "print(\"  - kernel_size=2, stride=2, padding=0 -> exact doubling\")\n",
    "print(\"  - kernel_size=3, stride=2, padding=1, output_padding=1 -> exact doubling\")\n",
    "print()\n",
    "print(\"Key differences from regular Conv:\")\n",
    "print(\"  - stride > 1 INCREASES output size (upsampling)\")\n",
    "print(\"  - padding DECREASES output size\")\n",
    "print(\"  - Weight shape is (in_ch, out_ch, ...) not (out_ch, in_ch, ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7cb560b-6c48-4d4f-bb36-2b981220d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV vs CONVTRANSPOSE: INVERSE SPATIAL TRANSFORMATIONS\n",
      "============================================================\n",
      "\n",
      "1D Example:\n",
      "  Original: (1, 16, 100)\n",
      "  After Conv1d(stride=2): (1, 32, 50)\n",
      "  After ConvTranspose1d(stride=2): (1, 16, 100)\n",
      "\n",
      "2D Example:\n",
      "  Original: (1, 3, 64, 64)\n",
      "  After Conv2d(stride=2): (1, 64, 32, 32)\n",
      "  After ConvTranspose2d(stride=2): (1, 3, 64, 64)\n",
      "\n",
      "3D Example:\n",
      "  Original: (1, 3, 16, 32, 32)\n",
      "  After Conv3d(stride=2): (1, 64, 8, 16, 16)\n",
      "  After ConvTranspose3d(stride=2): (1, 3, 16, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Comparison: Conv vs ConvTranspose (inverse spatial transformations)\n",
    "print(\"CONV vs CONVTRANSPOSE: INVERSE SPATIAL TRANSFORMATIONS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# 1D example\n",
    "print(\"1D Example:\")\n",
    "x1d = torch.randn(1, 16, 100)\n",
    "conv1d = nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1)\n",
    "conv_t1d = nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "print(f\"  Original: {tuple(x1d.shape)}\")\n",
    "print(f\"  After Conv1d(stride=2): {tuple(conv1d(x1d).shape)}\")\n",
    "print(f\"  After ConvTranspose1d(stride=2): {tuple(conv_t1d(conv1d(x1d)).shape)}\")\n",
    "print()\n",
    "\n",
    "# 2D example\n",
    "print(\"2D Example:\")\n",
    "x2d = torch.randn(1, 3, 64, 64)\n",
    "conv2d = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
    "conv_t2d = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "print(f\"  Original: {tuple(x2d.shape)}\")\n",
    "print(f\"  After Conv2d(stride=2): {tuple(conv2d(x2d).shape)}\")\n",
    "print(f\"  After ConvTranspose2d(stride=2): {tuple(conv_t2d(conv2d(x2d)).shape)}\")\n",
    "print()\n",
    "\n",
    "# 3D example\n",
    "print(\"3D Example:\")\n",
    "x3d = torch.randn(1, 3, 16, 32, 32)\n",
    "conv3d = nn.Conv3d(3, 64, kernel_size=4, stride=2, padding=1)\n",
    "conv_t3d = nn.ConvTranspose3d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "print(f\"  Original: {tuple(x3d.shape)}\")\n",
    "print(f\"  After Conv3d(stride=2): {tuple(conv3d(x3d).shape)}\")\n",
    "print(f\"  After ConvTranspose3d(stride=2): {tuple(conv_t3d(conv3d(x3d)).shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404d7a1-c627-4c0c-b708-af64ebffe576",
   "metadata": {},
   "source": [
    "## Recurrent Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6f0f5-3928-46b8-9aca-d4e6c48ed794",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nn.RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570f391-84ec-420f-b123-835cc8b14071",
   "metadata": {},
   "source": [
    "The `nn.RNN` layer (also known as a vanilla RNN or Elman RNN) processes sequential data by maintaining a hidden state across time steps. It is used in:\n",
    "\n",
    "- **Natural Language Processing (NLP)**: Text classification, sentiment analysis, language modeling\n",
    "- **Time Series Analysis**: Stock prediction, weather forecasting, sensor data\n",
    "- **Speech Recognition**: Processing audio sequences\n",
    "- **Sequence-to-Sequence Tasks**: Machine translation (often with LSTM/GRU variants)\n",
    "- **Video Analysis**: Processing frames sequentially\n",
    "\n",
    "\n",
    "#### Input Shape\n",
    "\n",
    "With `batch_first=False` (default):\n",
    "- **Input $X$**: $(L, B, H_{\\text{in}})$\n",
    "- **Initial hidden state $h_0$**: $(1, B, H_{\\text{out}})$\n",
    "\n",
    "With `batch_first=True`:\n",
    "- **Input $X$**: $(B, L, H_{\\text{in}})$\n",
    "- **Initial hidden state $h_0$**: $(1, B, H_{\\text{out}})$ (unchanged!)\n",
    "\n",
    "#### Output Shape\n",
    "\n",
    "The RNN returns **two** tensors:\n",
    "\n",
    "```python\n",
    "output, h_n = rnn(X, h_0)\n",
    "```\n",
    "\n",
    "| Return Value | Description | Shape (with `batch_first=True`) |\n",
    "|--------------|-------------|---------------------------------|\n",
    "| `output` | Hidden states at **ALL** time steps | $(B, L, H_{\\text{out}})$ |\n",
    "| `h_n` | Hidden state at the **LAST** time step only | $(1, B, H_{\\text{out}})$ |\n",
    "\n",
    "**Key Relationship**: `output[:, -1, :] == h_n[0]` (for single-layer, unidirectional RNN)\n",
    "\n",
    "#### Default Arguments\n",
    "\n",
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "```python\n",
    "nn.RNN(input_size, hidden_size, num_layers=1, nonlinearity='tanh', \n",
    "       bias=True, batch_first=False, dropout=0, bidirectional=False)\n",
    "```\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `input_size` | int | required | Number of features in input $H_{\\text{in}}$ |\n",
    "| `hidden_size` | int | required | Number of features in hidden state $H_{\\text{out}}$ |\n",
    "| `num_layers` | int | 1 | Number of stacked RNN layers |\n",
    "| `nonlinearity` | str | 'tanh' | Activation function: 'tanh' or 'relu' |\n",
    "| `bias` | bool | True | Whether to use bias terms $b_{ih}$ and $b_{hh}$ |\n",
    "| `batch_first` | bool | False | If True, input shape is $(B, L, H_{\\text{in}})$ |\n",
    "| `dropout` | float | 0 | Dropout probability between layers (if `num_layers > 1`) |\n",
    "| `bidirectional` | bool | False | If True, becomes bidirectional RNN |\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### 4. Mathematical Formulation\n",
    "\n",
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "For **each time step** $t = 0, 1, 2, \\ldots, L-1$, the RNN computes:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t^a =& \\tanh\\Big( x_t^b W_{ih}{a b} + b_{ih}^a + h_{t-1}^b W_{hh}^{a b} + b_{hh}^a \\Big)\\,,\n",
    "\\\\\n",
    "\\text{in vector notation:}&\n",
    "\\nonumber\n",
    "\\\\\n",
    "&\\boxed{h_t = \\tanh\\Big( x_t W_{ih}^T + b_{ih} + h_{t-1} W_{hh}^T + b_{hh} \\Big)}\n",
    "\\\\\n",
    "\\nonumber\n",
    "\\\\\n",
    "=&\\tanh\\Big( \\underbrace{x_t W_{ih}^T + b_{ih}}_{\\text{input contribution}} + \\underbrace{h_{t-1} W_{hh}^T + b_{hh}}_{\\text{recurrent contribution}} \\Big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where:\n",
    "- $x_t \\in \\mathbb{R}^{H_{\\text{in}}}$ is the input at time step $t$\n",
    "- $h_t \\in \\mathbb{R}^{H_{\\text{out}}}$ is the hidden state at time step $t$\n",
    "- $h_{t-1} \\in \\mathbb{R}^{H_{\\text{out}}}$ is the hidden state from the previous time step\n",
    "- $h_{-1} = h_0$ is the initial hidden state (defaults to zeros)\n",
    "\n",
    "and also the subscripts indicate the **transformation direction**:\n",
    "\n",
    "| Notation | Meaning | Transforms | Shape |\n",
    "|----------|---------|------------|-------|\n",
    "| $W_{ih}$ | \"**i**nput to **h**idden\" | $x_t \\to$ hidden space | $(H_{\\text{out}}, H_{\\text{in}})$ |\n",
    "| $W_{hh}$ | \"**h**idden to **h**idden\" | $h_{t-1} \\to$ hidden space | $(H_{\\text{out}}, H_{\\text{out}})$ |\n",
    "| $b_{ih}$ | \"**i**nput to **h**idden\" bias | bias for input transformation | $(H_{\\text{out}},)$ |\n",
    "| $b_{hh}$ | \"**h**idden to **h**idden\" bias | bias for recurrent transformation | $(H_{\\text{out}},)$ |\n",
    "\n",
    "**Important**: The subscript is NOT an index! It is a naming convention. All elements inside the weights $W_{ih}, W_{hh}$ and biases $b_{ih}, b_{hh}$ are trainable.\n",
    "\n",
    "\n",
    "\n",
    "**Remarks**:\n",
    "1. The above equation is applied **once per time step**, so for a sequence of length $L$, it is applied **$L$ times**.\n",
    "\n",
    "2. One call to `nn.RNN` processes the **entire sequence** automatically:\n",
    "\n",
    "```\n",
    "for t = 0, 1, 2, ..., L-1:\n",
    "    h_t = tanh(x_t @ W_ih.T + b_ih + h_{t-1} @ W_hh.T + b_hh)\n",
    "```\n",
    "3. **Crucially**: The **same weights** $W_{ih}$, $W_{hh}$, $b_{ih}$, $b_{hh}$ are reused at every time step. This is called **weight sharing** and is what makes RNNs able to handle variable-length sequences.\n",
    "\n",
    "4. One `nn.RNN` has two outputs:\n",
    "    * $[h_0, h_1, h_2, \\ldots, h_{L-1}]$ stacked along the sequence dimension\n",
    "    * $h_{L-1}$ (the final hidden state only)\n",
    "\n",
    "       \n",
    "    and it can be visualized diagrammatically as follows:\n",
    "\n",
    " Diagrammatically it has the following form:\n",
    "```\n",
    "                    SAME W_ih, W_hh, b_ih, b_hh used at ALL time steps\n",
    "                    ↓              ↓              ↓              ↓\n",
    "    \n",
    "    Time:        t=0            t=1            t=2            t=L-1\n",
    "    \n",
    "    Input:       x_0            x_1            x_2     ...    x_{L-1}\n",
    "                  ↓              ↓              ↓              ↓\n",
    "                ┌───┐          ┌───┐          ┌───┐          ┌───┐\n",
    "    h_0 ───────→│RNN│────h_1──→│RNN│────h_2──→│RNN│── ... ──→│RNN│────→ h_n\n",
    "    (init)      │   │          │   │          │   │          │   │     (final)\n",
    "                └───┘          └───┘          └───┘          └───┘\n",
    "                  ↓              ↓              ↓              ↓\n",
    "              output_0      output_1      output_2       output_{L-1}\n",
    "\n",
    "   ←─────────────────────── outputs: (all h's stacked), h_n ────────────────────→\n",
    "```\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "823e371d-dfa7-45c9-a9a8-71acb20c9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters:\n",
      "  W_ih (weight_ih_l0): torch.Size([20, 10])  → (H_out, H_in) = (20, 10)\n",
      "  W_hh (weight_hh_l0): torch.Size([20, 20]) → (H_out, H_out) = (20, 20)\n",
      "  b_ih (bias_ih_l0):   torch.Size([20])      → (H_out,) = (20,)\n",
      "  b_hh (bias_hh_l0):   torch.Size([20])      → (H_out,) = (20,)\n",
      "\n",
      "All require gradients: True\n"
     ]
    }
   ],
   "source": [
    "# Create an RNN and inspect its parameters\n",
    "input_size = 10   # H_in\n",
    "hidden_size = 20  # H_out\n",
    "\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "print(\"Trainable Parameters:\")\n",
    "print(f\"  W_ih (weight_ih_l0): {rnn.weight_ih_l0.shape}  → (H_out, H_in) = ({hidden_size}, {input_size})\")\n",
    "print(f\"  W_hh (weight_hh_l0): {rnn.weight_hh_l0.shape} → (H_out, H_out) = ({hidden_size}, {hidden_size})\")\n",
    "print(f\"  b_ih (bias_ih_l0):   {rnn.bias_ih_l0.shape}      → (H_out,) = ({hidden_size},)\")\n",
    "print(f\"  b_hh (bias_hh_l0):   {rnn.bias_hh_l0.shape}      → (H_out,) = ({hidden_size},)\")\n",
    "print(f\"\\nAll require gradients: {all(p.requires_grad for p in rnn.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1959d193-6230-4cfe-8df9-f6eb451c1921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X shape: torch.Size([1, 2, 3])\n",
      "Output shape: torch.Size([1, 2, 4])\n",
      "h_n shape: torch.Size([1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Verify: Manual computation matches nn.RNN output\n",
    "\n",
    "# Small dimensions for clarity\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "seq_length = 2\n",
    "batch_size = 1\n",
    "\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "# Input sequence: shape (B, L, H_in) = (1, 2, 3)\n",
    "X = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "# Initial hidden state: shape (1, B, H_out) = (1, 1, 4)\n",
    "h_0 = torch.zeros(1, batch_size, hidden_size)\n",
    "\n",
    "# Forward pass through nn.RNN\n",
    "output, h_n = rnn(X, h_0)\n",
    "\n",
    "print(\"Input X shape:\", X.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"h_n shape:\", h_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d04ea5ec-ee76-46bb-a973-d817362320c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TIME STEP t=0\n",
      "============================================================\n",
      "x_0 = [ 0.41716644 -1.9405079  -1.776579  ]\n",
      "h_{-1} = [0. 0. 0. 0.]\n",
      "\n",
      "Input contribution:     x_0 @ W_ih.T + b_ih\n",
      "Recurrent contribution: h_{-1} @ W_hh.T + b_hh\n",
      "\n",
      "Manual h_0 = [ 0.43191716  0.7227674   0.16095464 -0.1893854 ]\n",
      "nn.RNN h_0 = [ 0.43191716  0.7227674   0.16095464 -0.18938546]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# Manual step-by-step computation\n",
    "W_ih = rnn.weight_ih_l0  # (4, 3)\n",
    "W_hh = rnn.weight_hh_l0  # (4, 4)\n",
    "b_ih = rnn.bias_ih_l0    # (4,)\n",
    "b_hh = rnn.bias_hh_l0    # (4,)\n",
    "\n",
    "# Extract individual time step inputs\n",
    "x_0 = X[0, 0, :]  # First time step: shape (3,)\n",
    "x_1 = X[0, 1, :]  # Second time step: shape (3,)\n",
    "\n",
    "# Initial hidden state\n",
    "h_prev = h_0[0, 0, :]  # shape (4,)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TIME STEP t=0\")\n",
    "print(\"=\"*60)\n",
    "print(f\"x_0 = {x_0.detach().numpy()}\")\n",
    "print(f\"h_{{-1}} = {h_prev.detach().numpy()}\")\n",
    "\n",
    "# Manual computation for t=0\n",
    "# h_0 = tanh(x_0 @ W_ih.T + b_ih + h_{-1} @ W_hh.T + b_hh)\n",
    "input_contribution_0 = x_0 @ W_ih.T + b_ih\n",
    "recurrent_contribution_0 = h_prev @ W_hh.T + b_hh\n",
    "h_0_manual = torch.tanh(input_contribution_0 + recurrent_contribution_0)\n",
    "\n",
    "print(f\"\\nInput contribution:     x_0 @ W_ih.T + b_ih\")\n",
    "print(f\"Recurrent contribution: h_{{-1}} @ W_hh.T + b_hh\")\n",
    "print(f\"\\nManual h_0 = {h_0_manual.detach().numpy()}\")\n",
    "print(f\"nn.RNN h_0 = {output[0, 0, :].detach().numpy()}\")\n",
    "print(f\"Match: {torch.allclose(h_0_manual, output[0, 0, :])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bbcfd75b-35ce-428d-b1d4-5acdcbd31ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TIME STEP t=1\n",
      "============================================================\n",
      "x_1 = [1.1705459  1.7482485  0.73884743]\n",
      "h_0 = [ 0.43191716  0.7227674   0.16095464 -0.1893854 ]\n",
      "\n",
      "Manual h_1 = [-0.8093596  0.8595278 -0.6639339  0.5735556]\n",
      "nn.RNN h_1 = [-0.8093596   0.8595278  -0.6639339   0.57355565]\n",
      "Match: True\n",
      "\n",
      "============================================================\n",
      "VERIFY: output[:, -1, :] == h_n[0]\n",
      "============================================================\n",
      "output[:, -1, :] = [-0.8093596   0.8595278  -0.6639339   0.57355565]\n",
      "h_n[0]           = [-0.8093596   0.8595278  -0.6639339   0.57355565]\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TIME STEP t=1\")\n",
    "print(\"=\"*60)\n",
    "print(f\"x_1 = {x_1.detach().numpy()}\")\n",
    "print(f\"h_0 = {h_0_manual.detach().numpy()}\")\n",
    "\n",
    "# Manual computation for t=1\n",
    "# h_1 = tanh(x_1 @ W_ih.T + b_ih + h_0 @ W_hh.T + b_hh)\n",
    "input_contribution_1 = x_1 @ W_ih.T + b_ih\n",
    "recurrent_contribution_1 = h_0_manual @ W_hh.T + b_hh\n",
    "h_1_manual = torch.tanh(input_contribution_1 + recurrent_contribution_1)\n",
    "\n",
    "print(f\"\\nManual h_1 = {h_1_manual.detach().numpy()}\")\n",
    "print(f\"nn.RNN h_1 = {output[0, 1, :].detach().numpy()}\")\n",
    "print(f\"Match: {torch.allclose(h_1_manual, output[0, 1, :])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFY: output[:, -1, :] == h_n[0]\")\n",
    "print(\"=\"*60)\n",
    "print(f\"output[:, -1, :] = {output[0, -1, :].detach().numpy()}\")\n",
    "print(f\"h_n[0]           = {h_n[0, 0, :].detach().numpy()}\")\n",
    "print(f\"Match: {torch.allclose(output[:, -1, :], h_n[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69a383-f880-4c02-8165-a39845a63bda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c4cdd-5b5b-4019-a93e-8953b2fa9070",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "RNNs require **continuous vector inputs**, but words are **discrete tokens** (integers).\n",
    "\n",
    "\n",
    "An embedding layer maps each integer (word index) to a learnable dense vector:\n",
    "\n",
    "$$\\text{word\\_index} \\xrightarrow{\\text{nn.Embedding}} \\text{embedding\\_vector} \\in \\mathbb{R}^{d}$$\n",
    "\n",
    "The embedding matrix $E \\in \\mathbb{R}^{V \\times d}$ where:\n",
    "- $V$ = vocabulary size (number of unique words)\n",
    "- $d$ = embedding dimension\n",
    "\n",
    "**The embedding matrix is also trainable!**\n",
    "\n",
    "\n",
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "Let's trace exactly what happens when the sentence **\"I love Machine Learning\"** passes through an RNN.\n",
    "\n",
    "\n",
    "- **Sentence**: \"I love Machine Learning\"\n",
    "- **Tokens**: [\"I\", \"love\", \"Machine\", \"Learning\"]\n",
    "- **Sequence length** $L = 4$\n",
    "- **Vocabulary**: {\"I\": 0, \"love\": 1, \"Machine\": 2, \"Learning\": 3}\n",
    "- **Embedding dimension** $d = 8$\n",
    "- **Hidden size** $H_{\\text{out}} = 6$\n",
    "\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c61f41de-d09e-40fc-a0d6-f65906704fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I love Machine Learning'\n",
      "Tokens: ['I', 'love', 'Machine', 'Learning']\n",
      "Sequence length L = 4\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love Machine Learning\"\n",
    "tokens = sentence.split()\n",
    "print(f\"Sentence: '{sentence}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Sequence length L = {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "47f8ff40-fada-4dcb-872c-1a294918b330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (word → index):\n",
      "  'I' → 0\n",
      "  'love' → 1\n",
      "  'Machine' → 2\n",
      "  'Learning' → 3\n",
      "\n",
      "Word indices tensor: tensor([[0, 1, 2, 3]])\n",
      "Shape: torch.Size([1, 4]) → (batch_size=1, seq_length=4)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create vocabulary (word to index mapping)\n",
    "vocab = {word: idx for idx, word in enumerate(tokens)}\n",
    "print(\"Vocabulary (word → index):\")\n",
    "for word, idx in vocab.items():\n",
    "    print(f\"  '{word}' → {idx}\")\n",
    "\n",
    "# Convert sentence to indices\n",
    "word_indices = torch.tensor([[vocab[word] for word in tokens]])  # Shape: (1, 4)\n",
    "print(f\"\\nWord indices tensor: {word_indices}\")\n",
    "print(f\"Shape: {word_indices.shape} → (batch_size=1, seq_length=4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d1ad5f52-aa65-47ca-9a42-bf5504607191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size V = 4\n",
      "Embedding dimension d = H_in = 8\n",
      "Hidden size H_out = 6\n",
      "\n",
      "Embedding matrix shape: torch.Size([4, 8]) → (V, d) = (4, 8)\n",
      "W_ih shape: torch.Size([6, 8]) → (H_out, H_in) = (6, 8)\n",
      "W_hh shape: torch.Size([6, 6]) → (H_out, H_out) = (6, 6)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define dimensions\n",
    "vocab_size = len(vocab)  # V = 4\n",
    "embed_dim = 8            # d = 8 (embedding dimension = H_in for RNN)\n",
    "hidden_size = 6          # H_out = 6\n",
    "\n",
    "print(f\"Vocabulary size V = {vocab_size}\")\n",
    "print(f\"Embedding dimension d = H_in = {embed_dim}\")\n",
    "print(f\"Hidden size H_out = {hidden_size}\")\n",
    "\n",
    "# Create layers\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "print(f\"\\nEmbedding matrix shape: {embedding.weight.shape} → (V, d) = ({vocab_size}, {embed_dim})\")\n",
    "print(f\"W_ih shape: {rnn.weight_ih_l0.shape} → (H_out, H_in) = ({hidden_size}, {embed_dim})\")\n",
    "print(f\"W_hh shape: {rnn.weight_hh_l0.shape} → (H_out, H_out) = ({hidden_size}, {hidden_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "446bba4c-96c9-40c0-b0ea-6429f6e7fd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: Embedding Layer\n",
      "============================================================\n",
      "Input shape:  torch.Size([1, 4]) → (B=1, L=4)\n",
      "Output shape: torch.Size([1, 4, 8]) → (B=1, L=4, d=8)\n",
      "\n",
      "Each word is now a vector of dimension 8:\n",
      "  'I' (index 0) → vector of shape torch.Size([8])\n",
      "  'love' (index 1) → vector of shape torch.Size([8])\n",
      "  'Machine' (index 2) → vector of shape torch.Size([8])\n",
      "  'Learning' (index 3) → vector of shape torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply embedding layer\n",
    "# word_indices: (1, 4) → embedded: (1, 4, 8)\n",
    "embedded = embedding(word_indices)\n",
    "\n",
    "print(\"STEP: Embedding Layer\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input shape:  {word_indices.shape} → (B=1, L=4)\")\n",
    "print(f\"Output shape: {embedded.shape} → (B=1, L=4, d=8)\")\n",
    "print(\"\\nEach word is now a vector of dimension 8:\")\n",
    "for i, word in enumerate(tokens):\n",
    "    print(f\"  '{word}' (index {vocab[word]}) → vector of shape {embedded[0, i, :].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bbd83c44-8423-4d44-97ef-c72636113cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: RNN Forward Pass\n",
      "============================================================\n",
      "Input (embedded) shape: torch.Size([1, 4, 8]) → (B=1, L=4, H_in=8)\n",
      "Initial h_0 shape:      torch.Size([1, 1, 6]) → (num_layers=1, B=1, H_out=6)\n",
      "Output shape:           torch.Size([1, 4, 6]) → (B=1, L=4, H_out=6)\n",
      "Final h_n shape:        torch.Size([1, 1, 6]) → (num_layers=1, B=1, H_out=6)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Apply RNN\n",
    "h_0 = torch.zeros(1, 1, hidden_size)  # Initial hidden state\n",
    "output, h_n = rnn(embedded, h_0)\n",
    "\n",
    "print(\"STEP: RNN Forward Pass\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input (embedded) shape: {embedded.shape} → (B=1, L=4, H_in=8)\")\n",
    "print(f\"Initial h_0 shape:      {h_0.shape} → (num_layers=1, B=1, H_out=6)\")\n",
    "print(f\"Output shape:           {output.shape} → (B=1, L=4, H_out=6)\")\n",
    "print(f\"Final h_n shape:        {h_n.shape} → (num_layers=1, B=1, H_out=6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aa574fdd-d6ec-462d-ad4a-515f40a770fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED TRACE: Processing 'I love Machine Learning'\n",
      "======================================================================\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "TIME STEP t=0: Processing word 'I'\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  x_0 = embedding('I')\n",
      "  x_0.shape = torch.Size([8]) → (H_in=8,)\n",
      "  h_{-1} = previous hidden state\n",
      "  h_{-1}.shape = torch.Size([6]) → (H_out=6,)\n",
      "\n",
      "  Computation:\n",
      "    Input contribution:     x_0 @ W_ih.T + b_ih\n",
      "    Recurrent contribution: h_{-1} @ W_hh.T + b_hh\n",
      "    h_0 = tanh(input_contrib + recurrent_contrib)\n",
      "\n",
      "  Result: h_0 = [-0.2203  0.3922  0.4033 -0.0219 -0.1832  0.6363]\n",
      "  Verify: True\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "TIME STEP t=1: Processing word 'love'\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  x_1 = embedding('love')\n",
      "  x_1.shape = torch.Size([8]) → (H_in=8,)\n",
      "  h_{0} = previous hidden state\n",
      "  h_{0}.shape = torch.Size([6]) → (H_out=6,)\n",
      "\n",
      "  Computation:\n",
      "    Input contribution:     x_1 @ W_ih.T + b_ih\n",
      "    Recurrent contribution: h_{0} @ W_hh.T + b_hh\n",
      "    h_1 = tanh(input_contrib + recurrent_contrib)\n",
      "\n",
      "  Result: h_1 = [-0.2657  0.2674 -0.5499 -0.163   0.9374  0.8149]\n",
      "  Verify: True\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "TIME STEP t=2: Processing word 'Machine'\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  x_2 = embedding('Machine')\n",
      "  x_2.shape = torch.Size([8]) → (H_in=8,)\n",
      "  h_{1} = previous hidden state\n",
      "  h_{1}.shape = torch.Size([6]) → (H_out=6,)\n",
      "\n",
      "  Computation:\n",
      "    Input contribution:     x_2 @ W_ih.T + b_ih\n",
      "    Recurrent contribution: h_{1} @ W_hh.T + b_hh\n",
      "    h_2 = tanh(input_contrib + recurrent_contrib)\n",
      "\n",
      "  Result: h_2 = [-0.3633  0.3803  0.1709 -0.1456  0.3949  0.5767]\n",
      "  Verify: True\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "TIME STEP t=3: Processing word 'Learning'\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  x_3 = embedding('Learning')\n",
      "  x_3.shape = torch.Size([8]) → (H_in=8,)\n",
      "  h_{2} = previous hidden state\n",
      "  h_{2}.shape = torch.Size([6]) → (H_out=6,)\n",
      "\n",
      "  Computation:\n",
      "    Input contribution:     x_3 @ W_ih.T + b_ih\n",
      "    Recurrent contribution: h_{2} @ W_hh.T + b_hh\n",
      "    h_3 = tanh(input_contrib + recurrent_contrib)\n",
      "\n",
      "  Result: h_3 = [-0.2539  0.4837  0.6632  0.0011  0.3939  0.5933]\n",
      "  Verify: True\n",
      "\n",
      "======================================================================\n",
      "FINAL OUTPUT\n",
      "======================================================================\n",
      "h_n (final hidden state) = h_3 = [-0.2539  0.4837  0.6632  0.0011  0.3939  0.5933]\n",
      "Verify output[:, -1, :] == h_n[0]: True\n"
     ]
    }
   ],
   "source": [
    "# What exactly happens in details:\n",
    "\n",
    "# Extract weights\n",
    "W_ih = rnn.weight_ih_l0  # (6, 8)\n",
    "W_hh = rnn.weight_hh_l0  # (6, 6)\n",
    "b_ih = rnn.bias_ih_l0    # (6,)\n",
    "b_hh = rnn.bias_hh_l0    # (6,)\n",
    "\n",
    "print(\"DETAILED TRACE: Processing 'I love Machine Learning'\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "h_prev = h_0[0, 0, :]  # Start with zeros\n",
    "\n",
    "for t, word in enumerate(tokens):\n",
    "    x_t = embedded[0, t, :]  # Embedding for current word\n",
    "    \n",
    "    # The RNN equation:\n",
    "    # h_t = tanh(x_t @ W_ih.T + b_ih + h_{t-1} @ W_hh.T + b_hh)\n",
    "    input_contrib = x_t @ W_ih.T + b_ih\n",
    "    recurrent_contrib = h_prev @ W_hh.T + b_hh\n",
    "    h_t_manual = torch.tanh(input_contrib + recurrent_contrib)\n",
    "    \n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"TIME STEP t={t}: Processing word '{word}'\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    print(f\"  x_{t} = embedding('{word}')\")\n",
    "    print(f\"  x_{t}.shape = {x_t.shape} → (H_in=8,)\")\n",
    "    print(f\"  h_{{{t-1}}} = previous hidden state\")\n",
    "    print(f\"  h_{{{t-1}}}.shape = {h_prev.shape} → (H_out=6,)\")\n",
    "    print(f\"\")\n",
    "    print(f\"  Computation:\")\n",
    "    print(f\"    Input contribution:     x_{t} @ W_ih.T + b_ih\")\n",
    "    print(f\"    Recurrent contribution: h_{{{t-1}}} @ W_hh.T + b_hh\")\n",
    "    print(f\"    h_{t} = tanh(input_contrib + recurrent_contrib)\")\n",
    "    print(f\"\")\n",
    "    print(f\"  Result: h_{t} = {h_t_manual.detach().numpy().round(4)}\")\n",
    "    print(f\"  Verify: {torch.allclose(h_t_manual, output[0, t, :])}\")\n",
    "    \n",
    "    h_prev = h_t_manual  # Update for next time step\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINAL OUTPUT\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"h_n (final hidden state) = h_3 = {h_n[0, 0, :].detach().numpy().round(4)}\")\n",
    "print(f\"Verify output[:, -1, :] == h_n[0]: {torch.allclose(output[:, -1, :], h_n[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46553c3-058a-4976-bc6f-4a3e44da4732",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "```\n",
    "INPUT SENTENCE: \"I love Machine Learning\"\n",
    "                 ↓\n",
    "TOKENIZE:       [\"I\", \"love\", \"Machine\", \"Learning\"]\n",
    "                 ↓\n",
    "WORD INDICES:   [0, 1, 2, 3]\n",
    "                 ↓\n",
    "                 ↓ nn.Embedding(vocab_size=4, embed_dim=8)\n",
    "                 ↓\n",
    "EMBEDDINGS:     Shape (1, 4, 8)\n",
    "                [e_0, e_1, e_2, e_3] where each e_i ∈ ℝ^8\n",
    "\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "                           RNN PROCESSING\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "t=0: \"I\"              t=1: \"love\"          t=2: \"Machine\"       t=3: \"Learning\"\n",
    "     ↓                     ↓                    ↓                     ↓\n",
    "   e_0 ∈ ℝ^8            e_1 ∈ ℝ^8           e_2 ∈ ℝ^8            e_3 ∈ ℝ^8\n",
    "     ↓                     ↓                    ↓                     ↓\n",
    "  ┌─────┐               ┌─────┐              ┌─────┐              ┌─────┐\n",
    "  │     │               │     │              │     │              │     │\n",
    "→ │ RNN │─── h_0 ─────→ │ RNN │─── h_1 ────→ │ RNN │─── h_2 ────→ │ RNN │─── h_3 → (h_n)\n",
    "  │     │    ∈ ℝ^6      │     │    ∈ ℝ^6     │     │    ∈ ℝ^6     │     │    ∈ ℝ^6\n",
    "  └─────┘               └─────┘              └─────┘              └─────┘\n",
    "     ↓                     ↓                    ↓                     ↓\n",
    " output_0              output_1             output_2             output_3\n",
    "\n",
    "h_{-1}=zeros\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "EQUATION AT EACH STEP:\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  h_t = tanh( x_t @ W_ih.T + b_ih  +  h_{t-1} @ W_hh.T + b_hh )             │\n",
    "│              └───────┬───────┘       └──────────┬──────────┘               │\n",
    "│              input contribution      recurrent contribution                 │\n",
    "│              (8,)@(8,6)=(6,)         (6,)@(6,6)=(6,)                        │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "OUTPUT:\n",
    "  • output: shape (1, 4, 6) → hidden states at ALL time steps [h_0, h_1, h_2, h_3]\n",
    "  • h_n:    shape (1, 1, 6) → FINAL hidden state h_3 only\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a3574-d469-4b49-aeac-d495ebc822a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Multilayer nn.RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb07ca-aaa3-4bd2-a432-ddf6deb25f1c",
   "metadata": {},
   "source": [
    "**Multi-Layer Parameter Naming**\n",
    "\n",
    "In the above examples we used `l0` to extract the parameters. But why we took `l0` ?\n",
    "\n",
    "The `l0` means **\"layer 0\"** (zero-indexed). For a single-layer RNN, you only have layer 0:\n",
    "\n",
    "| Parameter | Meaning |\n",
    "|-----------|---------|\n",
    "| `weight_ih_l0` | $W_{ih}^{(0)}$ — layer 0, input→hidden weights |\n",
    "| `weight_hh_l0` | $W_{hh}^{(0)}$ — layer 0, hidden→hidden weights |\n",
    "| `bias_ih_l0` | $b_{ih}^{(0)}$ — layer 0, input→hidden bias |\n",
    "| `bias_hh_l0` | $b_{hh}^{(0)}$ — layer 0, hidden→hidden bias |\n",
    "\n",
    "\n",
    "\n",
    "**Multi-Layer RNN (`num_layers > 1`)**\n",
    "\n",
    "With `num_layers=3`, we get parameters `l0`, `l1`, `l2`:\n",
    "\n",
    "\n",
    "| Layer | Input to that layer | $W_{ih}$ shape |\n",
    "|-------|---------------------|----------------|\n",
    "| Layer 0 | Original input $x_t$ | $(H_{out}, H_{in})$ |\n",
    "| Layer 1 | Output of layer 0: $h_t^{(0)}$ | $(H_{out}, H_{out})$ |\n",
    "| Layer 2 | Output of layer 1: $h_t^{(1)}$ | $(H_{out}, H_{out})$ |\n",
    "\n",
    "\n",
    "\n",
    "**The Equation for Layer $\\ell$**\n",
    "\n",
    "$$h_t^{(\\ell)} = \\tanh\\Big( h_t^{(\\ell-1)} W_{ih}^{(\\ell)T} + b_{ih}^{(\\ell)} + h_{t-1}^{(\\ell)} W_{hh}^{(\\ell)T} + b_{hh}^{(\\ell)} \\Big)$$\n",
    "\n",
    "where $h_t^{(-1)} = x_t$ (the original input).\n",
    "\n",
    "**Key point**: Every layer uses the **same equation**,  only the input differs.\n",
    "\n",
    "\n",
    "\n",
    "**Data Flow Diagram**\n",
    "```\n",
    "Input x_t → [Layer 0] → h_t^(0) → [Layer 1] → h_t^(1) → [Layer 2] → h_t^(2)\n",
    "↑                      ↑                      ↑\n",
    "h_{t-1}^(0)           h_{t-1}^(1)            h_{t-1}^(2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**Output Shapes**\n",
    "\n",
    "```python\n",
    "output, h_n = rnn(x, h_0)\n",
    "```\n",
    "\n",
    "| Return | Shape | Contains |\n",
    "|--------|-------|----------|\n",
    "| `output` | $(B, L, H_{out})$ | Hidden states from **last layer only**, all time steps |\n",
    "| `h_n` | $(num\\_layers, B, H_{out})$ | Final hidden state from **each layer** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9169546c-94f9-4a36-9f16-2b5f9ae86e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-layer RNN parameters:\n",
      "  weight_ih_l0: torch.Size([20, 10])\n",
      "  weight_hh_l0: torch.Size([20, 20])\n",
      "  bias_ih_l0: torch.Size([20])\n",
      "  bias_hh_l0: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Single-layer RNN\n",
    "rnn_single = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=True)\n",
    "\n",
    "print(\"Single-layer RNN parameters:\")\n",
    "for name, param in rnn_single.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4a3df0b1-8ec5-4b00-890d-6c274d7711c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer RNN (3 layers) parameters:\n",
      "  weight_ih_l0: torch.Size([20, 10])\n",
      "  weight_hh_l0: torch.Size([20, 20])\n",
      "  bias_ih_l0: torch.Size([20])\n",
      "  bias_hh_l0: torch.Size([20])\n",
      "  weight_ih_l1: torch.Size([20, 20])\n",
      "  weight_hh_l1: torch.Size([20, 20])\n",
      "  bias_ih_l1: torch.Size([20])\n",
      "  bias_hh_l1: torch.Size([20])\n",
      "  weight_ih_l2: torch.Size([20, 20])\n",
      "  weight_hh_l2: torch.Size([20, 20])\n",
      "  bias_ih_l2: torch.Size([20])\n",
      "  bias_hh_l2: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Multi-layer RNN (3 layers)\n",
    "rnn_multi = nn.RNN(input_size=10, hidden_size=20, num_layers=3, batch_first=True)\n",
    "\n",
    "print(\"Multi-layer RNN (3 layers) parameters:\")\n",
    "for name, param in rnn_multi.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f459289a-1887-4319-989c-ecef43c89910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([2, 5, 10])\n",
      "Output shape: torch.Size([2, 5, 20])\n",
      "h_n shape:    torch.Size([3, 2, 20])\n",
      "\n",
      "h_n[2] == output[:, -1, :]: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Forward pass with multi-layer RNN\n",
    "x = torch.randn(2, 5, 10)  # (batch=2, seq_len=5, input_size=10)\n",
    "h_0 = torch.zeros(3, 2, 20)  # (num_layers=3, batch=2, hidden_size=20)\n",
    "\n",
    "output, h_n = rnn_multi(x, h_0)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")      # (2, 5, 10)\n",
    "print(f\"Output shape: {output.shape}\") # (2, 5, 20) — last layer, all time steps\n",
    "print(f\"h_n shape:    {h_n.shape}\")    # (3, 2, 20) — all layers, final time step\n",
    "\n",
    "# h_n[0] = final hidden state of layer 0\n",
    "# h_n[1] = final hidden state of layer 1\n",
    "# h_n[2] = final hidden state of layer 2 = output[:, -1, :]\n",
    "print(f\"\\nh_n[2] == output[:, -1, :]: {torch.allclose(h_n[2], output[:, -1, :])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0f1195ed-616f-48c8-9164-e6d5cbf0cd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 W_ih shape: torch.Size([4, 3]) — (H_out=4, H_in=3)\n",
      "Layer 0 W_hh shape: torch.Size([4, 4]) — (H_out=4, H_out=4)\n",
      "Layer 1 W_ih shape: torch.Size([4, 4]) — (H_out=4, H_out=4)\n",
      "Layer 1 W_hh shape: torch.Size([4, 4]) — (H_out=4, H_out=4)\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Verify layer shapes\n",
    "rnn = nn.RNN(input_size=3, hidden_size=4, num_layers=2, batch_first=True)\n",
    "\n",
    "print(f\"Layer 0 W_ih shape: {rnn.weight_ih_l0.shape} — (H_out=4, H_in=3)\")\n",
    "print(f\"Layer 0 W_hh shape: {rnn.weight_hh_l0.shape} — (H_out=4, H_out=4)\")\n",
    "print(f\"Layer 1 W_ih shape: {rnn.weight_ih_l1.shape} — (H_out=4, H_out=4)\")\n",
    "print(f\"Layer 1 W_hh shape: {rnn.weight_hh_l1.shape} — (H_out=4, H_out=4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01fd35-79b7-40ac-b8e2-bcaf10298290",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb020ca0-a77f-4742-84d5-b63ab50335c5",
   "metadata": {},
   "source": [
    "\n",
    "A multi-layer RNN (also called a **stacked RNN**) consists of multiple RNN layers where the output of each layer becomes the input to the next layer. This allows the network to learn hierarchical representations of sequential data.\n",
    "\n",
    "---\n",
    "\n",
    "**Architecture Setup**\n",
    "\n",
    "Consider a multi-layer RNN with:\n",
    "- **Vocabulary size** $V$\n",
    "- **Embedding dimension** $d$ (this equals $H_{in}$ for the first RNN layer)\n",
    "- **Hidden size** $H_{out}$\n",
    "- **Number of layers** $N_{layers}$\n",
    "- **Sequence length** $L$\n",
    "- **Batch size** $B$\n",
    "\n",
    "\n",
    "Complete Data Flow Diagram on one example\n",
    "\n",
    "1. **Embeddings**\n",
    "\n",
    "```\n",
    "INPUT: \"I love Machine Learning\"\n",
    "                │\n",
    "                ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           TOKENIZATION                                       │\n",
    "│                                                                             │\n",
    "│    \"I love Machine Learning\"  →  [\"I\", \"love\", \"Machine\", \"Learning\"]      │\n",
    "│                                           │                                 │\n",
    "│                                           ▼                                 │\n",
    "│                               [0, 1, 2, 3]  (word indices)                 │\n",
    "│                               Shape: (B, L) = (1, 4)                        │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                  │\n",
    "                                  ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         EMBEDDING LAYER                                      │\n",
    "│                     nn.Embedding(V, d)                                       │\n",
    "│                                                                             │\n",
    "│    Word indices (1, 4) → Embedded vectors (1, 4, d)                        │\n",
    "│                                                                             │\n",
    "│    Each word index becomes a dense vector of dimension d                    │\n",
    "│    [0, 1, 2, 3] → [e₀, e₁, e₂, e₃]  where eᵢ ∈ ℝᵈ                         │\n",
    "│                                                                             │\n",
    "│    Output shape: (B, L, d) = (1, 4, d)                                     │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                │\n",
    "                │  Input to RNN: shape (B, L, H_in) where H_in = d\n",
    "                ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                                                                             │\n",
    "│                         MULTI-LAYER RNN                                     │\n",
    "│               nn.RNN(input_size=d, hidden_size=H_out, num_layers=3)        │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "2. **Layer 0: Processes Original Embeddings**\n",
    "\n",
    "\n",
    "```\n",
    "   Time:    t=0         t=1          t=2           t=3\n",
    "   Word:    \"I\"        \"love\"     \"Machine\"    \"Learning\"\n",
    "            e₀          e₁           e₂            e₃\n",
    "            ↓           ↓            ↓             ↓\n",
    "          ┌───┐       ┌───┐        ┌───┐        ┌───┐\n",
    "   h₋₁⁽⁰⁾→│ 0 │─h₀⁽⁰⁾→│ 0 │─h₁⁽⁰⁾──→│ 0 │─h₂⁽⁰⁾──→│ 0 │──→ h₃⁽⁰⁾\n",
    "    =0    └───┘       └───┘        └───┘        └───┘\n",
    "            ↓           ↓            ↓             ↓\n",
    "          h₀⁽⁰⁾       h₁⁽⁰⁾        h₂⁽⁰⁾         h₃⁽⁰⁾\n",
    "```\n",
    "\n",
    "**Equation:** $h_t^{(0)} = \\tanh\\Big(e_t W_{ih}^{(0)T} + b_{ih}^{(0)} + h_{t-1}^{(0)} W_{hh}^{(0)T} + b_{hh}^{(0)}\\Big)$\n",
    "\n",
    "$W_{ih}^{(0)}$ shape: $(H_{out}, d)$ ← Input dimension is $d$ (embedding dim)\n",
    "\n",
    "\n",
    "3. **Layer 1**: Processes Outputs from Layer 0\n",
    "\n",
    "```\n",
    "   Input:  h₀⁽⁰⁾       h₁⁽⁰⁾        h₂⁽⁰⁾         h₃⁽⁰⁾\n",
    "            ↓           ↓            ↓             ↓\n",
    "          ┌───┐       ┌───┐        ┌───┐        ┌───┐\n",
    "   h₋₁⁽¹⁾→│ 1 │─h₀⁽¹⁾→│ 1 │─h₁⁽¹⁾──→│ 1 │─h₂⁽¹⁾──→│ 1 │──→ h₃⁽¹⁾\n",
    "    =0    └───┘       └───┘        └───┘        └───┘\n",
    "            ↓           ↓            ↓             ↓\n",
    "          h₀⁽¹⁾       h₁⁽¹⁾        h₂⁽¹⁾         h₃⁽¹⁾\n",
    "```\n",
    "\n",
    "**Equation:** $h_t^{(1)} = \\tanh\\Big(h_t^{(0)} W_{ih}^{(1)T} + b_{ih}^{(1)} + h_{t-1}^{(1)} W_{hh}^{(1)T} + b_{hh}^{(1)}\\Big)$\n",
    "\n",
    "$W_{ih}^{(1)}$ shape: $(H_{out}, H_{out})$ ← Note: input is now $H_{out}$, not $d$!\n",
    "\n",
    "4. **Layer 2**: Processes Outputs from Layer 1 (FINAL LAYER)\n",
    "\n",
    "```\n",
    "   Input:  h₀⁽¹⁾       h₁⁽¹⁾        h₂⁽¹⁾         h₃⁽¹⁾\n",
    "            ↓           ↓            ↓             ↓\n",
    "          ┌───┐       ┌───┐        ┌───┐        ┌───┐\n",
    "   h₋₁⁽²⁾→│ 2 │─h₀⁽²⁾→│ 2 │─h₁⁽²⁾──→│ 2 │─h₂⁽²⁾──→│ 2 │──→ h₃⁽²⁾\n",
    "    =0    └───┘       └───┘        └───┘        └───┘\n",
    "            ↓           ↓            ↓             ↓\n",
    "          h₀⁽²⁾       h₁⁽²⁾        h₂⁽²⁾         h₃⁽²⁾\n",
    "```\n",
    "\n",
    "**Equation:** $h_t^{(2)} = \\tanh\\Big(h_t^{(1)} W_{ih}^{(2)T} + b_{ih}^{(2)} + h_{t-1}^{(2)} W_{hh}^{(2)T} + b_{hh}^{(2)}\\Big)$\n",
    "\n",
    "$W_{ih}^{(2)}$ shape: $(H_{out}, H_{out})$\n",
    "\n",
    "\n",
    "\n",
    "5. **RNN Outputs**\n",
    "   * ```[h₀⁽²⁾, h₁⁽²⁾, h₂⁽²⁾, h₃⁽²⁾]  ← FROM LAST LAYER ONLY! Shape: (B, L, H_out) = (1, 4, H_out) ```\n",
    "   * ```[h_n = [h₃⁽⁰⁾, h₃⁽¹⁾, h₃⁽²⁾]  ← FINAL TIME STEP, ALL LAYERS! Shape: (num_layers, B, H_out) = (3, 1, H_out) ```\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "**Vertical Slice View (What happens at a single time step)**\n",
    "\n",
    "At each time step $t$, the computation flows **vertically** through all layers:\n",
    "\n",
    "```\n",
    "Time step t:\n",
    "\n",
    "         Input: eₜ (embedding)\n",
    "                │\n",
    "                ▼\n",
    "        ┌───────────────┐\n",
    "        │   LAYER 0     │  h_t⁽⁰⁾ = tanh(eₜ W_ih⁽⁰⁾ᵀ + ... + h_{t-1}⁽⁰⁾ W_hh⁽⁰⁾ᵀ + ...)\n",
    "        │               │\n",
    "        │  W_ih: (H,d)  │  ← Input dimension is d (embedding dim)\n",
    "        └───────┬───────┘\n",
    "                │ h_t⁽⁰⁾\n",
    "                ▼\n",
    "        ┌───────────────┐\n",
    "        │   LAYER 1     │  h_t⁽¹⁾ = tanh(h_t⁽⁰⁾ W_ih⁽¹⁾ᵀ + ... + h_{t-1}⁽¹⁾ W_hh⁽¹⁾ᵀ + ...)\n",
    "        │               │\n",
    "        │  W_ih: (H,H)  │  ← Input dimension is H_out (from layer 0)\n",
    "        └───────┬───────┘\n",
    "                │ h_t⁽¹⁾\n",
    "                ▼\n",
    "        ┌───────────────┐\n",
    "        │   LAYER 2     │  h_t⁽²⁾ = tanh(h_t⁽¹⁾ W_ih⁽²⁾ᵀ + ... + h_{t-1}⁽²⁾ W_hh⁽²⁾ᵀ + ...)\n",
    "        │               │\n",
    "        │  W_ih: (H,H)  │  ← Input dimension is H_out (from layer 1)\n",
    "        └───────┬───────┘\n",
    "                │ h_t⁽²⁾\n",
    "                ▼\n",
    "         Output at time t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac38839-2b80-4095-90de-a64445635f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eac824fe-0f82-4595-a444-770f1a4bb946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETUP\n",
      "============================================================\n",
      "Sentence: 'I love Machine Learning'\n",
      "Tokens: ['I', 'love', 'Machine', 'Learning']\n",
      "Vocabulary: {'I': 0, 'love': 1, 'Machine': 2, 'Learning': 3}\n"
     ]
    }
   ],
   "source": [
    "# Complete example: \"I love Machine Learning\" through multi-layer RNN\n",
    "\n",
    "# Step 1: Setup\n",
    "sentence = \"I love Machine Learning\"\n",
    "tokens = sentence.split()\n",
    "vocab = {word: idx for idx, word in enumerate(tokens)}\n",
    "\n",
    "print(\"SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sentence: '{sentence}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ffe10a53-0a1f-4804-9534-ab6e6cfaa754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIMENSIONS\n",
      "============================================================\n",
      "Vocabulary size V = 4\n",
      "Embedding dimension d = H_in = 8\n",
      "Hidden size H_out = 6\n",
      "Number of layers = 3\n",
      "Sequence length L = 4\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define dimensions\n",
    "vocab_size = len(vocab)    # V = 4\n",
    "embed_dim = 8              # d = 8 (H_in for first RNN layer)\n",
    "hidden_size = 6            # H_out = 6\n",
    "num_layers = 3             # 3 stacked RNN layers\n",
    "batch_size = 1\n",
    "seq_length = len(tokens)   # L = 4\n",
    "\n",
    "print(\"\\nDIMENSIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Vocabulary size V = {vocab_size}\")\n",
    "print(f\"Embedding dimension d = H_in = {embed_dim}\")\n",
    "print(f\"Hidden size H_out = {hidden_size}\")\n",
    "print(f\"Number of layers = {num_layers}\")\n",
    "print(f\"Sequence length L = {seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "22198cd6-185e-4093-bc6d-37eab3975958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAYER PARAMETERS\n",
      "============================================================\n",
      "Embedding weight: torch.Size([4, 8]) → (V=4, d=8)\n",
      "\n",
      "RNN parameters:\n",
      "  weight_ih_l0: torch.Size([6, 8])\n",
      "  weight_hh_l0: torch.Size([6, 6])\n",
      "  bias_ih_l0: torch.Size([6])\n",
      "  bias_hh_l0: torch.Size([6])\n",
      "  weight_ih_l1: torch.Size([6, 6])\n",
      "  weight_hh_l1: torch.Size([6, 6])\n",
      "  bias_ih_l1: torch.Size([6])\n",
      "  bias_hh_l1: torch.Size([6])\n",
      "  weight_ih_l2: torch.Size([6, 6])\n",
      "  weight_hh_l2: torch.Size([6, 6])\n",
      "  bias_ih_l2: torch.Size([6])\n",
      "  bias_hh_l2: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create layers\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_size, \n",
    "             num_layers=num_layers, batch_first=True)\n",
    "\n",
    "print(\"\\nLAYER PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Embedding weight: {embedding.weight.shape} → (V={vocab_size}, d={embed_dim})\")\n",
    "print()\n",
    "print(\"RNN parameters:\")\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1e17130f-bbaf-4b8c-8ec4-b46b472778c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FORWARD PASS\n",
      "============================================================\n",
      "Word indices shape: torch.Size([1, 4]) → (B=1, L=4)\n",
      "Embedded shape:     torch.Size([1, 4, 8]) → (B=1, L=4, d=8)\n",
      "Initial h_0 shape:  torch.Size([3, 1, 6]) → (num_layers=3, B=1, H_out=6)\n",
      "\n",
      "Output shape:       torch.Size([1, 4, 6]) → (B=1, L=4, H_out=6)\n",
      "h_n shape:          torch.Size([3, 1, 6]) → (num_layers=3, B=1, H_out=6)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Forward pass\n",
    "word_indices = torch.tensor([[vocab[w] for w in tokens]])  # (1, 4)\n",
    "embedded = embedding(word_indices)                          # (1, 4, 8)\n",
    "h_0 = torch.zeros(num_layers, batch_size, hidden_size)      # (3, 1, 6)\n",
    "\n",
    "output, h_n = rnn(embedded, h_0)\n",
    "\n",
    "print(\"\\nFORWARD PASS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Word indices shape: {word_indices.shape} → (B=1, L=4)\")\n",
    "print(f\"Embedded shape:     {embedded.shape} → (B=1, L=4, d=8)\")\n",
    "print(f\"Initial h_0 shape:  {h_0.shape} → (num_layers=3, B=1, H_out=6)\")\n",
    "print()\n",
    "print(f\"Output shape:       {output.shape} → (B=1, L=4, H_out=6)\")\n",
    "print(f\"h_n shape:          {h_n.shape} → (num_layers=3, B=1, H_out=6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "42122633-5333-42f2-ad39-6576270e1808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERPRETING OUTPUTS\n",
      "============================================================\n",
      "\n",
      "'output' contains hidden states from LAST LAYER (layer 2) at ALL time steps:\n",
      "  output[0, 0, :] = h_0^(2) (after processing 'I')\n",
      "  output[0, 1, :] = h_1^(2) (after processing 'love')\n",
      "  output[0, 2, :] = h_2^(2) (after processing 'Machine')\n",
      "  output[0, 3, :] = h_3^(2) (after processing 'Learning')\n",
      "\n",
      "'h_n' contains FINAL hidden states from ALL LAYERS:\n",
      "  h_n[0, 0, :] = h_3^(0) (layer 0, after 'Learning')\n",
      "  h_n[1, 0, :] = h_3^(1) (layer 1, after 'Learning')\n",
      "  h_n[2, 0, :] = h_3^(2) (layer 2, after 'Learning')\n",
      "\n",
      "Key relationship:\n",
      "  h_n[-1] == output[:, -1, :] ? True\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Interpret the outputs\n",
    "print(\"\\nINTERPRETING OUTPUTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n'output' contains hidden states from LAST LAYER (layer 2) at ALL time steps:\")\n",
    "for t, word in enumerate(tokens):\n",
    "    print(f\"  output[0, {t}, :] = h_{t}^(2) (after processing '{word}')\")\n",
    "\n",
    "print(\"\\n'h_n' contains FINAL hidden states from ALL LAYERS:\")\n",
    "for layer in range(num_layers):\n",
    "    print(f\"  h_n[{layer}, 0, :] = h_3^({layer}) (layer {layer}, after 'Learning')\")\n",
    "\n",
    "print(\"\\nKey relationship:\")\n",
    "print(f\"  h_n[-1] == output[:, -1, :] ? {torch.allclose(h_n[-1], output[:, -1, :])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ba18424f-a25b-4451-8988-fa89b1265606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VERIFY W_ih SHAPES\n",
      "============================================================\n",
      "Layer 0: W_ih shape = torch.Size([6, 8]) → (H_out=6, H_in=8)\n",
      "Layer 1: W_ih shape = torch.Size([6, 6]) → (H_out=6, H_out=6)\n",
      "Layer 2: W_ih shape = torch.Size([6, 6]) → (H_out=6, H_out=6)\n",
      "\n",
      "Notice: Layer 0 takes embedding (dim=8), layers 1,2 take hidden states (dim=6)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Verify W_ih shapes differ between layer 0 and other layers\n",
    "print(\"\\nVERIFY W_ih SHAPES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Layer 0: W_ih shape = {rnn.weight_ih_l0.shape} → (H_out={hidden_size}, H_in={embed_dim})\")\n",
    "print(f\"Layer 1: W_ih shape = {rnn.weight_ih_l1.shape} → (H_out={hidden_size}, H_out={hidden_size})\")\n",
    "print(f\"Layer 2: W_ih shape = {rnn.weight_ih_l2.shape} → (H_out={hidden_size}, H_out={hidden_size})\")\n",
    "print()\n",
    "print(\"Notice: Layer 0 takes embedding (dim=8), layers 1,2 take hidden states (dim=6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c4469a12-ed6c-480c-bf3c-c362f028da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARAMETER COUNT\n",
      "============================================================\n",
      "Embedding: 4 × 8 = 32\n",
      "RNN total: 264\n",
      "  - Layer 0: W_ih(6×8) + W_hh(6×6) + biases\n",
      "  - Layer 1: W_ih(6×6) + W_hh(6×6) + biases\n",
      "  - Layer 2: W_ih(6×6) + W_hh(6×6) + biases\n",
      "\n",
      "Grand total: 296\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Count total parameters\n",
    "print(\"\\nPARAMETER COUNT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "embed_params = vocab_size * embed_dim\n",
    "print(f\"Embedding: {vocab_size} × {embed_dim} = {embed_params}\")\n",
    "\n",
    "rnn_params = sum(p.numel() for p in rnn.parameters())\n",
    "print(f\"RNN total: {rnn_params}\")\n",
    "print(f\"  - Layer 0: W_ih({hidden_size}×{embed_dim}) + W_hh({hidden_size}×{hidden_size}) + biases\")\n",
    "print(f\"  - Layer 1: W_ih({hidden_size}×{hidden_size}) + W_hh({hidden_size}×{hidden_size}) + biases\")\n",
    "print(f\"  - Layer 2: W_ih({hidden_size}×{hidden_size}) + W_hh({hidden_size}×{hidden_size}) + biases\")\n",
    "\n",
    "print(f\"\\nGrand total: {embed_params + rnn_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadea888-5344-46ec-9588-125c836e8b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
